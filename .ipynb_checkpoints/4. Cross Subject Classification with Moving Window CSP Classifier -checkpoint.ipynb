{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clinical BCI Challenge-WCCI2020\n",
    "- [website link](https://sites.google.com/view/bci-comp-wcci/?fbclid=IwAR37WLQ_xNd5qsZvktZCT8XJerHhmVb_bU5HDu69CnO85DE3iF0fs57vQ6M)\n",
    "\n",
    "\n",
    "- [Dataset Link](https://github.com/5anirban9/Clinical-Brain-Computer-Interfaces-Challenge-WCCI-2020-Glasgow)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import mne\n",
    "from scipy.io import loadmat\n",
    "import scipy\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as lda\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # to ignore warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False                    # global variable to suppress output display of MNE functions\n",
    "mne.set_log_level(verbose=verbose) # to suppress large info outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using kappa as evaluation metric\n",
    "kappa = sklearn.metrics.make_scorer(sklearn.metrics.cohen_kappa_score) # kappa scorer\n",
    "acc = sklearn.metrics.make_scorer(sklearn.metrics.accuracy_score)      # accuracy scorer\n",
    "scorer = kappa          # just assign another scorer to replace kappa scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = -1  # for multicore parallel processing, set it to 1 if cause memory issues, for full utilization set to -1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits=10, random_state=0)  # cross validation strategy to use \n",
    "csp_comps = [4]                                           # CSP n_components to use in Grid Search"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Conversion to MNE Datatypes\n",
    "[Mike Cohen Tutorials link for EEG Preprocessing](https://www.youtube.com/watch?v=uWB5tjhataY&list=PLn0OLiymPak2gDD-VDA90w9_iGDgOOb2o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_folder = globals()['_dh'][0]  # a hack to get path of current folder in which juptyter file is located\n",
    "data_path = os.path.join(current_folder, 'Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 8, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files        = glob.glob(data_path + '/*.mat')\n",
    "training_files   = glob.glob(data_path + '/*T.mat')\n",
    "evaluation_files = glob.glob(data_path + '/*E.mat')\n",
    "len(all_files), len(training_files), len(evaluation_files)     # if these return zero,then no file is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mne_epochs(filepath, verbose=verbose, t_start=2, fs=512, mode='train'):\n",
    "    '''\n",
    "    This function reads the EEG data from .mat file and convert it to MNE-Python Compatible epochs\n",
    "    data structure. It takes data from [0, 8] sec range and return it by setting t = 0 at cue onset\n",
    "    i.e. 3 seconds and dropping first two seconds so the output data is in [-1.0, 5.0] sec range. The\n",
    "    Details can be found in the preprocessing section of the attached document\n",
    "    '''\n",
    "    mat_data = loadmat(filepath) # read .mat file\n",
    "    eeg_data= mat_data['RawEEGData']\n",
    "    idx_start = fs*t_start      \n",
    "    eeg_data = eeg_data[:, :, idx_start:]\n",
    "    event_id = {'left-hand': 1, 'right-hand': 2}\n",
    "    channel_names = ['F3', 'FC3', 'C3', 'CP3', 'P3', 'FCz', 'CPz', 'F4', 'FC4', 'C4', 'CP4', 'P4']\n",
    "    info = mne.create_info(ch_names=channel_names, sfreq=fs, ch_types='eeg')\n",
    "    epochs = mne.EpochsArray(eeg_data, info, verbose=verbose, tmin=t_start-3.0)\n",
    "    epochs.set_montage('standard_1020')\n",
    "    epochs.filter(1., None) \n",
    "    epochs.apply_baseline(baseline=(-.250, 0)) # linear baseline correction\n",
    "    \n",
    "    if mode == 'train': # this in only applicable for training data\n",
    "        epochs.event_id = event_id\n",
    "        epochs.events[:,2] = mat_data['Labels'].ravel()    \n",
    "    return epochs \n",
    "\n",
    "def get_labels(filepath):\n",
    "    mat_data = loadmat(filepath) # read .mat file\n",
    "    return mat_data['Labels'].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of EEG Data:  (80, 12, 3072) \t Shape of Labels:  (80,)\n"
     ]
    }
   ],
   "source": [
    "epochs, labels = get_mne_epochs(training_files[0], verbose=verbose), get_labels(training_files[0])\n",
    "data = epochs.get_data()\n",
    "print('Shape of EEG Data: ', data.shape, '\\t Shape of Labels: ', labels.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets Append Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mne_epochs_complete(files_paths, verbose=verbose, t_start=2, fs=512, mode='train'):\n",
    "    '''\n",
    "    similar to get_mne_epochs, just appends data from all relevant files together to give a single\n",
    "    epoch object\n",
    "    '''\n",
    "    eeg_data = []\n",
    "    for filepath in files_paths:\n",
    "        mat_data = loadmat(filepath)\n",
    "        eeg_data.extend(mat_data['RawEEGData'])\n",
    "\n",
    "    idx_start = fs*t_start      # fs*ts\n",
    "    eeg_data = np.array(eeg_data)\n",
    "    eeg_data = eeg_data[:, :, idx_start:]\n",
    "    event_id = {'left-hand': 1, 'right-hand': 2}\n",
    "    channel_names = ['F3', 'FC3', 'C3', 'CP3', 'P3', 'FCz', 'CPz', 'F4', 'FC4', 'C4', 'CP4', 'P4']\n",
    "    info = mne.create_info(ch_names=channel_names, sfreq=fs, ch_types='eeg')\n",
    "    epochs = mne.EpochsArray(eeg_data, info, verbose=verbose, tmin=t_start-3.0)\n",
    "    epochs.set_montage('standard_1020')\n",
    "    epochs.filter(1., None) # required be ICA, (7-30 Hz) later\n",
    "    epochs.apply_baseline(baseline=(-.250, 0)) # linear baseline correction\n",
    "    \n",
    "    if mode == 'train': # this in only applicable for training data\n",
    "        labels = []\n",
    "        for filepath in files_paths:\n",
    "            mat_data = loadmat(filepath)\n",
    "            labels.extend(mat_data['Labels'].ravel())\n",
    "        epochs.event_id = event_id\n",
    "        epochs.events[:,2] = labels    \n",
    "    return epochs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading with Band Pass Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading relevant files\n",
    "training_epochs_all = get_mne_epochs_complete(training_files).filter(7,32)            # for all training subjects\n",
    "evaluation_epochs_9 = get_mne_epochs(evaluation_files[-2], mode='eval').filter(7,32)  # for subject 9\n",
    "evaluation_epochs_10 = get_mne_epochs(evaluation_files[-1], mode='eval').filter(7,32) # for subject 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave One Group Out CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group parameter for leave one group out cross validation in sklearn, each subject is given unique identifier\n",
    "group_list = []\n",
    "for subject in np.linspace(1,8,8):   # since we have total 8 subjects\n",
    "    group_list.extend([subject for _ in range(80)]) # since we have 80 samples in each training file\n",
    "groups = np.array(group_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = LeaveOneGroupOut()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Moving Window CSP Classifier \n",
    "Best Model for within Subject Category\n",
    "surface laplacian gives error with csp\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.decoding import CSP\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "class Custom_Segmented_CSP(TransformerMixin, BaseEstimator):\n",
    "    \"\"\"\n",
    "    Apply CSP individually to each window and then merge their features\n",
    "    Expects data in the format (trials, channels, eeg_data)\n",
    "    individually apply CSP on each band and then concatenate to give output of the form (trials, csp_filtered_data)\n",
    "    By Default Applies CSP on a single window [0.0,4.0] sec\n",
    "    Note: This funciton expects arrays/lists as input for t_start and t_end\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components=4, t_start=[0.0], t_end=[4.0], fs=512):\n",
    "        self.n_components = n_components           # csp components to retain\n",
    "        self.Csp = []                              # would carry list of CSP's applied individually to each window\n",
    "        self.t_start = t_start\n",
    "        self.t_end = t_end\n",
    "        self.fs = 512\n",
    "        self.start_idxs = (np.array(self.t_start)*self.fs).astype(np.int)\n",
    "        self.end_idxs =   (np.array(self.t_end)*self.fs).astype(np.int)\n",
    "        self.num_windows = 0\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        self.num_windows = len(self.start_idxs)\n",
    "        self.Csp = [CSP(n_components=self.n_components) for _ in range(self.num_windows)]\n",
    "        for i in range(self.num_windows):\n",
    "            x_seg = x[:,:,self.start_idxs[i]:self.end_idxs[i]]\n",
    "            self.Csp[i].fit(x_seg, y)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y=None):\n",
    "        dummy_array = []\n",
    "        for i in range(self.num_windows):\n",
    "            x_seg = x[:,:,self.start_idxs[i]:self.end_idxs[i]]\n",
    "            dummy_array.append(self.Csp[i].transform(x_seg))\n",
    "        return np.concatenate(dummy_array, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Exploration\n",
    "as number of windows increase so do the time required. Beware to only pass correct range [1.5,4.5] of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Information About Window Selection ----------\n",
      "Total Windows:  6\n",
      "Starting Time of Windows:  [0.  0.3 0.6 0.9 1.2 1.5]\n",
      "Ending Time of Windows  : [2.5 2.8 3.1 3.4 3.7 4. ]\n"
     ]
    }
   ],
   "source": [
    "print('-'*10, 'Information About Window Selection', '-'*10)\n",
    "t_start = np.arange(0,1.501,0.3)\n",
    "length_window = 2.5\n",
    "t_end = t_start + length_window\n",
    "print('Total Windows: ', len(t_start))\n",
    "print('Starting Time of Windows: ', t_start)\n",
    "print('Ending Time of Windows  :' ,  t_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 4                               # 4 csp comps\n",
    "x_train, y_train = training_epochs_all.get_data(), training_epochs_all.events[:,-1]\n",
    "x_train = x_train[:,:,512+256:-256] # from 0.5-4.5 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Classification Scores Comparison with default Parameters **********\n",
      "KNN           :  0.275\n",
      "Log-Regression:  0.31875\n",
      "Linear SVM    :  0.328125\n",
      "kernal SVM    :  0.2875\n",
      "LDA           :  0.33125\n"
     ]
    }
   ],
   "source": [
    "# using all channels, custom csp on 3 overlapping windows of segmented data\n",
    "t_start = np.arange(0,1.501,0.75)\n",
    "length_window = 2.5\n",
    "t_end = t_start + length_window\n",
    "print('*'*10, 'Classification Scores Comparison with default Parameters' ,'*'*10)\n",
    "print('KNN           : ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), StandardScaler(), KNeighborsClassifier()), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))\n",
    "print('Log-Regression: ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), StandardScaler(), LogisticRegression(max_iter=1000)), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))\n",
    "print('Linear SVM    : ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), StandardScaler(), LinearSVC(random_state=0)), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))\n",
    "print('kernal SVM    : ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), StandardScaler(), SVC(gamma='scale')), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))\n",
    "print('LDA           : ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), StandardScaler(), lda()), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Classification Scores Comparison with default Parameters **********\n",
      "KNN           :  0.209375\n",
      "Log-Regression:  0.265625\n",
      "Linear SVM    :  0.2625\n",
      "kernal SVM    :  0.2625\n",
      "LDA           :  0.26875000000000004\n"
     ]
    }
   ],
   "source": [
    "# using all channels, custom csp on single window of length 4\n",
    "t_start = np.array([0])\n",
    "length_window = 4\n",
    "t_end = t_start + length_window\n",
    "print('*'*10, 'Classification Scores Comparison with default Parameters' ,'*'*10)\n",
    "print('KNN           : ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), StandardScaler(), KNeighborsClassifier()), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))\n",
    "print('Log-Regression: ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), StandardScaler(), LogisticRegression(max_iter=1000)), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))\n",
    "print('Linear SVM    : ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), StandardScaler(), LinearSVC(random_state=0)), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))\n",
    "print('kernal SVM    : ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), StandardScaler(), SVC(gamma='scale')), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))\n",
    "print('LDA           : ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), StandardScaler(), lda()), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Classification Scores Comparison with default Parameters **********\n",
      "KNN           :  0.203125\n",
      "Log-Regression:  0.3125\n",
      "Linear SVM    :  0.315625\n",
      "kernal SVM    :  0.31875\n",
      "LDA           :  0.303125\n"
     ]
    }
   ],
   "source": [
    "# using all channels, custom csp on single window of length 3 from 1.5-4.5 sec, results improved\n",
    "x_train, y_train = training_epochs_all.get_data(), training_epochs_all.events[:,-1]\n",
    "x_train = x_train[:,:,512+256+512:-256] # from 0.5-4.5 sec\n",
    "t_start = np.array([0])\n",
    "length_window = 3\n",
    "t_end = t_start + length_window\n",
    "print('*'*10, 'Classification Scores Comparison with default Parameters' ,'*'*10)\n",
    "print('KNN           : ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), StandardScaler(), KNeighborsClassifier()), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))\n",
    "print('Log-Regression: ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), StandardScaler(), LogisticRegression(max_iter=1000)), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))\n",
    "print('Linear SVM    : ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), StandardScaler(), LinearSVC(random_state=0)), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))\n",
    "print('kernal SVM    : ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), StandardScaler(), SVC(gamma='scale')), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))\n",
    "print('LDA           : ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), StandardScaler(), lda()), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Classification Scores Comparison with default Parameters **********\n",
      "KNN           :  0.259375\n",
      "Log-Regression:  0.39999999999999997\n",
      "Linear SVM    :  0.409375\n",
      "kernal SVM    :  0.30625\n",
      "LDA           :  0.396875\n"
     ]
    }
   ],
   "source": [
    "# using all channels, custom csp with 3 window of length 2.5 from 1.5-4.5 sec \n",
    "x_train, y_train = training_epochs_all.get_data(), training_epochs_all.events[:,-1]\n",
    "x_train = x_train[:,:,512+256+512:-256] # from 0.5-4.5 sec\n",
    "t_start = np.arange(0,0.501,0.25)\n",
    "length_window = 2.5\n",
    "t_end = t_start + length_window\n",
    "print('*'*10, 'Classification Scores Comparison with default Parameters' ,'*'*10)\n",
    "print('KNN           : ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), StandardScaler(), KNeighborsClassifier()), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))\n",
    "print('Log-Regression: ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), StandardScaler(), LogisticRegression(max_iter=1000)), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))\n",
    "print('Linear SVM    : ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), StandardScaler(), LinearSVC(random_state=0)), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))\n",
    "print('kernal SVM    : ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), StandardScaler(), SVC(gamma='scale')), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))\n",
    "print('LDA           : ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), StandardScaler(), lda()), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "its very slow. Best setting 1.5-4.5 sec with 3 windows of length 2.5 with 250 ms displacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 4  # csp_comps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Information About Window Selection ----------\n",
      "Total Windows:  3\n",
      "Starting Time of Windows:  [0.   0.25 0.5 ]\n",
      "Ending Time of Windows  : [2.5  2.75 3.  ]\n"
     ]
    }
   ],
   "source": [
    "print('-'*10, 'Information About Window Selection', '-'*10)\n",
    "t_start = np.arange(0,0.501,0.25)\n",
    "length_window = 2.5\n",
    "t_end = t_start + length_window\n",
    "print('Total Windows: ', len(t_start))\n",
    "print('Starting Time of Windows: ', t_start)\n",
    "print('Ending Time of Windows  :' ,  t_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, y_train = training_epochs_all.get_data(), training_epochs_all.events[:,-1]\n",
    "x_train = x_train[:,:,512+512+256:-256] # to work only with [1.5,4.5] sec sweet spot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape  :  (640, 12, 1536)\n",
      "Training Labels Shape:  (640,)\n"
     ]
    }
   ],
   "source": [
    "print('Training Data Shape  : ', x_train.shape)\n",
    "print('Training Labels Shape: ', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Cross Validation Score: 0.284\n",
      "Optimal Parameters:  {'kneighborsclassifier__n_neighbors': 4}\n"
     ]
    }
   ],
   "source": [
    "param_grid_knn = {'kneighborsclassifier__n_neighbors': np.arange(2,15,2)}\n",
    "grid_knn = GridSearchCV(make_pipeline(Custom_Segmented_CSP(n,t_start,t_end), StandardScaler(), KNeighborsClassifier()), \n",
    "                        param_grid=param_grid_knn, cv=cv, scoring=scorer, n_jobs=n_jobs)\n",
    "grid_knn.fit(x_train, y_train, groups=groups) \n",
    "print('Maximum Cross Validation Score: {:.3f}'.format(grid_knn.best_score_))\n",
    "print('Optimal Parameters: ', grid_knn.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Cross Validation Score:  0.412\n",
      "Optimal Parameters:  {'logisticregression__C': 4.641588833612782}\n"
     ]
    }
   ],
   "source": [
    "# for logistic regression\n",
    "param_grid_log_reg = {'logisticregression__C' : np.logspace(-4, 3, 16)}    \n",
    "            \n",
    "grid_log_reg = GridSearchCV(make_pipeline(Custom_Segmented_CSP(n,t_start,t_end), StandardScaler(), LogisticRegression())\n",
    "                            , param_grid=param_grid_log_reg, cv=cv, scoring=scorer) \n",
    "grid_log_reg.fit(x_train, y_train, groups=groups) \n",
    "print('Maximum Cross Validation Score: ',  round(grid_log_reg.best_score_,3))\n",
    "print('Optimal Parameters: ', grid_log_reg.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Cross Validation Score:  0.416\n",
      "Optimal Parameters:  {'linearsvc__C': 0.31622776601683794}\n"
     ]
    }
   ],
   "source": [
    "# for linear svm\n",
    "param_grid_linear_svm =     {'linearsvc__C' : np.logspace(-4, 3, 15)}\n",
    "grid_linear_svm = GridSearchCV(make_pipeline(Custom_Segmented_CSP(n,t_start,t_end), StandardScaler(), LinearSVC()), \n",
    "                               param_grid=param_grid_linear_svm, cv=cv, scoring=scorer, n_jobs=n_jobs)\n",
    "grid_linear_svm.fit(x_train, y_train, groups=groups) \n",
    "print('Maximum Cross Validation Score: ',  round(grid_linear_svm.best_score_,3))\n",
    "print('Optimal Parameters: ', grid_linear_svm.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Cross Validation Score:  0.394\n",
      "Optimal Parameters:  {'svc__C': 1000.0, 'svc__gamma': 0.00015625}\n"
     ]
    }
   ],
   "source": [
    "# kernel svm\n",
    "param_grid_kernel_svm = {'svc__C': np.logspace(-4, 3, 8),\n",
    "                         'svc__gamma': np.logspace(-3, 1, 3) / x_train.shape[0]} \n",
    "                         \n",
    "grid_kernel_svm = GridSearchCV(make_pipeline(Custom_Segmented_CSP(n,t_start,t_end), StandardScaler(), SVC()), \n",
    "                            param_grid=param_grid_kernel_svm, cv=cv, scoring=scorer, n_jobs=n_jobs)\n",
    "grid_kernel_svm.fit(x_train, y_train, groups=groups) \n",
    "print('Maximum Cross Validation Score: ',  round(grid_kernel_svm.best_score_,3))\n",
    "print('Optimal Parameters: ', grid_kernel_svm.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Cross Validation Score:  0.403\n",
      "Optimal Parameters:  {'lineardiscriminantanalysis__shrinkage': 'auto'}\n"
     ]
    }
   ],
   "source": [
    "# lda, auto shrinkage works pretty well\n",
    "shrinkage = list(np.arange(0,1.01,0.5))\n",
    "shrinkage.append('auto')\n",
    "\n",
    "param_grid_lda = {'lineardiscriminantanalysis__shrinkage': shrinkage}   \n",
    "grid_lda = GridSearchCV(make_pipeline(Custom_Segmented_CSP(n,t_start,t_end), StandardScaler(), lda(solver='eigen')), \n",
    "                        param_grid=param_grid_lda, cv=cv, scoring=scorer, n_jobs=n_jobs)\n",
    "grid_lda.fit(x_train, y_train, groups=groups) \n",
    "print('Maximum Cross Validation Score: ',  round(grid_lda.best_score_,3))\n",
    "print('Optimal Parameters: ', grid_lda.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Predicted Counts on Subject 9 **********\n",
      "KNN          :  Class 1 = 27 Class 2 = 13\n",
      "LogReg       :  Class 1 = 20 Class 2 = 20\n",
      "LinearSVM    :  Class 1 = 20 Class 2 = 20\n",
      "KernelSVM    :  Class 1 = 21 Class 2 = 19\n",
      "LDA          :  Class 1 = 17 Class 2 = 23\n"
     ]
    }
   ],
   "source": [
    "# predictions counts Subject 9\n",
    "x_eval = evaluation_epochs_9.get_data()\n",
    "x_eval = x_eval[:,:,256+512+512:-256] # from 1.5-4.5 sec\n",
    "preds_knn = grid_knn.predict(x_eval)\n",
    "preds_log_reg = grid_log_reg.predict(x_eval)\n",
    "preds_linear_svm = grid_linear_svm.predict(x_eval)\n",
    "preds_kernel_svm = grid_kernel_svm.predict(x_eval)\n",
    "preds_lda = grid_lda.predict(x_eval)\n",
    "print('*'*10, 'Predicted Counts on Subject 9' ,'*'*10)\n",
    "print('KNN          : ', 'Class 1 =', sum(preds_knn==1), 'Class 2 =', sum(preds_knn==2)) \n",
    "print('LogReg       : ', 'Class 1 =', sum(preds_log_reg==1), 'Class 2 =', sum(preds_log_reg==2))\n",
    "print('LinearSVM    : ', 'Class 1 =', sum(preds_linear_svm==1), 'Class 2 =', sum(preds_linear_svm==2))\n",
    "print('KernelSVM    : ', 'Class 1 =', sum(preds_kernel_svm==1), 'Class 2 =', sum(preds_kernel_svm==2))\n",
    "print('LDA          : ', 'Class 1 =', sum(preds_lda==1), 'Class 2 =', sum(preds_lda==2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Predicted Counts on Subject 10 **********\n",
      "KNN          :  Class 1 = 18 Class 2 = 22\n",
      "LogReg       :  Class 1 = 3 Class 2 = 37\n",
      "LinearSVM    :  Class 1 = 3 Class 2 = 37\n",
      "KernelSVM    :  Class 1 = 2 Class 2 = 38\n",
      "LDA          :  Class 1 = 2 Class 2 = 38\n"
     ]
    }
   ],
   "source": [
    "# predictions counts Subject 10\n",
    "x_eval =evaluation_epochs_10.get_data()\n",
    "x_eval = x_eval[:,:,256+512+512:-256]  # from 1.5-4.5 sec\n",
    "preds_knn = grid_knn.predict(x_eval)\n",
    "preds_log_reg = grid_log_reg.predict(x_eval)\n",
    "preds_linear_svm = grid_linear_svm.predict(x_eval)\n",
    "preds_kernel_svm = grid_kernel_svm.predict(x_eval)\n",
    "preds_lda = grid_lda.predict(x_eval)\n",
    "print('*'*10, 'Predicted Counts on Subject 10' ,'*'*10)\n",
    "print('KNN          : ', 'Class 1 =', sum(preds_knn==1), 'Class 2 =', sum(preds_knn==2)) \n",
    "print('LogReg       : ', 'Class 1 =', sum(preds_log_reg==1), 'Class 2 =', sum(preds_log_reg==2))\n",
    "print('LinearSVM    : ', 'Class 1 =', sum(preds_linear_svm==1), 'Class 2 =', sum(preds_linear_svm==2))\n",
    "print('KernelSVM    : ', 'Class 1 =', sum(preds_kernel_svm==1), 'Class 2 =', sum(preds_kernel_svm==2))\n",
    "print('LDA          : ', 'Class 1 =', sum(preds_lda==1), 'Class 2 =', sum(preds_lda==2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "results look good for subject 9 and bad for 10. Linear SVM is the winner"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
