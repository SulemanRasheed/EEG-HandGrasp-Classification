{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clinical BCI Challenge-WCCI2020\n",
    "- [website link](https://sites.google.com/view/bci-comp-wcci/?fbclid=IwAR37WLQ_xNd5qsZvktZCT8XJerHhmVb_bU5HDu69CnO85DE3iF0fs57vQ6M)\n",
    "\n",
    "\n",
    " - [Dataset Link](https://github.com/5anirban9/Clinical-Brain-Computer-Interfaces-Challenge-WCCI-2020-Glasgow)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import mne\n",
    "from scipy.io import loadmat\n",
    "import scipy\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from mne.decoding import CSP\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.compose import make_column_transformer, make_column_selector\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # to ignore warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False # to universally just change it to true/false for different output display\n",
    "mne.set_log_level(verbose=verbose) # to suppress large info outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using kappa as evaluation metric\n",
    "kappa = sklearn.metrics.make_scorer(sklearn.metrics.cohen_kappa_score) # kappa scorer\n",
    "acc = sklearn.metrics.make_scorer(sklearn.metrics.accuracy_score)      # accuracy scorer\n",
    "scorer = kappa          # just assign another scorer to replace kappa scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = None  # for multicore parallel processing, set it to 1 if cause memory issues, for full utilization set to -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Conversion to MNE Datatypes\n",
    "[Mike Cohen Tutorials link for EEG Preprocessing](https://www.youtube.com/watch?v=uWB5tjhataY&list=PLn0OLiymPak2gDD-VDA90w9_iGDgOOb2o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_folder = globals()['_dh'][0]  # a hack to get path of current folder in which juptyter file is located\n",
    "data_path = os.path.join(current_folder, 'Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 8, 10)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files        = glob.glob(data_path + '/*.mat')\n",
    "training_files   = glob.glob(data_path + '/*T.mat')\n",
    "evaluation_files = glob.glob(data_path + '/*E.mat')\n",
    "len(all_files), len(training_files), len(evaluation_files)     # if these return zero,then no file is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mne_epochs(filepath, verbose=verbose, t_start=2, fs=512, mode='train'):\n",
    "    '''\n",
    "    This function reads the EEG data from .mat file and convert it to MNE-Python Compatible epochs\n",
    "    data structure. It takes data from [0, 8] sec range and return it by setting t = 0 at cue onset\n",
    "    i.e. 3 seconds and dropping first two seconds so the output data is in [-1.0, 5.0] sec range. The\n",
    "    Details can be found in the preprocessing section of the attached document\n",
    "    '''\n",
    "    mat_data = loadmat(filepath) # read .mat file\n",
    "    eeg_data= mat_data['RawEEGData']\n",
    "    idx_start = fs*t_start      \n",
    "    eeg_data = eeg_data[:, :, idx_start:]\n",
    "    event_id = {'left-hand': 1, 'right-hand': 2}\n",
    "    channel_names = ['F3', 'FC3', 'C3', 'CP3', 'P3', 'FCz', 'CPz', 'F4', 'FC4', 'C4', 'CP4', 'P4']\n",
    "    info = mne.create_info(ch_names=channel_names, sfreq=fs, ch_types='eeg')\n",
    "    epochs = mne.EpochsArray(eeg_data, info, verbose=verbose, tmin=t_start-3.0)\n",
    "    epochs.set_montage('standard_1020')\n",
    "    epochs.filter(1., None) \n",
    "    epochs.apply_baseline(baseline=(-.250, 0)) # linear baseline correction\n",
    "    \n",
    "    if mode == 'train': # this in only applicable for training data\n",
    "        epochs.event_id = event_id\n",
    "        epochs.events[:,2] = mat_data['Labels'].ravel()    \n",
    "    return epochs \n",
    "\n",
    "def get_labels(filepath):\n",
    "    mat_data = loadmat(filepath) # read .mat file\n",
    "    return mat_data['Labels'].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of EEG Data:  (80, 12, 3072) \t Shape of Labels:  (80,)\n"
     ]
    }
   ],
   "source": [
    "epochs, labels = get_mne_epochs(training_files[0], verbose=verbose), get_labels(training_files[0])\n",
    "data = epochs.get_data()\n",
    "print('Shape of EEG Data: ', data.shape, '\\t Shape of Labels: ', labels.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading original data\n",
    "epochs_list_train = []\n",
    "for i in training_files:\n",
    "    epochs_list_train.append(get_mne_epochs(i, verbose=verbose))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Data\n",
    "first 8 for single subject and last 2 are for cross subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_list_eval = []\n",
    "for i in evaluation_files:\n",
    "    epochs_list_eval.append(get_mne_epochs(i, mode='test', verbose=verbose))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bandpass filtering of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epochs in epochs_list_train:\n",
    "    epochs.filter(7.0, 32.0)\n",
    "    \n",
    "for epochs in epochs_list_eval:\n",
    "    epochs.filter(7.0, 32.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets try doing some classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits=5, random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = epochs_list_train[3]\n",
    "psds, freqs = mne.time_frequency.psd_multitaper(epochs, tmin=0.5, tmax=4.5, fmin=8, fmax=30 ,n_jobs=1)\n",
    "psds = 10 * np.log10(psds) # to convert powers to DB\n",
    "labels = epochs.events[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set:  features:  (60, 12, 88) labels:  (60,)\n",
      "Test  set:  features:  (20, 12, 88) labels:  (20,)\n"
     ]
    }
   ],
   "source": [
    "x_trainVal, x_test, y_trainVal, y_test = train_test_split(psds, labels.ravel(), shuffle=True, stratify=labels, random_state=0)  to avoid confusing names and reusing x_trainVal\n",
    "print('train set:  features: ', x_trainVal.shape, 'labels: ', y_trainVal.shape)\n",
    "print('Test  set:  features: ', x_test.shape, 'labels: ', y_test.shape)\n",
    "y_train = y_trainVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Classification Scores Comparison with default Parameters **********\n",
      "############### Using All Channels ###############\n",
      "KNN           :  0.4666666666666667\n",
      "Log-Regression:  0.8\n",
      "Linear SVM    :  0.8\n",
      "kernal SVM    :  0.6666666666666667\n",
      "LDA           :  0.6000000000000001\n"
     ]
    }
   ],
   "source": [
    "# using all channels\n",
    "trials, channels, eeg = x_trainVal.shape\n",
    "x_train = x_trainVal.reshape(trials, channels*eeg) \n",
    "print('*'*10, 'Classification Scores Comparison with default Parameters' ,'*'*10)\n",
    "print('#'*15, 'Using All Channels', '#'*15)\n",
    "print('KNN           : ', np.mean(cross_val_score(make_pipeline(StandardScaler(),KNeighborsClassifier()), x_train, y_train, cv=cv, scoring=scorer)))\n",
    "print('Log-Regression: ', np.mean(cross_val_score(make_pipeline(StandardScaler(),LogisticRegression(max_iter=1000)), x_train, y_train, cv=cv, scoring=scorer)))\n",
    "print('Linear SVM    : ', np.mean(cross_val_score(make_pipeline(StandardScaler(),LinearSVC(random_state=0)), x_train, y_train, cv=cv, scoring=scorer)))\n",
    "print('kernal SVM    : ', np.mean(cross_val_score(make_pipeline(StandardScaler(), SVC(gamma='scale')), x_train, y_train, cv=cv, scoring=scorer)))\n",
    "print('LDA           : ', np.mean(cross_val_score(make_pipeline(StandardScaler(), lda()), x_train, y_train, cv=cv, scoring=scorer)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "with [0.5, 4.5] seconds time interval and [8, 30] Hz freqs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedShuffleSplit(10, random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for linear svm\n",
    "param_grid_linear_svm =   { 'linearsvc__C' : np.logspace(-4, 2, 15)}\n",
    "\n",
    "# lda, auto shrinkage performs pretty well mostly \n",
    "shrinkage = list(np.arange(0.1,1.01,0.1))\n",
    "shrinkage.append('auto')\n",
    "param_grid_lda = {'lineardiscriminantanalysis__shrinkage': shrinkage} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids_linear_svm_list = [GridSearchCV(make_pipeline(StandardScaler(), LinearSVC(random_state=0)), \n",
    "                               param_grid=param_grid_linear_svm, cv=cv, n_jobs=n_jobs, scoring=scorer)\n",
    "                        for _ in range(len(training_files))]\n",
    "\n",
    "grids_lda_list = [GridSearchCV(make_pipeline(StandardScaler(), lda(solver='eigen')), \n",
    "                        param_grid=param_grid_lda, cv=cv, n_jobs=n_jobs, scoring=scorer)\n",
    "                 for _ in range(len(training_files))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_function(subject_index=0):\n",
    "    # this time training function trains on whole training set\n",
    "    print('-'*25, 'Training for Subject:', subject_index+1, '-'*25)\n",
    "    epochs = epochs_list_train[subject_index]\n",
    "    psds, freqs = mne.time_frequency.psd_multitaper(epochs, tmin=0.5, tmax=4.5, fmin=8, fmax=30 ,n_jobs=1)\n",
    "    psds = 10 * np.log10(psds)\n",
    "    psds = psds.reshape(psds.shape[0], -1)\n",
    "    labels = epochs.events[:,-1]\n",
    "\n",
    "    grids_linear_svm_list[subject_index].fit(psds, labels)\n",
    "    print('LinearSVM: Maximum Cross Validation Score = ', round(grids_linear_svm_list[subject_index].best_score_,3))\n",
    "    grids_lda_list[subject_index].fit(psds, labels)\n",
    "    print('LDA      : Maximum Cross Validation Score = ', round(grids_lda_list[subject_index].best_score_,3))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_function(subject_index=0):  \n",
    "    # prints the prediction counts for each class\n",
    "    epochs = epochs_list_eval[subject_index]\n",
    "    psds, freqs = mne.time_frequency.psd_multitaper(epochs, tmin=0.5, tmax=4.5, fmin=8, fmax=30 ,n_jobs=1)\n",
    "    psds = 10 * np.log10(psds)\n",
    "    psds = psds.reshape(psds.shape[0], -1)\n",
    "    \n",
    "    preds_linear_svm = grids_linear_svm_list[subject_index].predict(psds)\n",
    "    preds_lda =  grids_lda_list[subject_index].predict(psds)\n",
    "    print('-'*25, 'Predictions Counts Subject:', subject_index+1, '-'*25)\n",
    "    print('Linear SVM: Class 1 =', sum(preds_linear_svm==1), 'Class 2 =', sum(preds_linear_svm==2))\n",
    "    print('LDA       : Class 1 =', sum(preds_lda==1), 'Class 2 =', sum(preds_lda==2))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's Training Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- Training for Subject: 1 -------------------------\n",
      "LinearSVM: Maximum Cross Validation Score =  0.65\n",
      "LDA      : Maximum Cross Validation Score =  0.625\n",
      "\n",
      "------------------------- Training for Subject: 2 -------------------------\n",
      "LinearSVM: Maximum Cross Validation Score =  0.75\n",
      "LDA      : Maximum Cross Validation Score =  0.75\n",
      "\n",
      "------------------------- Training for Subject: 3 -------------------------\n",
      "LinearSVM: Maximum Cross Validation Score =  0.675\n",
      "LDA      : Maximum Cross Validation Score =  0.675\n",
      "\n",
      "------------------------- Training for Subject: 4 -------------------------\n",
      "LinearSVM: Maximum Cross Validation Score =  0.775\n",
      "LDA      : Maximum Cross Validation Score =  0.775\n",
      "\n",
      "------------------------- Training for Subject: 5 -------------------------\n",
      "LinearSVM: Maximum Cross Validation Score =  0.425\n",
      "LDA      : Maximum Cross Validation Score =  0.425\n",
      "\n",
      "------------------------- Training for Subject: 6 -------------------------\n",
      "LinearSVM: Maximum Cross Validation Score =  0.825\n",
      "LDA      : Maximum Cross Validation Score =  0.825\n",
      "\n",
      "------------------------- Training for Subject: 7 -------------------------\n",
      "LinearSVM: Maximum Cross Validation Score =  0.35\n",
      "LDA      : Maximum Cross Validation Score =  0.275\n",
      "\n",
      "------------------------- Training for Subject: 8 -------------------------\n",
      "LinearSVM: Maximum Cross Validation Score =  0.4\n",
      "LDA      : Maximum Cross Validation Score =  0.475\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for subject in range(len(training_files)):\n",
    "    training_function(subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- Predictions Counts Subject: 1 -------------------------\n",
      "Linear SVM: Class 1 = 29 Class 2 = 11\n",
      "LDA       : Class 1 = 29 Class 2 = 11\n",
      "\n",
      "------------------------- Predictions Counts Subject: 2 -------------------------\n",
      "Linear SVM: Class 1 = 39 Class 2 = 1\n",
      "LDA       : Class 1 = 38 Class 2 = 2\n",
      "\n",
      "------------------------- Predictions Counts Subject: 3 -------------------------\n",
      "Linear SVM: Class 1 = 27 Class 2 = 13\n",
      "LDA       : Class 1 = 29 Class 2 = 11\n",
      "\n",
      "------------------------- Predictions Counts Subject: 4 -------------------------\n",
      "Linear SVM: Class 1 = 32 Class 2 = 8\n",
      "LDA       : Class 1 = 30 Class 2 = 10\n",
      "\n",
      "------------------------- Predictions Counts Subject: 5 -------------------------\n",
      "Linear SVM: Class 1 = 36 Class 2 = 4\n",
      "LDA       : Class 1 = 37 Class 2 = 3\n",
      "\n",
      "------------------------- Predictions Counts Subject: 6 -------------------------\n",
      "Linear SVM: Class 1 = 18 Class 2 = 22\n",
      "LDA       : Class 1 = 23 Class 2 = 17\n",
      "\n",
      "------------------------- Predictions Counts Subject: 7 -------------------------\n",
      "Linear SVM: Class 1 = 26 Class 2 = 14\n",
      "LDA       : Class 1 = 20 Class 2 = 20\n",
      "\n",
      "------------------------- Predictions Counts Subject: 8 -------------------------\n",
      "Linear SVM: Class 1 = 15 Class 2 = 25\n",
      "LDA       : Class 1 = 16 Class 2 = 24\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for subject in range(len(training_files)):\n",
    "    evaluation_function(subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "svm always better except the last subject so only last entry for lda and all others for svm in excel file"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
