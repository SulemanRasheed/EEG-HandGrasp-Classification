{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clinical BCI Challenge-WCCI2020\n",
    "- [website link](https://sites.google.com/view/bci-comp-wcci/?fbclid=IwAR37WLQ_xNd5qsZvktZCT8XJerHhmVb_bU5HDu69CnO85DE3iF0fs57vQ6M)\n",
    "\n",
    "\n",
    " - [Dataset Link](https://github.com/5anirban9/Clinical-Brain-Computer-Interfaces-Challenge-WCCI-2020-Glasgow)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import mne\n",
    "import pywt\n",
    "import scipy\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from mne.decoding import CSP\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedShuffleSplit \n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # to ignore warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False                    # global variable to suppress output display of MNE functions\n",
    "mne.set_log_level(verbose=verbose) # to suppress large info outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using kappa as evaluation metric\n",
    "kappa = sklearn.metrics.make_scorer(sklearn.metrics.cohen_kappa_score) # kappa scorer\n",
    "acc = sklearn.metrics.make_scorer(sklearn.metrics.accuracy_score)      # accuracy scorer\n",
    "scorer = kappa          # just assign another scorer to replace kappa scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = -1  # for multicore parallel processing, set it to 1 if cause memory issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Conversion to MNE Datatypes\n",
    "[Mike Cohen Tutorials link for EEG Preprocessing](https://www.youtube.com/watch?v=uWB5tjhataY&list=PLn0OLiymPak2gDD-VDA90w9_iGDgOOb2o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_folder = globals()['_dh'][0]  # a hack to get path of current folder in which juptyter file is located\n",
    "data_path = os.path.join(current_folder, 'Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 8, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files        = glob.glob(data_path + '/*.mat')\n",
    "training_files   = glob.glob(data_path + '/*T.mat')\n",
    "evaluation_files = glob.glob(data_path + '/*E.mat')\n",
    "len(all_files), len(training_files), len(evaluation_files) # if these return zero,then no file is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mne_epochs(filepath, verbose=verbose, t_start=2, fs=512, mode='train'):\n",
    "    '''\n",
    "    This function reads the EEG data from .mat file and convert it to MNE-Python Compatible epochs\n",
    "    data structure. It takes data from [0, 8] sec range and return it by setting t = 0 at cue onset\n",
    "    i.e. 3 seconds and dropping first two seconds so the output data is in [-1.0, 5.0] sec range. The\n",
    "    Details can be found in the preprocessing section of the attached document\n",
    "    '''\n",
    "    mat_data = loadmat(filepath) # read .mat file\n",
    "    eeg_data= mat_data['RawEEGData']\n",
    "    idx_start = fs*t_start      \n",
    "    eeg_data = eeg_data[:, :, idx_start:]\n",
    "    event_id = {'left-hand': 1, 'right-hand': 2}\n",
    "    channel_names = ['F3', 'FC3', 'C3', 'CP3', 'P3', 'FCz', 'CPz', 'F4', 'FC4', 'C4', 'CP4', 'P4']\n",
    "    info = mne.create_info(ch_names=channel_names, sfreq=fs, ch_types='eeg')\n",
    "    epochs = mne.EpochsArray(eeg_data, info, verbose=verbose, tmin=t_start-3.0)\n",
    "    epochs.set_montage('standard_1020')\n",
    "    epochs.filter(1., None) \n",
    "    epochs.apply_baseline(baseline=(-.250, 0)) # linear baseline correction\n",
    "    \n",
    "    if mode == 'train': # this in only applicable for training data\n",
    "        epochs.event_id = event_id\n",
    "        epochs.events[:,2] = mat_data['Labels'].ravel()    \n",
    "    return epochs \n",
    "\n",
    "def get_labels(filepath):\n",
    "    mat_data = loadmat(filepath) # read .mat file\n",
    "    return mat_data['Labels'].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of EEG Data:  (80, 12, 3072) \t Shape of Labels:  (80,)\n"
     ]
    }
   ],
   "source": [
    "epochs, labels = get_mne_epochs(training_files[0], verbose=verbose), get_labels(training_files[0])\n",
    "data = epochs.get_data()\n",
    "print('Shape of EEG Data: ', data.shape, '\\t Shape of Labels: ', labels.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets Append Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mne_epochs_complete(files_paths, verbose=verbose, t_start=2, fs=512, mode='train'):\n",
    "    '''\n",
    "    similar to get_mne_epochs, just appends data from all relevant files together to give a single\n",
    "    epoch object\n",
    "    '''\n",
    "    eeg_data = []\n",
    "    for filepath in files_paths:\n",
    "        mat_data = loadmat(filepath)\n",
    "        eeg_data.extend(mat_data['RawEEGData'])\n",
    "\n",
    "    idx_start = fs*t_start      # fs*ts\n",
    "    eeg_data = np.array(eeg_data)\n",
    "    eeg_data = eeg_data[:, :, idx_start:]\n",
    "    event_id = {'left-hand': 1, 'right-hand': 2}\n",
    "    channel_names = ['F3', 'FC3', 'C3', 'CP3', 'P3', 'FCz', 'CPz', 'F4', 'FC4', 'C4', 'CP4', 'P4']\n",
    "    info = mne.create_info(ch_names=channel_names, sfreq=fs, ch_types='eeg')\n",
    "    epochs = mne.EpochsArray(eeg_data, info, verbose=verbose, tmin=t_start-3.0)\n",
    "    epochs.set_montage('standard_1020')\n",
    "    epochs.filter(1., None) # required be ICA, (7-30 Hz) later\n",
    "    epochs.apply_baseline(baseline=(-.250, 0)) # linear baseline correction\n",
    "    \n",
    "    if mode == 'train': # this in only applicable for training data\n",
    "        labels = []\n",
    "        for filepath in files_paths:\n",
    "            mat_data = loadmat(filepath)\n",
    "            labels.extend(mat_data['Labels'].ravel())\n",
    "        epochs.event_id = event_id\n",
    "        epochs.events[:,2] = labels    \n",
    "    return epochs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading with Band Pass Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading relevant files\n",
    "training_epochs_all = get_mne_epochs_complete(training_files).filter(7,32)\n",
    "evaluation_epochs_9 = get_mne_epochs(evaluation_files[-2], mode='eval').filter(7,32)\n",
    "evaluation_epochs_10 = get_mne_epochs(evaluation_files[-1], mode='eval').filter(7,32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave One Group Out CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group parameter for leave one group out cross validation in sklearn, each subject is given unique identifier\n",
    "group_list = []\n",
    "for subject in np.linspace(1,8,8):\n",
    "    group_list.extend([subject for _ in range(80)]) # since we have 80 samples in each training file\n",
    "groups = np.array(group_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = LeaveOneGroupOut()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wavelet Packet Decomposition WPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wpd(X, wavelet ='coif1', maxlevel=3): \n",
    "    coeffs = pywt.WaveletPacket(X, wavelet ,mode='symmetric',maxlevel=maxlevel) # coif1 is recommended by a paper\n",
    "    return coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wpd_coeffs(x, wavelet='coif1', wpd_levels=3, order='natural'):\n",
    "    '''\n",
    "    expects data in (trials, channels, raw_data) and return in (trials, bands, channels, wpd_coeffs) for sklearn compatibility\n",
    "    '''\n",
    "    \n",
    "    num_bands = 2**wpd_levels\n",
    "    trials, channels = x.shape[0], x.shape[1]\n",
    "    \n",
    "    # first find the last dim size, bcz it is different for different levels and different mother wavelets\n",
    "    coeff_size = wpd(x[0,0,:], wavelet, wpd_levels).get_level(wpd_levels)[0].data.shape[0]\n",
    "    \n",
    "    # we shall fill this array (trials, bands, channels, eegdata)\n",
    "    bands = np.empty((trials, num_bands, channels, coeff_size))\n",
    "       \n",
    "    for trial in range(trials):\n",
    "        for ch in range(channels):\n",
    "            pos = []\n",
    "            coeff = wpd(x[trial,ch,:], wavelet, wpd_levels) \n",
    "            nodes_paths = [node.path for node in coeff.get_level(level=wpd_levels, order=order)]\n",
    "            \n",
    "            for band, path in enumerate(nodes_paths): \n",
    "                bands[trial,band,ch,:] = coeff[path].data \n",
    "        \n",
    "    return bands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_power_wpd(data):\n",
    "    trials, bands, channels, N = data.shape\n",
    "    power = np.empty(shape=(trials, bands, channels))\n",
    "    \n",
    "    for trial in range(trials):\n",
    "        for band in range(bands):\n",
    "            for ch in range(channels):\n",
    "                power[trial,band,ch] = (sum(data[trial,band,ch,:]**2))/N\n",
    "    return power      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics_wpd(data, ax=-1):\n",
    "    mean = np.nanmean(data, axis=ax)\n",
    "    std = np.nanstd(data, axis=ax)\n",
    "    return np.stack([mean, std], axis=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wpd_features(data, wavelet='coif2', level=5, retained_coeffs = 'All', stat_feat=True, power_feat=True):\n",
    "    '''\n",
    "    takes (trials, channels, data) as input and returns (trials, bands, channels, features) as output\n",
    "    has flexibility to control which set of features to use\n",
    "    '''\n",
    "    wpd_coeffs = get_wpd_coeffs(data,  wavelet=wavelet, wpd_levels=level)\n",
    "    \n",
    "    if retained_coeffs != 'All':\n",
    "        wpd_coeffs = wpd_coeffs[:,retained_coeffs,:,:]\n",
    "        \n",
    "    if power_feat == True and stat_feat == True:        # both power and statistical features\n",
    "        power = np.expand_dims(calculate_avg_power_wpd(wpd_coeffs), axis=-1)\n",
    "        statistics = calculate_statistics_wpd(wpd_coeffs)\n",
    "        band_features = np.concatenate((power, statistics), axis=-1)\n",
    "    elif power_feat == True and stat_feat == False:     # only power features\n",
    "        power = np.expand_dims(calculate_avg_power_wpd(wpd_coeffs), axis=-1)\n",
    "        band_features = power\n",
    "    elif power_feat == False and stat_feat == True:     # only statistical features\n",
    "        statistics = calculate_statistics_wpd(wpd_coeffs)\n",
    "        band_features = statistics\n",
    "    elif power_feat == False and stat_feat == False:    # invalid case\n",
    "        print('Cannot Set Both Power and Statistical Features to False')\n",
    "        \n",
    "    return band_features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Wavelet Transform DWT\n",
    "wavedec returns [cA_n, cD_n, cD_n-1, ..., cD2, cD1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dwt(data, wavelet='coif1', level=3):\n",
    "    coeffs = pywt.wavedec(data, wavelet=wavelet, level=level)\n",
    "    return coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_power_dwt(data):\n",
    "    # expects individual coefficient of DWT as input\n",
    "    trials, channels, N = data.shape\n",
    "    power = np.empty(shape=(trials, channels))\n",
    "    \n",
    "    for trial in range(trials):\n",
    "        for ch in range(channels):\n",
    "            power[trial,ch] = (sum(data[trial,ch,:]**2))/N # division by N converts energy to power\n",
    "    return power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you only want to use power features then use this function \n",
    "def get_dwt_power_features(data, wavelet='coif1', level=3, retained_coeffs = 'All'):\n",
    "    '''\n",
    "    takes (trials, channels, data) as input and returns (trials, bands, channels) as output\n",
    "    '''\n",
    "    dwt_coeffs = pywt.wavedec(data, wavelet=wavelet, level=level)\n",
    "    if retained_coeffs == 'All':\n",
    "        energy_dwt = np.stack(arrays = ([calculate_avg_power_dwt(dwt_coeffs[i]) for i in range(level+1)]), axis=1)\n",
    "    else: \n",
    "        energy_dwt = np.stack(arrays = ([calculate_avg_power_dwt(dwt_coeffs[i]) for i in retained_coeffs]), axis=1)\n",
    "    return energy_dwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_statistics_dwt(data, ax=-1):\n",
    "    mean = np.nanmean(data, axis=ax)\n",
    "    std = np.nanstd(data, axis=ax)\n",
    "    return np.stack([mean, std], axis=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dwt_features(data, wavelet='coif2', level=5, retained_coeffs = 'All', stat_feat=True, power_feat=True):\n",
    "    '''\n",
    "    takes (trials, channels, data) as input and returns (trials, bands, channels, features) as output\n",
    "    has flexibility to control which set of features to use\n",
    "    '''\n",
    "    dwt_coeffs = pywt.wavedec(data, wavelet=wavelet, level=level)\n",
    "    dummy_list = []\n",
    "    \n",
    "    if retained_coeffs == 'All':\n",
    "        iter_list = range(len(dwt_coeffs))\n",
    "    else:\n",
    "        iter_list = retained_coeffs\n",
    "    \n",
    "    for i in iter_list:\n",
    "        \n",
    "        if power_feat == True and stat_feat == True:         # both power and statistical features\n",
    "            power = np.expand_dims(calculate_avg_power_dwt(dwt_coeffs[i]), axis=-1)\n",
    "            statistics = calculate_statistics_dwt(dwt_coeffs[i])\n",
    "            band_features = np.concatenate((power, statistics), axis = -1)\n",
    "        elif power_feat == True and stat_feat == False:     # only power features\n",
    "            power = np.expand_dims(calculate_avg_power_dwt(dwt_coeffs[i]), axis=-1)\n",
    "            band_features = power\n",
    "        elif power_feat == False and stat_feat == True:     # only statistical features\n",
    "            statistics = calculate_statistics_dwt(dwt_coeffs[i])\n",
    "            band_features = statistics\n",
    "        elif power_feat == False and stat_feat == False:    # invalid case\n",
    "            print('Cannot Set Both Power and Statistical Features to False')\n",
    "            break\n",
    "        \n",
    "        dummy_list.append(band_features)\n",
    "        \n",
    "    return np.stack(dummy_list, axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing a custom transformer \n",
    "that can combine csp and reshape the wpd_data/dwt_data from (traila, bands, channels, data) to  (trials, channels, data) by applying CSP individually on each band and then concatenating the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from mne.decoding import CSP\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "class Custom_WT_CSP(TransformerMixin, BaseEstimator):\n",
    "    \"\"\"\n",
    "    Expects data in the format (trials, bands, channels, wavelet_coeffs_data)\n",
    "    individually apply CSP on each band and then concatenate to give output of the form (trials, channels, csp_filtered_data)\n",
    "    \"\"\"\n",
    "    def __init__(self, csp_components=4):\n",
    "        self.csp_components = csp_components       # csp components to retain\n",
    "        self.num_bands = 0                         # captures the total frequency bands in filtered data\n",
    "        self.Csp = []                              # would carry list of CSP's applied individually to each freq band\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        self.num_bands = x.shape[1]\n",
    "        self.Csp = [CSP(n_components=self.csp_components, reg=None, log=True, norm_trace=False) for _ in range(self.num_bands)]\n",
    "        [self.Csp[i].fit(x[:,i,:,:], y) for i in range(self.num_bands)]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y=None):\n",
    "        return np.concatenate(tuple(self.Csp[i].transform(x[:,i,:,:]) for i in range(self.num_bands)),axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions to Check which Wavelet to Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelets = ['haar', 'db2', 'db4', 'coif1', 'coif2', 'coif4', 'sym2', 'sym4']\n",
    "level_wpd = 3\n",
    "level_dwt = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to calculate mean scores across the classifiers\n",
    "def find_mean_score(data, labels, scorer=kappa, csp_comps=4):\n",
    "    classifiers = [\n",
    "        LinearSVC(random_state=0),\n",
    "        LDA(solver='eigen', shrinkage='auto')]\n",
    "\n",
    "    scores = []\n",
    "    for clf in classifiers:\n",
    "        scores.append(np.mean(cross_val_score(make_pipeline(Custom_WT_CSP(csp_comps), StandardScaler(), clf), data, labels, scoring=scorer, cv=5)))\n",
    "    return np.round(np.mean(scores), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_wavelet(data, labels, wavelets, level_dwt=5, level_wpd=3, csp_comps=4):\n",
    "    scores = []\n",
    "    for wavelet in wavelets:\n",
    "        dwt_features = get_dwt_features(data, wavelet=wavelet, level=level_dwt)\n",
    "        wpd_features = get_wpd_features(data, wavelet=wavelet, level=level_wpd)\n",
    "        combo_features = np.concatenate((wpd_features, dwt_features), axis=1)\n",
    "        scores.append(find_mean_score(combo_features, labels, csp_comps=csp_comps))\n",
    "    best_wavelet = wavelets[np.argmax(scores)]\n",
    "    print('Best Wavelet:', best_wavelet, '\\tBest Score:', max(scores))    \n",
    "    return best_wavelet     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Best Wavelet: coif2 \tBest Score: 0.288\n"
     ]
    }
   ],
   "source": [
    "# for all training subjects\n",
    "epochs = training_epochs_all\n",
    "data, labels = epochs.get_data(), epochs.events[:,-1]\n",
    "data = data[:,:,512+256:-256]            # to capture data from 0.5s to 4.5s range\n",
    "best_wavelet = get_best_wavelet(data, labels, wavelets, csp_comps=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_wavelet = 'coif2'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Performance with Default Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "i = 4  # csp components"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Classification Scores Comparison with Default Settings ----------\n",
      "KNN           :  0.184375\n",
      "Log-Regression:  0.28125\n",
      "Linear SVM    :  0.271875\n",
      "kernal SVM    :  0.278125\n",
      "LDA           :  0.2875\n"
     ]
    }
   ],
   "source": [
    "# from 0.5-4.5 sec\n",
    "epochs = training_epochs_all.copy()\n",
    "data, labels = epochs.get_data(), epochs.events[:,-1]\n",
    "data = data[:,:,256+512:-256]\n",
    "dwt_features = get_dwt_features(data, wavelet= best_wavelet, level=level_dwt)\n",
    "wpd_features = get_wpd_features(data, wavelet= best_wavelet, level=level_wpd)\n",
    "combo_wavelet_features = np.concatenate((wpd_features, dwt_features), axis=1)\n",
    "x_train = combo_wavelet_features\n",
    "y_train = labels\n",
    "print('-'*10, 'Classification Scores Comparison with Default Settings', 10*'-')\n",
    "print('KNN           : ', np.mean(cross_val_score(make_pipeline(Custom_WT_CSP(i), StandardScaler(), KNeighborsClassifier()), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))\n",
    "print('Log-Regression: ', np.mean(cross_val_score(make_pipeline(Custom_WT_CSP(i), StandardScaler(), LogisticRegression(max_iter=1000)), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))\n",
    "print('Linear SVM    : ', np.mean(cross_val_score(make_pipeline(Custom_WT_CSP(i), StandardScaler(), LinearSVC(random_state=0)), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))\n",
    "print('kernal SVM    : ', np.mean(cross_val_score(make_pipeline(Custom_WT_CSP(i), StandardScaler(), SVC(gamma='scale')), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))\n",
    "print('LDA           : ', np.mean(cross_val_score(make_pipeline(Custom_WT_CSP(i), StandardScaler(), LDA()), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Classification Scores Comparison with Default Settings ----------\n",
      "KNN           :  0.1875\n",
      "Log-Regression:  0.296875\n",
      "Linear SVM    :  0.28437500000000004\n",
      "kernal SVM    :  0.31875\n",
      "LDA           :  0.296875\n"
     ]
    }
   ],
   "source": [
    "# from 1.5-4.5 sec, best one\n",
    "epochs = training_epochs_all.copy()\n",
    "data, labels = epochs.get_data(), epochs.events[:,-1]\n",
    "data = data[:,:,256+512+512:-256]\n",
    "dwt_features = get_dwt_features(data, wavelet= best_wavelet, level=level_dwt)\n",
    "wpd_features = get_wpd_features(data, wavelet= best_wavelet, level=level_wpd)\n",
    "combo_wavelet_features = np.concatenate((wpd_features, dwt_features), axis=1)\n",
    "x_train = combo_wavelet_features\n",
    "y_train = labels\n",
    "print('-'*10, 'Classification Scores Comparison with Default Settings', 10*'-')\n",
    "print('KNN           : ', np.mean(cross_val_score(make_pipeline(Custom_WT_CSP(i), StandardScaler(), KNeighborsClassifier()), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))\n",
    "print('Log-Regression: ', np.mean(cross_val_score(make_pipeline(Custom_WT_CSP(i), StandardScaler(), LogisticRegression(max_iter=1000)), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))\n",
    "print('Linear SVM    : ', np.mean(cross_val_score(make_pipeline(Custom_WT_CSP(i), StandardScaler(), LinearSVC(random_state=0)), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))\n",
    "print('kernal SVM    : ', np.mean(cross_val_score(make_pipeline(Custom_WT_CSP(i), StandardScaler(), SVC(gamma='scale')), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))\n",
    "print('LDA           : ', np.mean(cross_val_score(make_pipeline(Custom_WT_CSP(i), StandardScaler(), LDA()), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "[1.5,4.5] sec window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = training_epochs_all.copy()\n",
    "data, labels = epochs.get_data(), epochs.events[:,-1]\n",
    "data = data[:,:,256+512+512:-256] # from 1.5 sec to 4.5 sec\n",
    "dwt_features = get_dwt_features(data, wavelet= best_wavelet, level=level_dwt)\n",
    "wpd_features = get_wpd_features(data, wavelet= best_wavelet, level=level_wpd)\n",
    "combo_wavelet_features = np.concatenate((wpd_features, dwt_features), axis=1)\n",
    "x_train = combo_wavelet_features\n",
    "y_train = labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Cross Validation Accuracy: 0.325\n",
      "Optimal Parameters:  {'custom_wt_csp__csp_components': 4, 'kneighborsclassifier__n_neighbors': 12}\n"
     ]
    }
   ],
   "source": [
    "# for kNN\n",
    "param_grid_knn = {'kneighborsclassifier__n_neighbors': np.arange(2,15,2),\n",
    "                 'custom_wt_csp__csp_components': csp_comps}\n",
    "grid_knn = GridSearchCV(make_pipeline(Custom_WT_CSP(), StandardScaler(), KNeighborsClassifier()), \n",
    "                        param_grid=param_grid_knn, cv=cv, scoring=scorer, n_jobs=n_jobs)\n",
    "grid_knn.fit(x_train, y_train, groups=groups) \n",
    "print('Maximum Cross Validation Accuracy: {:.3f}'.format(grid_knn.best_score_))\n",
    "print('Optimal Parameters: ', grid_knn.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Cross Validation Accuracy:  0.322\n",
      "Optimal Parameters:  {'custom_wt_csp__csp_components': 4, 'logisticregression__C': 0.01}\n"
     ]
    }
   ],
   "source": [
    "# for logistic regression\n",
    "param_grid_log_reg = {'logisticregression__C' : np.logspace(-4, 3, 15),\n",
    "                     'custom_wt_csp__csp_components': csp_comps}\n",
    "grid_log_reg = GridSearchCV(make_pipeline(Custom_WT_CSP(), StandardScaler(), LogisticRegression()), \n",
    "                            param_grid=param_grid_log_reg, cv=cv, scoring=scorer) \n",
    "grid_log_reg.fit(x_train, y_train, groups=groups) \n",
    "print('Maximum Cross Validation Accuracy: ',  round(grid_log_reg.best_score_,3))\n",
    "print('Optimal Parameters: ', grid_log_reg.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Cross Validation Score:  0.325\n",
      "Optimal Parameters:  {'custom_wt_csp__csp_components': 4, 'linearsvc__C': 0.0025118864315095794}\n"
     ]
    }
   ],
   "source": [
    "# for linear svm\n",
    "param_grid_linear_svm =     {'linearsvc__C' : np.logspace(-4, 3, 16),\n",
    "                             'custom_wt_csp__csp_components': csp_comps}\n",
    "grid_linear_svm = GridSearchCV(make_pipeline(Custom_WT_CSP(), StandardScaler(), LinearSVC()), \n",
    "                               param_grid=param_grid_linear_svm, cv=cv, scoring=scorer, n_jobs=n_jobs)\n",
    "grid_linear_svm.fit(x_train, y_train, groups=groups) \n",
    "print('Maximum Cross Validation Score: ',  round(grid_linear_svm.best_score_,3))\n",
    "print('Optimal Parameters: ', grid_linear_svm.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Cross Validation Accuracy:  0.353\n",
      "Optimal Parameters:  {'custom_wt_csp__csp_components': 4, 'svc__C': 10.0, 'svc__gamma': 0.0015625}\n"
     ]
    }
   ],
   "source": [
    "# kernel svm\n",
    "param_grid_kernel_svm = {'svc__C': np.logspace(-4, 3, 8),\n",
    "                         'svc__gamma': np.logspace(-4, 2, 4) / x_train.shape[0],\n",
    "                        'custom_wt_csp__csp_components': csp_comps} \n",
    "                         \n",
    "grid_kernel_svm = GridSearchCV(make_pipeline(Custom_WT_CSP(), StandardScaler(), SVC()), \n",
    "                            param_grid=param_grid_kernel_svm, cv=cv, scoring=scorer, n_jobs=n_jobs)\n",
    "grid_kernel_svm.fit(x_train, y_train, groups=groups) \n",
    "print('Maximum Cross Validation Accuracy: ',  round(grid_kernel_svm.best_score_,3))\n",
    "print('Optimal Parameters: ', grid_kernel_svm.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Cross Validation Score:  0.331\n",
      "Optimal Parameters:  {'custom_wt_csp__csp_components': 4, 'lineardiscriminantanalysis__shrinkage': 0.2}\n"
     ]
    }
   ],
   "source": [
    "# for lda\n",
    "shrinkage = list(np.arange(0.0,1.01,0.05))\n",
    "shrinkage.append('auto')\n",
    "\n",
    "param_grid_lda = {'lineardiscriminantanalysis__shrinkage': shrinkage,\n",
    "                  'custom_wt_csp__csp_components': csp_comps}   \n",
    "grid_lda = GridSearchCV(make_pipeline(Custom_WT_CSP(), StandardScaler(), LDA(solver='eigen')), \n",
    "                        param_grid=param_grid_lda, cv=cv, scoring=scorer, n_jobs=n_jobs)\n",
    "grid_lda.fit(x_train, y_train, groups=groups) \n",
    "print('Maximum Cross Validation Score: ',  round(grid_lda.best_score_,3))\n",
    "print('Optimal Parameters: ', grid_lda.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Predictions Counts on Evaluation set **********\n",
      "KNN          :  Class 1 = 19 Class 2 = 21\n",
      "LogReg       :  Class 1 = 20 Class 2 = 20\n",
      "LinearSVM    :  Class 1 = 20 Class 2 = 20\n",
      "KernelSVM    :  Class 1 = 20 Class 2 = 20\n",
      "LDA          :  Class 1 = 20 Class 2 = 20\n"
     ]
    }
   ],
   "source": [
    "# evaluation subject 9\n",
    "epochs_eval = evaluation_epochs_9\n",
    "data_eval = epochs_eval.get_data()\n",
    "data_eval = data_eval[:,:,512+512+256:-256]    # from 1.5 to 4.5 seconds spot\n",
    "dwt_features_eval = get_dwt_features(data_eval, wavelet= best_wavelet, level=level_dwt)\n",
    "wpd_features_eval = get_wpd_features(data_eval, wavelet= best_wavelet, level=level_wpd)\n",
    "combo_wavelet_features_eval = np.concatenate((wpd_features_eval, dwt_features_eval), axis=1)\n",
    "x_eval = combo_wavelet_features_eval\n",
    "\n",
    "preds_knn = grid_knn.predict(x_eval)\n",
    "preds_log_reg = grid_log_reg.predict(x_eval)\n",
    "preds_linear_svm = grid_linear_svm.predict(x_eval)\n",
    "preds_kernel_svm = grid_kernel_svm.predict(x_eval)\n",
    "preds_lda = grid_lda.predict(x_eval)\n",
    "print('*'*10, 'Predictions Counts on Evaluation set' ,'*'*10)\n",
    "print('KNN          : ', 'Class 1 =', sum(preds_knn==1), 'Class 2 =', sum(preds_knn==2)) \n",
    "print('LogReg       : ', 'Class 1 =', sum(preds_log_reg==1), 'Class 2 =', sum(preds_log_reg==2))\n",
    "print('LinearSVM    : ', 'Class 1 =', sum(preds_linear_svm==1), 'Class 2 =', sum(preds_linear_svm==2))\n",
    "print('KernelSVM    : ', 'Class 1 =', sum(preds_kernel_svm==1), 'Class 2 =', sum(preds_kernel_svm==2))\n",
    "print('LDA          : ', 'Class 1 =', sum(preds_lda==1), 'Class 2 =', sum(preds_lda==2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Predictions Counts on Evaluation set **********\n",
      "KNN          :  Class 1 = 19 Class 2 = 21\n",
      "LogReg       :  Class 1 = 16 Class 2 = 24\n",
      "LinearSVM    :  Class 1 = 17 Class 2 = 23\n",
      "KernelSVM    :  Class 1 = 18 Class 2 = 22\n",
      "LDA          :  Class 1 = 17 Class 2 = 23\n"
     ]
    }
   ],
   "source": [
    "# evaluation epochs 10\n",
    "epochs_eval = evaluation_epochs_10\n",
    "data_eval = epochs_eval.get_data()\n",
    "data_eval = data_eval[:,:,512+256+512:-256] # from 1.5 to 4.5 seconds spot\n",
    "dwt_features_eval = get_dwt_features(data_eval, wavelet= best_wavelet, level=level_dwt)\n",
    "wpd_features_eval = get_wpd_features(data_eval, wavelet= best_wavelet, level=level_wpd)\n",
    "combo_wavelet_features_eval = np.concatenate((wpd_features_eval, dwt_features_eval), axis=1)\n",
    "x_eval = combo_wavelet_features_eval\n",
    "\n",
    "preds_knn = grid_knn.predict(x_eval)\n",
    "preds_log_reg = grid_log_reg.predict(x_eval)\n",
    "preds_linear_svm = grid_linear_svm.predict(x_eval)\n",
    "preds_kernel_svm = grid_kernel_svm.predict(x_eval)\n",
    "preds_lda = grid_lda.predict(x_eval)\n",
    "print('*'*10, 'Predictions Counts on Evaluation set' ,'*'*10)\n",
    "print('KNN          : ', 'Class 1 =', sum(preds_knn==1), 'Class 2 =', sum(preds_knn==2)) \n",
    "print('LogReg       : ', 'Class 1 =', sum(preds_log_reg==1), 'Class 2 =', sum(preds_log_reg==2))\n",
    "print('LinearSVM    : ', 'Class 1 =', sum(preds_linear_svm==1), 'Class 2 =', sum(preds_linear_svm==2))\n",
    "print('KernelSVM    : ', 'Class 1 =', sum(preds_kernel_svm==1), 'Class 2 =', sum(preds_kernel_svm==2))\n",
    "print('LDA          : ', 'Class 1 =', sum(preds_lda==1), 'Class 2 =', sum(preds_lda==2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "Kernel SVM seems to be the winner. The good thing is that results are stable and distribution is like what we expect but the bad thing is that the kappa values are less than other two methods"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
