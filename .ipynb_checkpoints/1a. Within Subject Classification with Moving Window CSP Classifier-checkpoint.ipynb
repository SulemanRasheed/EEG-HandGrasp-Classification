{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clinical BCI Challenge-WCCI2020\n",
    "- [website link](https://sites.google.com/view/bci-comp-wcci/?fbclid=IwAR37WLQ_xNd5qsZvktZCT8XJerHhmVb_bU5HDu69CnO85DE3iF0fs57vQ6M)\n",
    "\n",
    "\n",
    " - [Dataset Link](https://github.com/5anirban9/Clinical-Brain-Computer-Interfaces-Challenge-WCCI-2020-Glasgow)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import mne\n",
    "from scipy.io import loadmat\n",
    "import scipy\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from mne.decoding import CSP\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # to ignore warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False                    # global variable to suppress output display of MNE functions\n",
    "mne.set_log_level(verbose=verbose) # to suppress large info outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using kappa as evaluation metric\n",
    "kappa = sklearn.metrics.make_scorer(sklearn.metrics.cohen_kappa_score) # kappa scorer\n",
    "acc = sklearn.metrics.make_scorer(sklearn.metrics.accuracy_score)      # accuracy scorer\n",
    "scorer = kappa          # just assign another scorer to replace kappa scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = None  # for multicore parallel processing, set it to 1 if cause memory issues, for full utilization set to -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Conversion to MNE Datatypes\n",
    "[Mike Cohen Tutorials link for EEG Preprocessing](https://www.youtube.com/watch?v=uWB5tjhataY&list=PLn0OLiymPak2gDD-VDA90w9_iGDgOOb2o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_folder = globals()['_dh'][0]  # a hack to get path of current folder in which juptyter file is located\n",
    "data_path = os.path.join(current_folder, 'Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_files   = glob.glob(data_path + '/*T.mat')\n",
    "len(training_files)     # if  return zero,then no file is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mne_epochs(filepath, verbose=verbose, t_start=2, fs=512, mode='train'):\n",
    "    '''\n",
    "    This function reads the EEG data from .mat file and convert it to MNE-Python Compatible epochs\n",
    "    data structure. It takes data from [0, 8] sec range and return it by setting t = 0 at cue onset\n",
    "    i.e. 3 seconds and dropping first two seconds so the output data is in [-1.0, 5.0] sec range. The\n",
    "    Details can be found in the preprocessing section of the attached document\n",
    "    '''\n",
    "    mat_data = loadmat(filepath) # read .mat file\n",
    "    eeg_data= mat_data['RawEEGData']\n",
    "    idx_start = fs*t_start      \n",
    "    eeg_data = eeg_data[:, :, idx_start:]\n",
    "    event_id = {'left-hand': 1, 'right-hand': 2}\n",
    "    channel_names = ['F3', 'FC3', 'C3', 'CP3', 'P3', 'FCz', 'CPz', 'F4', 'FC4', 'C4', 'CP4', 'P4']\n",
    "    info = mne.create_info(ch_names=channel_names, sfreq=fs, ch_types='eeg')\n",
    "    epochs = mne.EpochsArray(eeg_data, info, verbose=verbose, tmin=t_start-3.0)\n",
    "    epochs.set_montage('standard_1020')\n",
    "    epochs.filter(1., None) \n",
    "    epochs.apply_baseline(baseline=(-.250, 0)) # linear baseline correction\n",
    "    \n",
    "    if mode == 'train': # this in only applicable for training data\n",
    "        epochs.event_id = event_id\n",
    "        epochs.events[:,2] = mat_data['Labels'].ravel()    \n",
    "    return epochs \n",
    "\n",
    "def get_labels(filepath):\n",
    "    mat_data = loadmat(filepath) # read .mat file\n",
    "    return mat_data['Labels'].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of EEG Data:  (80, 12, 3072) \t Shape of Labels:  (80,)\n"
     ]
    }
   ],
   "source": [
    "epochs, labels = get_mne_epochs(training_files[0], verbose=verbose), get_labels(training_files[0])\n",
    "data = epochs.get_data()\n",
    "print('Shape of EEG Data: ', data.shape, '\\t Shape of Labels: ', labels.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading original data\n",
    "epochs_list_train = []\n",
    "for i in training_files:\n",
    "    epochs_list_train.append(get_mne_epochs(i, verbose=verbose))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bandpass filtering of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epochs in epochs_list_train:\n",
    "    epochs.filter(7.0, 32.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Transformer \n",
    "Applies CSP algo in each window of eeg separately and merge the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.decoding import CSP\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "class Custom_Segmented_CSP(TransformerMixin, BaseEstimator):\n",
    "    \"\"\"\n",
    "    Apply CSP individually to each window and then merge their features\n",
    "    Expects data in the format (trials, channels, eeg_data)\n",
    "    individually apply CSP on each band and then concatenate to give output of the form (trials, csp_filtered_data)\n",
    "    By Default Applies CSP on a single window [0.0,4.0] sec\n",
    "    Note: This funciton expects arrays/lists as input for t_start and t_end\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components=4, t_start=[0.0], t_end=[4.0], fs=512):\n",
    "        self.n_components = n_components           # csp components to retain\n",
    "        self.Csp = []                              # would carry list of CSP's applied individually to each window\n",
    "        self.t_start = t_start\n",
    "        self.t_end = t_end\n",
    "        self.fs = 512\n",
    "        self.start_idxs = (np.array(self.t_start)*self.fs).astype(np.int)\n",
    "        self.end_idxs =   (np.array(self.t_end)*self.fs).astype(np.int)\n",
    "        self.num_windows = 0\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        self.num_windows = len(self.start_idxs)\n",
    "        self.Csp = [CSP(n_components=self.n_components) for _ in range(self.num_windows)]\n",
    "        for i in range(self.num_windows):\n",
    "            x_seg = x[:,:,self.start_idxs[i]:self.end_idxs[i]]\n",
    "            self.Csp[i].fit(x_seg, y)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y=None):\n",
    "        dummy_array = []\n",
    "        for i in range(self.num_windows):\n",
    "            x_seg = x[:,:,self.start_idxs[i]:self.end_idxs[i]]\n",
    "            dummy_array.append(self.Csp[i].transform(x_seg))\n",
    "        return np.concatenate(dummy_array, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets try doing some classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = epochs_list_train[4]\n",
    "data = epochs.get_data()\n",
    "data = data[:,:,512+256:-256] # # from 0.5s to 4.5s\n",
    "labels = epochs.events[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set:  features:  (60, 12, 2048) labels:  (60,)\n",
      "Test  set:  features:  (20, 12, 2048) labels:  (20,)\n"
     ]
    }
   ],
   "source": [
    "x_trainVal, x_test, y_trainVal, y_test = train_test_split(data, labels.ravel(), shuffle=True, stratify=labels, random_state=0) # to avoid confusing names and reusing x_trainVal\n",
    "print('train set:  features: ', x_trainVal.shape, 'labels: ', y_trainVal.shape)\n",
    "print('Test  set:  features: ', x_test.shape, 'labels: ', y_test.shape)\n",
    "y_train = y_trainVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits=5, random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Classification Scores Comparison with default Parameters **********\n",
      "KNN           :  0.4666666666666668\n",
      "Log-Regression:  0.7333333333333334\n",
      "Linear SVM    :  0.6666666666666667\n",
      "kernal SVM    :  0.4666666666666668\n",
      "LDA           :  0.2666666666666667\n"
     ]
    }
   ],
   "source": [
    "# using all channels, custom csp on 11 overlapping windows of segmented data\n",
    "t_start = np.arange(0,1.6,0.15)\n",
    "length_window = 2.5\n",
    "t_end = t_start + length_window\n",
    "i = 4\n",
    "x_train = x_trainVal\n",
    "print('*'*10, 'Classification Scores Comparison with default Parameters' ,'*'*10)\n",
    "print('KNN           : ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), KNeighborsClassifier()), x_train, y_train, cv=cv, scoring=scorer)))\n",
    "print('Log-Regression: ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), LogisticRegression(max_iter=1000)), x_train, y_train, cv=cv, scoring=scorer)))\n",
    "print('Linear SVM    : ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), LinearSVC(random_state=0)), x_train, y_train, cv=cv, scoring=scorer)))\n",
    "print('kernal SVM    : ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), SVC(gamma='scale')), x_train, y_train, cv=cv, scoring=scorer)))\n",
    "print('LDA           : ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), lda()), x_train, y_train, cv=cv, scoring=scorer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Classification Scores Comparison with default Parameters **********\n",
      "KNN           :  0.4666666666666668\n",
      "Log-Regression:  0.7333333333333334\n",
      "Linear SVM    :  0.6666666666666667\n",
      "kernal SVM    :  0.4666666666666668\n",
      "LDA           :  0.4666666666666667\n"
     ]
    }
   ],
   "source": [
    "# using all channels, custom csp on 5 overlapping windows of segmented data\n",
    "t_start = np.arange(0,1.6,0.375)\n",
    "length_window = 2.5\n",
    "t_end = t_start + length_window\n",
    "i = 4\n",
    "x_train = x_trainVal\n",
    "print('*'*10, 'Classification Scores Comparison with default Parameters' ,'*'*10)\n",
    "print('KNN           : ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), KNeighborsClassifier()), x_train, y_train, cv=cv, scoring=scorer)))\n",
    "print('Log-Regression: ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), LogisticRegression(max_iter=1000)), x_train, y_train, cv=cv, scoring=scorer)))\n",
    "print('Linear SVM    : ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), LinearSVC(random_state=0)), x_train, y_train, cv=cv, scoring=scorer)))\n",
    "print('kernal SVM    : ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), SVC(gamma='scale')), x_train, y_train, cv=cv, scoring=scorer)))\n",
    "print('LDA           : ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), lda()), x_train, y_train, cv=cv, scoring=scorer)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "- csp_comps = 4 only \n",
    "- CV=10\n",
    "- 5 windows with 2.5s window size and 375 ms displacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Information About Window Selection ----------\n",
      "Total Windows:  5\n",
      "Starting Time of Windows:  [0.    0.375 0.75  1.125 1.5  ]\n",
      "Ending Time of Windows  : [2.5   2.875 3.25  3.625 4.   ]\n"
     ]
    }
   ],
   "source": [
    "print('-'*10, 'Information About Window Selection', '-'*10)\n",
    "t_start = np.arange(0,1.501,0.375)\n",
    "length_window = 2.5\n",
    "t_end = t_start + length_window\n",
    "print('Total Windows: ', len(t_start))\n",
    "print('Starting Time of Windows: ', t_start)\n",
    "print('Ending Time of Windows  :' ,  t_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedShuffleSplit(10, random_state=0)\n",
    "csp_comps = [4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for linear svm\n",
    "param_grid_linear_svm =   { 'linearsvc__C' : np.logspace(-4, 3, 15), \n",
    "                           'custom_segmented_csp__n_components': csp_comps}\n",
    "\n",
    "# lda, auto shrinkage performs pretty well mostly \n",
    "shrinkage = list(np.arange(0,1.01,0.1))\n",
    "shrinkage.append('auto')\n",
    "param_grid_lda = {'custom_segmented_csp__n_components': csp_comps, \n",
    "                  'lineardiscriminantanalysis__shrinkage': shrinkage} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids_linear_svm_list = [GridSearchCV(make_pipeline(Custom_Segmented_CSP(t_start=t_start, t_end=t_end), \n",
    "                                                    LinearSVC(random_state=0)), \n",
    "                               param_grid=param_grid_linear_svm, cv=cv, n_jobs=n_jobs, scoring=scorer)\n",
    "                        for _ in range(len(training_files))]\n",
    "\n",
    "grids_lda_list = [GridSearchCV(make_pipeline(Custom_Segmented_CSP(t_start=t_start, t_end=t_end), lda(solver='eigen')), \n",
    "                        param_grid=param_grid_lda, cv=cv, n_jobs=n_jobs, scoring=scorer)\n",
    "                 for _ in range(len(training_files))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_function(subject_index=0):\n",
    "    # this time training function trains on whole training set\n",
    "    print('-'*25, 'Training for Subject:', subject_index+1, '-'*25)\n",
    "    epochs = epochs_list_train[subject_index]\n",
    "    data = epochs.get_data()\n",
    "    data = data[:,:,256+512:-256] # from 0.5s to 4.5s\n",
    "    labels = epochs.events[:,-1]\n",
    "\n",
    "    grids_linear_svm_list[subject_index].fit(data, labels)\n",
    "    print('LinearSVM: Maximum Cross Validation Score = ', round(grids_linear_svm_list[subject_index].best_score_,3))\n",
    "    grids_lda_list[subject_index].fit(data, labels)\n",
    "    print('LDA      : Maximum Cross Validation Score = ', round(grids_lda_list[subject_index].best_score_,3))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's Training Time\n",
    "with 5 overlapping windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- Training for Subject: 1 -------------------------\n",
      "LinearSVM: Maximum Cross Validation Score =  0.75\n",
      "LDA      : Maximum Cross Validation Score =  0.75\n",
      "\n",
      "------------------------- Training for Subject: 2 -------------------------\n",
      "LinearSVM: Maximum Cross Validation Score =  0.85\n",
      "LDA      : Maximum Cross Validation Score =  0.775\n",
      "\n",
      "------------------------- Training for Subject: 3 -------------------------\n",
      "LinearSVM: Maximum Cross Validation Score =  0.6\n",
      "LDA      : Maximum Cross Validation Score =  0.6\n",
      "\n",
      "------------------------- Training for Subject: 4 -------------------------\n",
      "LinearSVM: Maximum Cross Validation Score =  0.575\n",
      "LDA      : Maximum Cross Validation Score =  0.625\n",
      "\n",
      "------------------------- Training for Subject: 5 -------------------------\n",
      "LinearSVM: Maximum Cross Validation Score =  0.725\n",
      "LDA      : Maximum Cross Validation Score =  0.725\n",
      "\n",
      "------------------------- Training for Subject: 6 -------------------------\n",
      "LinearSVM: Maximum Cross Validation Score =  0.525\n",
      "LDA      : Maximum Cross Validation Score =  0.575\n",
      "\n",
      "------------------------- Training for Subject: 7 -------------------------\n",
      "LinearSVM: Maximum Cross Validation Score =  0.35\n",
      "LDA      : Maximum Cross Validation Score =  0.35\n",
      "\n",
      "------------------------- Training for Subject: 8 -------------------------\n",
      "LinearSVM: Maximum Cross Validation Score =  0.35\n",
      "LDA      : Maximum Cross Validation Score =  0.275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for subject in range(len(training_files)):\n",
    "    training_function(subject)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
