{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clinical BCI Challenge-WCCI2020\n",
    "- [website link](https://sites.google.com/view/bci-comp-wcci/?fbclid=IwAR37WLQ_xNd5qsZvktZCT8XJerHhmVb_bU5HDu69CnO85DE3iF0fs57vQ6M)\n",
    "\n",
    "\n",
    " - [Dataset Link](https://github.com/5anirban9/Clinical-Brain-Computer-Interfaces-Challenge-WCCI-2020-Glasgow)\n",
    " \n",
    " \n",
    " - [Braindecode Tutorial](https://braindecode.org/auto_examples/plot_bcic_iv_2a_moabb_trial.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import braindecode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import mne\n",
    "from scipy.io import loadmat\n",
    "import scipy\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from mne.decoding import CSP\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # to ignore warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False                    # global variable to suppress output display of MNE functions\n",
    "mne.set_log_level(verbose=verbose) # to suppress large info outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose_clf = False # control output of FBCSP function\n",
    "freqs_band = np.linspace(8, 32, 7) # filter bank choice\n",
    "cv = 10\n",
    "train_ratio = 0.75 # 75:25 for trian-valid split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = None  # for multicore parallel processing, set it to 1 if cause memory issues, for full utilization set to -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Conversion to MNE Datatypes\n",
    "[Mike Cohen Tutorials link for EEG Preprocessing](https://www.youtube.com/watch?v=uWB5tjhataY&list=PLn0OLiymPak2gDD-VDA90w9_iGDgOOb2o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_folder = globals()['_dh'][0]  # a hack to get path of current folder in which jupyter file is located\n",
    "data_path = os.path.join(current_folder, 'Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 8, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# although we have evaluation files but their labels aren't opensource so we won't use them here\n",
    "all_files        = glob.glob(data_path + '/*.mat')\n",
    "training_files   = glob.glob(data_path + '/*T.mat')\n",
    "evaluation_files = glob.glob(data_path + '/*E.mat')\n",
    "len(all_files), len(training_files), len(evaluation_files)     # if these return zero,then no file is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we have modified the labels values from [1, 2] to [0, 1] as pytorch \n",
    "# expects labels/classes to be in [0, n_classes-1] format\n",
    "def get_mne_epochs(filepath, verbose=verbose, t_start=2, fs=512, mode='train'):\n",
    "    '''\n",
    "    This function reads the EEG data from .mat file and convert it to MNE-Python Compatible epochs\n",
    "    data structure. It takes data from [0, 8] sec range and return it by setting t = 0 at cue onset\n",
    "    i.e. 3 seconds and dropping first two seconds so the output data is in [-1.0, 5.0] sec range. The\n",
    "    Details can be found in the preprocessing section of the attached document\n",
    "    '''\n",
    "    mat_data = loadmat(filepath) # read .mat file\n",
    "    eeg_data= mat_data['RawEEGData']\n",
    "    idx_start = fs*t_start      \n",
    "    eeg_data = eeg_data[:, :, idx_start:]\n",
    "    event_id = {'left-hand': 0, 'right-hand': 1} # pytorch expects labels in [0, n_classes-1]\n",
    "    channel_names = ['F3', 'FC3', 'C3', 'CP3', 'P3', 'FCz', 'CPz', 'F4', 'FC4', 'C4', 'CP4', 'P4']\n",
    "    info = mne.create_info(ch_names=channel_names, sfreq=fs, ch_types='eeg')\n",
    "    epochs = mne.EpochsArray(eeg_data, info, verbose=verbose, tmin=t_start-3.0)\n",
    "    epochs.set_montage('standard_1020')\n",
    "    epochs.filter(1., None) \n",
    "    epochs.apply_baseline(baseline=(-.250, 0)) # linear baseline correction\n",
    "    \n",
    "    if mode == 'train': # this in only applicable for training data\n",
    "        epochs.event_id = event_id \n",
    "        epochs.events[:,2] = mat_data['Labels'].ravel() - 1    \n",
    "    return epochs \n",
    "\n",
    "def get_labels(filepath):\n",
    "    mat_data = loadmat(filepath) # read .mat file\n",
    "    return mat_data['Labels'].ravel() - 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of EEG Data:  (80, 12, 3072) \t Shape of Labels:  (80,)\n"
     ]
    }
   ],
   "source": [
    "epochs, labels = get_mne_epochs(training_files[0], verbose=verbose), get_labels(training_files[0])\n",
    "data = epochs.get_data()\n",
    "print('Shape of EEG Data: ', data.shape, '\\t Shape of Labels: ', labels.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 1, 1, 1,\n",
       "       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, 0,\n",
       "       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=uint8)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading original data\n",
    "epochs_list_train = []\n",
    "for i in training_files:\n",
    "    epochs_list_train.append(get_mne_epochs(i, verbose=verbose))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Data\n",
    "first 8 for single subject and last 2 are for cross subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_list_eval = []\n",
    "for i in evaluation_files:\n",
    "    epochs_list_eval.append(get_mne_epochs(i, mode='test', verbose=verbose))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Deep Learning with Braindecode "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Preprocessing\n",
    "https://braindecode.org/auto_examples/plot_mne_dataset_example.html\n",
    "\n",
    "https://braindecode.org/auto_examples/plot_bcic_iv_2a_moabb_trial.html\n",
    "\n",
    "Applying NumpyPreproc to mne.epochs give error related to apply_function implementation: https://github.com/braindecode/braindecode/issues/160\n",
    "\n",
    "can modify the description attribute of datasets by manually passing a pandas dataframe/series and then split accordingly\n",
    "\n",
    "the issue of size mismatch means we are using a different shape input as compared to the built-in models, using EEGNet somehow solves it\n",
    "\n",
    "Target 2 is out of bounds: class labels should be [0, num_classes-1]\n",
    "\n",
    "loading and saving skorch model https://skorch.readthedocs.io/en/stable/user/save_load.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.datautil import create_from_mne_epochs\n",
    "\n",
    "# convert epochs to braindecode compatible datastructure \n",
    "# 2sec windows with 0.5sec stride\n",
    "window_size = 1024 #50 # 3072\n",
    "window_stride = 256 # 50\n",
    "\n",
    "windows_datasets = create_from_mne_epochs(\n",
    "    [epochs_list_train[0]], # list of epochs\n",
    "    window_size_samples = window_size,\n",
    "    window_stride_samples = window_stride,\n",
    "    drop_last_window = False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_set = windows_datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 1, 1, 1, 1, 1, 1, 1, 1], dtype=int64)"
      ]
     },
     "execution_count": 185,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.datasets[50].y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = []\n",
    "for i in range(len(train_set.datasets)):\n",
    "    labels.extend(train_set.datasets[i].y)\n",
    "labels = np.array(labels) # subtracting 1 as pytorch nll expects labels to be in [0, n_classes-1]\n",
    "train_set.description = pd.DataFrame(data=labels, columns=['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(360, 360)"
      ]
     },
     "execution_count": 187,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(np.array(labels)==0), sum(np.array(labels)==1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "# train_set.description = pd.DataFrame(data=labels, columns=['labels'])\n",
    "# train_set.description['session'] = None # fill that up later with train/test \n",
    "# df = train_set.description\n",
    "# split_idx = int(len(df)*0.75) # hold-out CV with 75:25 split \n",
    "# df['session'][:split_idx], df['session'][split_idx:] = 'train', 'test'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>labels</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>715</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>716</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>717</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>718</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>719</th>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>720 rows × 1 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     labels\n",
       "0         0\n",
       "1         0\n",
       "2         0\n",
       "3         0\n",
       "4         0\n",
       "..      ...\n",
       "715       0\n",
       "716       0\n",
       "717       0\n",
       "718       0\n",
       "719       0\n",
       "\n",
       "[720 rows x 1 columns]"
      ]
     },
     "execution_count": 188,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_set.description"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([-31.29383482,  87.77848413, 134.99000647,  60.95198251,\n",
       "       -50.06313685])"
      ]
     },
     "execution_count": 189,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "windows_datasets.datasets[0].windows.get_data()[0,0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.datautil.preprocess import exponential_moving_standardize\n",
    "from braindecode.datautil.preprocess import MNEPreproc, NumpyPreproc, preprocess\n",
    "\n",
    "low_cut_hz = 7.  # low cut frequency for filtering\n",
    "high_cut_hz = 32.  # high cut frequency for filtering\n",
    "# Parameters for exponential moving standardization\n",
    "factor_new = 1e-3\n",
    "init_block_size = 1000\n",
    "\n",
    "# using NumpyPreproc with exponential_moving_standardize gave error so \n",
    "# I passed a custom callable to MnePreproc\n",
    "def custom_exp_moving_std_fn(epochs, factor_new=factor_new, init_block_size=init_block_size):\n",
    "    data = epochs.get_data()\n",
    "    for i in range(len(data)):\n",
    "        epochs._data[i] = exponential_moving_standardize(data[i], \n",
    "                        factor_new=factor_new, init_block_size=init_block_size)\n",
    "    return epochs\n",
    "\n",
    "preprocessors = [\n",
    "    # keep only EEG sensors\n",
    "    MNEPreproc(fn='pick_types', eeg=True, meg=False, stim=False),\n",
    "    # convert from volt to microvolt, directly modifying the numpy array, \n",
    "    # I think my data is already in microvolts\n",
    "    # NumpyPreproc(fn=lambda x: x * 1e6),\n",
    "    # bandpass filter\n",
    "    MNEPreproc(fn='filter', l_freq=low_cut_hz, h_freq=high_cut_hz),\n",
    "    # exponential moving standardization\n",
    "    MNEPreproc(fn=custom_exp_moving_std_fn, factor_new=factor_new,\n",
    "        init_block_size=init_block_size)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 191,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1.23328384, 2.57082541, 3.72159681, 4.59938505, 5.14926365])"
      ]
     },
     "execution_count": 191,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "preprocess(windows_datasets, preprocessors)\n",
    "windows_datasets.datasets[0].windows.get_data()[0,0,:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 195,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model\n",
    "import torch\n",
    "from braindecode.util import set_random_seeds\n",
    "from braindecode.models import ShallowFBCSPNet, EEGNetv4\n",
    "\n",
    "cuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "if cuda:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed = 20200220  # random seed to make results reproducible\n",
    "# Set random seed to be able to reproduce results\n",
    "set_random_seeds(seed=seed, cuda=cuda)\n",
    "\n",
    "n_classes=2\n",
    "# Extract number of chans and time steps from dataset\n",
    "n_chans = train_set[0][0].shape[0]\n",
    "input_window_samples = train_set[0][0].shape[1]\n",
    "\n",
    "model = EEGNetv4(\n",
    "    n_chans,\n",
    "    n_classes,\n",
    "    input_window_samples = window_size, #input_window_samples,\n",
    "    final_conv_length='auto',\n",
    ")\n",
    "\n",
    "# Send model to GPU\n",
    "if cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 206,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training time\n",
    "from skorch.callbacks import LRScheduler\n",
    "from skorch.helper import predefined_split\n",
    "from braindecode import EEGClassifier\n",
    "\n",
    "lr = 1 * 0.05 \n",
    "weight_decay = 0.5 * 0.001\n",
    "\n",
    "batch_size = 8 #64\n",
    "n_epochs = 25\n",
    "\n",
    "clf = EEGClassifier(\n",
    "    model,\n",
    "    criterion=torch.nn.NLLLoss,\n",
    "    optimizer=torch.optim.AdamW,\n",
    "    #train_split=predefined_split(train_set),  # using valid_set for validation\n",
    "    optimizer__lr=lr,\n",
    "    optimizer__weight_decay=weight_decay,\n",
    "    batch_size=batch_size,\n",
    "    callbacks=[\n",
    "        \"accuracy\", (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n",
    "    ],\n",
    "    device=device,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 161,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<class 'braindecode.classifier.EEGClassifier'>[initialized](\n",
       "  module_=EEGNetv4(\n",
       "    (ensuredims): Ensure4d()\n",
       "    (dimshuffle): Expression(expression=_transpose_to_b_1_c_0) \n",
       "    (conv_temporal): Conv2d(1, 8, kernel_size=(1, 64), stride=(1, 1), padding=(0, 32), bias=False)\n",
       "    (bnorm_temporal): BatchNorm2d(8, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    (conv_spatial): Conv2dWithConstraint(8, 16, kernel_size=(12, 1), stride=(1, 1), groups=8, bias=False)\n",
       "    (bnorm_1): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    (elu_1): Expression(expression=elu) \n",
       "    (pool_1): AvgPool2d(kernel_size=(1, 4), stride=(1, 4), padding=0)\n",
       "    (drop_1): Dropout(p=0.25, inplace=False)\n",
       "    (conv_separable_depth): Conv2d(16, 16, kernel_size=(1, 16), stride=(1, 1), padding=(0, 8), groups=16, bias=False)\n",
       "    (conv_separable_point): Conv2d(16, 16, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
       "    (bnorm_2): BatchNorm2d(16, eps=0.001, momentum=0.01, affine=True, track_running_stats=True)\n",
       "    (elu_2): Expression(expression=elu) \n",
       "    (pool_2): AvgPool2d(kernel_size=(1, 8), stride=(1, 8), padding=0)\n",
       "    (drop_2): Dropout(p=0.25, inplace=False)\n",
       "    (conv_classifier): Conv2d(16, 2, kernel_size=(1, 32), stride=(1, 1))\n",
       "    (softmax): LogSoftmax(dim=1)\n",
       "    (permute_back): Expression(expression=_transpose_1_0) \n",
       "    (squeeze): Expression(expression=squeeze_final_output) \n",
       "  ),\n",
       ")"
      ]
     },
     "execution_count": 161,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clf"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 207,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.5035\u001b[0m        \u001b[32m1.2641\u001b[0m            \u001b[35m0.5000\u001b[0m        \u001b[31m8.6870\u001b[0m  0.0500  4.6160\n",
      "      2            \u001b[36m0.7674\u001b[0m        \u001b[32m0.7391\u001b[0m            \u001b[35m0.5486\u001b[0m        \u001b[31m3.0782\u001b[0m  0.0498  5.5880\n",
      "      3            \u001b[36m0.8455\u001b[0m        0.8920            0.5347        \u001b[31m1.4123\u001b[0m  0.0491  4.9990\n",
      "      4            0.7552        \u001b[32m0.6417\u001b[0m            0.4722        2.4552  0.0481  5.8080\n",
      "      5            0.7917        0.7602            \u001b[35m0.5694\u001b[0m        \u001b[31m1.3458\u001b[0m  0.0467  5.1710\n",
      "      6            0.7396        \u001b[32m0.4640\u001b[0m            0.5417        1.4634  0.0448  4.8490\n",
      "      7            \u001b[36m0.9375\u001b[0m        \u001b[32m0.4465\u001b[0m            \u001b[35m0.5972\u001b[0m        \u001b[31m0.9277\u001b[0m  0.0427  4.8480\n",
      "      8            0.7517        0.4938            0.5417        1.7603  0.0402  5.5680\n",
      "      9            0.6285        \u001b[32m0.4253\u001b[0m            0.5139        1.7298  0.0375  5.2980\n",
      "     10            0.9219        \u001b[32m0.3934\u001b[0m            0.5694        0.9448  0.0346  5.0440\n",
      "     11            0.7656        \u001b[32m0.3866\u001b[0m            0.4931        1.7612  0.0315  4.9780\n",
      "     12            \u001b[36m0.9410\u001b[0m        \u001b[32m0.3203\u001b[0m            \u001b[35m0.6736\u001b[0m        \u001b[31m0.8166\u001b[0m  0.0283  5.0400\n",
      "     13            0.7535        \u001b[32m0.2514\u001b[0m            0.5625        1.6042  0.0250  4.8530\n",
      "     14            0.8559        0.2584            0.5903        1.6984  0.0217  5.0060\n",
      "     15            \u001b[36m0.9896\u001b[0m        \u001b[32m0.2160\u001b[0m            0.5625        1.1566  0.0185  5.0400\n",
      "     16            0.9618        \u001b[32m0.1931\u001b[0m            0.5625        1.1966  0.0154  5.1540\n",
      "     17            0.9184        0.2108            0.5486        1.6540  0.0125  4.9480\n",
      "     18            0.9097        \u001b[32m0.1494\u001b[0m            0.5417        2.0470  0.0098  4.9320\n",
      "     19            0.9392        \u001b[32m0.1179\u001b[0m            0.4722        1.4672  0.0073  4.9190\n",
      "     20            0.9219        0.1281            0.4792        1.7403  0.0052  4.7410\n",
      "     21            0.9844        0.1227            0.5417        1.3955  0.0033  4.8970\n",
      "     22            \u001b[36m0.9983\u001b[0m        \u001b[32m0.0965\u001b[0m            0.5625        1.3079  0.0019  4.8080\n",
      "     23            0.9965        0.1037            0.5417        1.2993  0.0009  4.8290\n",
      "     24            \u001b[36m1.0000\u001b[0m        0.1243            0.5486        1.3327  0.0002  4.8390\n",
      "     25            1.0000        \u001b[32m0.0912\u001b[0m            0.5347        1.3670  0.0000  4.8480\n"
     ]
    }
   ],
   "source": [
    "clf.fit(train_set, y=train_set.description.labels, epochs=n_epochs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Re-initializing optimizer because the following parameters were re-set: lr, weight_decay.\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.9670\u001b[0m        \u001b[32m0.2535\u001b[0m            \u001b[35m0.8194\u001b[0m        \u001b[31m0.5020\u001b[0m  0.0100  6.0250\n",
      "      2            0.7726        \u001b[32m0.2463\u001b[0m            0.6389        1.1504  0.0100  6.1590\n",
      "      3            0.8733        \u001b[32m0.2108\u001b[0m            0.6458        1.1607  0.0098  5.0160\n",
      "      4            \u001b[36m0.9809\u001b[0m        \u001b[32m0.1826\u001b[0m            0.8056        \u001b[31m0.4239\u001b[0m  0.0096  4.8140\n",
      "      5            \u001b[36m0.9878\u001b[0m        \u001b[32m0.1551\u001b[0m            0.7847        0.6272  0.0093  4.8320\n",
      "      6            \u001b[36m0.9913\u001b[0m        \u001b[32m0.1152\u001b[0m            0.7361        0.7034  0.0090  4.7670\n",
      "      7            \u001b[36m1.0000\u001b[0m        0.1287            0.7778        0.5754  0.0085  4.6110\n",
      "      8            0.9878        0.1213            0.7292        0.9005  0.0080  4.8450\n",
      "      9            0.9913        0.1270            0.7431        0.9025  0.0075  4.9960\n",
      "     10            0.9514        \u001b[32m0.0908\u001b[0m            0.6597        1.1948  0.0069  4.9990\n",
      "     11            0.9965        0.0931            0.7222        0.7489  0.0063  5.0770\n",
      "     12            0.9774        \u001b[32m0.0570\u001b[0m            0.6458        1.0880  0.0057  4.7550\n",
      "     13            0.9983        0.0703            0.7153        0.7835  0.0050  4.7080\n",
      "     14            1.0000        \u001b[32m0.0496\u001b[0m            0.7014        0.7956  0.0043  4.8470\n",
      "     15            1.0000        0.0532            0.7014        0.8332  0.0037  4.7990\n",
      "     16            0.9948        0.0550            0.6458        1.2263  0.0031  4.6820\n",
      "     17            1.0000        0.0529            0.7500        0.7464  0.0025  4.7380\n",
      "     18            1.0000        0.0607            0.7431        0.8714  0.0020  4.7830\n",
      "     19            1.0000        \u001b[32m0.0304\u001b[0m            0.7708        0.7933  0.0015  4.9380\n",
      "     20            1.0000        0.0564            0.7361        0.7929  0.0010  4.8610\n",
      "     21            1.0000        0.0599            0.7431        0.7994  0.0007  4.9890\n",
      "     22            1.0000        0.0346            0.7361        0.7903  0.0004  4.6270\n",
      "     23            1.0000        0.0498            0.7431        0.7906  0.0002  5.6620\n",
      "     24            1.0000        0.0459            0.7431        0.7975  0.0000  5.0620\n",
      "     25            1.0000        0.0308            0.7431        0.8005  0.0000  5.5190\n"
     ]
    }
   ],
   "source": [
    "# Model training for a specified number of epochs. `y` is None as it is already supplied\n",
    "# in the dataset.\n",
    "clf.fit(train_set, y=train_set.description.labels, epochs=n_epochs);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving and loading the model state\n",
    "clf.save_params(f_params='model.pkl', f_optimizer='opt.pkl', f_history='history.json')\n",
    "clf.initialize() # This is important!\n",
    "clf.load_params(f_params='model.pkl', f_optimizer='opt.pkl', f_history='history.json')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's Training Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 306,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.datautil import create_from_mne_epochs\n",
    "\n",
    "# convert epochs to braindecode compatible datastructure \n",
    "# 2sec windows with 0.250 sec stride\n",
    "window_size = 1024 #50 # 3072\n",
    "window_stride = 128 #256 # 50\n",
    "\n",
    "windows_datasets_list = []\n",
    "for epoch in epochs_list_train:\n",
    "    windows_datasets_list.append(\n",
    "            create_from_mne_epochs(\n",
    "            [epoch], # list of epochs\n",
    "            window_size_samples = window_size,\n",
    "            window_stride_samples = window_stride,\n",
    "            drop_last_window = False\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 314,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windows_datasets_labels(windows_dataset):\n",
    "    labels = []\n",
    "    for i in range(len(windows_dataset.datasets)):\n",
    "        labels.extend(windows_dataset.datasets[i].y)\n",
    "    return np.array(labels) \n",
    "\n",
    "for windows_dataset in windows_datasets_list:\n",
    "    windows_dataset.description = pd.DataFrame(data=get_windows_datasets_labels(windows_dataset), \n",
    "                                           columns=['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 322,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Windows in a Single Dataset:  1360\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Windows in a Single Dataset: \", len(windows_datasets_list[0].description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 315,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.datautil.preprocess import exponential_moving_standardize\n",
    "from braindecode.datautil.preprocess import MNEPreproc, NumpyPreproc, preprocess\n",
    "\n",
    "low_cut_hz = 7.  # low cut frequency for filtering\n",
    "high_cut_hz = 32.  # high cut frequency for filtering\n",
    "# Parameters for exponential moving standardization\n",
    "factor_new = 1e-3\n",
    "init_block_size = 1000\n",
    "\n",
    "def custom_exp_moving_std_fn(epochs, factor_new=factor_new, init_block_size=init_block_size):\n",
    "    data = epochs.get_data()\n",
    "    for i in range(len(data)):\n",
    "        epochs._data[i] = exponential_moving_standardize(data[i], \n",
    "                        factor_new=factor_new, init_block_size=init_block_size)\n",
    "    return epochs\n",
    "\n",
    "preprocessors = [\n",
    "    # keep only EEG sensors\n",
    "    MNEPreproc(fn='pick_types', eeg=True, meg=False, stim=False),\n",
    "    # bandpass filter\n",
    "    MNEPreproc(fn='filter', l_freq=low_cut_hz, h_freq=high_cut_hz),\n",
    "    # exponential moving standardization\n",
    "    MNEPreproc(fn=custom_exp_moving_std_fn, factor_new=factor_new,\n",
    "        init_block_size=init_block_size)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 316,
   "metadata": {},
   "outputs": [],
   "source": [
    "for windows_dataset in windows_datasets_list: \n",
    "    preprocess(windows_dataset, preprocessors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 323,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 #64\n",
    "n_epochs = 20 #25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model\n",
    "import torch\n",
    "from braindecode.util import set_random_seeds\n",
    "from braindecode.models import ShallowFBCSPNet, EEGNetv4\n",
    "\n",
    "cuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "if cuda:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed = 20200220  # random seed to make results reproducible\n",
    "# Set random seed to be able to reproduce results\n",
    "set_random_seeds(seed=seed, cuda=cuda)\n",
    "\n",
    "n_classes=2\n",
    "# Extract number of chans and time steps from dataset\n",
    "n_chans = windows_datasets_list[0][0].shape[0]\n",
    "input_window_samples = windows_datasets_list[0][0].shape[1]\n",
    "\n",
    "model = EEGNetv4(\n",
    "    n_chans,\n",
    "    n_classes,\n",
    "    input_window_samples = window_size, #input_window_samples,\n",
    "    final_conv_length='auto',\n",
    ")\n",
    "\n",
    "# Send model to GPU\n",
    "if cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 325,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training time\n",
    "from skorch.callbacks import LRScheduler\n",
    "from skorch.helper import predefined_split\n",
    "from braindecode import EEGClassifier\n",
    "\n",
    "lr = 1 * 0.02 \n",
    "weight_decay = 0.5 * 0.001\n",
    "\n",
    "clfs_list = []\n",
    "for i in range(len(epochs_list_train)):\n",
    "    clfs_list.append(\n",
    "        EEGClassifier(\n",
    "                    model,\n",
    "                    criterion=torch.nn.NLLLoss,\n",
    "                    optimizer=torch.optim.AdamW,\n",
    "                    #train_split=predefined_split(train_set),  # using valid_set for validation\n",
    "                    optimizer__lr=lr,\n",
    "                    optimizer__weight_decay=weight_decay,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[\n",
    "                        \"accuracy\", (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n",
    "                    ],\n",
    "                    device=device,\n",
    "                    )\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 326,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_function(subject_index=0):\n",
    "    print('\\n', '#'*25, 'Training for Subject:', subject_index+1, '#'*25, '\\n')\n",
    "    dataset = windows_datasets_list[subject_index]\n",
    "    clfs_list[subject_index].fit(dataset, y=dataset.description.labels, epochs=n_epochs);\n",
    "    best_validation_acc = clfs_list[subject_index].callbacks_[4][1].best_score_ # a hack to get best validation accuracy\n",
    "    best_validation_kappa = (2*best_validation_acc)-1\n",
    "    print(\"Best Cross Validation Kappa Score: {:.2f}\".format(best_validation_kappa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 327,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ######################### Training for Subject: 1 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr      dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  -------\n",
      "      1            \u001b[36m0.5846\u001b[0m        \u001b[32m0.4500\u001b[0m            \u001b[35m0.5000\u001b[0m        \u001b[31m3.1665\u001b[0m  0.0200  10.7910\n",
      "      2            \u001b[36m0.6471\u001b[0m        \u001b[32m0.2812\u001b[0m            0.5000        3.4927  0.0199  9.6790\n",
      "      3            \u001b[36m0.7849\u001b[0m        0.3008            \u001b[35m0.5956\u001b[0m        \u001b[31m1.0324\u001b[0m  0.0195  9.9190\n",
      "      4            \u001b[36m0.8355\u001b[0m        0.2879            0.5809        \u001b[31m0.9754\u001b[0m  0.0188  8.3600\n",
      "      5            \u001b[36m0.9862\u001b[0m        \u001b[32m0.2319\u001b[0m            \u001b[35m0.6250\u001b[0m        1.0986  0.0179  8.6610\n",
      "      6            0.7932        \u001b[32m0.2257\u001b[0m            0.5515        2.3600  0.0168  8.1550\n",
      "      7            0.6406        0.2371            0.5110        2.2768  0.0155  8.1230\n",
      "      8            0.9301        0.2629            0.5074        1.3901  0.0140  7.9880\n",
      "      9            0.7914        \u001b[32m0.1899\u001b[0m            0.5404        2.6097  0.0125  8.0070\n",
      "     10            0.9577        \u001b[32m0.1797\u001b[0m            0.5478        1.4728  0.0108  8.8980\n",
      "     11            0.9807        \u001b[32m0.1563\u001b[0m            0.5882        1.3104  0.0092  8.0110\n",
      "     12            \u001b[36m0.9991\u001b[0m        \u001b[32m0.1156\u001b[0m            0.5331        1.4042  0.0075  8.4500\n",
      "     13            0.9991        \u001b[32m0.1071\u001b[0m            0.5294        1.5888  0.0060  7.9900\n",
      "     14            0.9982        0.1136            0.5037        1.6383  0.0045  7.9330\n",
      "     15            0.9972        \u001b[32m0.1024\u001b[0m            0.5037        1.6843  0.0032  8.1180\n",
      "     16            0.9982        \u001b[32m0.0916\u001b[0m            0.5184        1.6475  0.0021  7.9590\n",
      "     17            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0896\u001b[0m            0.5294        1.6750  0.0012  7.9810\n",
      "     18            1.0000        0.0904            0.5368        1.6889  0.0005  7.9440\n",
      "     19            1.0000        \u001b[32m0.0709\u001b[0m            0.5257        1.7208  0.0001  7.9600\n",
      "     20            1.0000        0.0836            0.5074        1.7440  0.0000  8.0760\n",
      "Best Cross Validation Kappa Score: 0.25\n",
      "\n",
      " ######################### Training for Subject: 2 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.6204\u001b[0m        \u001b[32m0.4899\u001b[0m            \u001b[35m0.5478\u001b[0m        \u001b[31m1.3166\u001b[0m  0.0200  7.9730\n",
      "      2            \u001b[36m0.9449\u001b[0m        \u001b[32m0.3197\u001b[0m            \u001b[35m0.8640\u001b[0m        \u001b[31m0.2684\u001b[0m  0.0199  9.1400\n",
      "      3            0.7610        \u001b[32m0.2641\u001b[0m            0.6544        0.8394  0.0195  8.5490\n",
      "      4            0.7325        \u001b[32m0.2293\u001b[0m            0.6250        0.8991  0.0188  7.9060\n",
      "      5            \u001b[36m0.9494\u001b[0m        0.2408            0.8566        0.3196  0.0179  7.8660\n",
      "      6            0.6645        0.2493            0.6360        1.8861  0.0168  7.9220\n",
      "      7            0.7932        \u001b[32m0.1891\u001b[0m            0.7243        1.0522  0.0155  8.1480\n",
      "      8            0.9062        \u001b[32m0.1846\u001b[0m            0.7390        0.5820  0.0140  10.4180\n",
      "      9            0.9476        \u001b[32m0.1490\u001b[0m            0.8419        0.4136  0.0125  9.5250\n",
      "     10            \u001b[36m0.9770\u001b[0m        0.1738            0.8566        0.2707  0.0108  9.5170\n",
      "     11            \u001b[36m0.9853\u001b[0m        \u001b[32m0.1095\u001b[0m            \u001b[35m0.9007\u001b[0m        \u001b[31m0.2037\u001b[0m  0.0092  9.4510\n",
      "     12            0.9614        0.1321            0.8015        0.3849  0.0075  8.4760\n",
      "     13            \u001b[36m0.9963\u001b[0m        \u001b[32m0.1002\u001b[0m            0.8787        0.2349  0.0060  11.4600\n",
      "     14            0.9945        \u001b[32m0.0890\u001b[0m            0.8897        0.2321  0.0045  9.6380\n",
      "     15            0.9917        0.1042            0.8750        0.3054  0.0032  7.9870\n",
      "     16            0.9945        \u001b[32m0.0590\u001b[0m            0.8493        0.3258  0.0021  7.8290\n",
      "     17            \u001b[36m0.9972\u001b[0m        0.0661            0.8824        0.3003  0.0012  8.1830\n",
      "     18            \u001b[36m1.0000\u001b[0m        0.0855            0.8676        0.2703  0.0005  7.7840\n",
      "     19            1.0000        0.0723            0.8676        0.2703  0.0001  7.7520\n",
      "     20            1.0000        0.0769            0.8676        0.2754  0.0000  7.8470\n",
      "Best Cross Validation Kappa Score: 0.80\n",
      "\n",
      " ######################### Training for Subject: 3 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.5129\u001b[0m        \u001b[32m0.6273\u001b[0m            \u001b[35m0.8824\u001b[0m        \u001b[31m0.3664\u001b[0m  0.0200  7.7520\n",
      "      2            \u001b[36m0.7252\u001b[0m        \u001b[32m0.3286\u001b[0m            0.8640        \u001b[31m0.3651\u001b[0m  0.0199  7.7600\n",
      "      3            \u001b[36m0.9274\u001b[0m        \u001b[32m0.2291\u001b[0m            0.8713        0.4488  0.0195  7.8400\n",
      "      4            0.7344        0.2403            0.8787        \u001b[31m0.3004\u001b[0m  0.0188  8.5350\n",
      "      5            0.7022        0.2760            0.5147        2.8627  0.0179  7.7920\n",
      "      6            0.8502        \u001b[32m0.1649\u001b[0m            0.6287        1.3026  0.0168  7.7670\n",
      "      7            0.6195        0.1898            0.5000        5.1595  0.0155  7.8010\n",
      "      8            0.9237        \u001b[32m0.1605\u001b[0m            0.7757        0.6694  0.0140  7.7770\n",
      "      9            0.8355        0.1713            0.7426        0.9748  0.0125  7.7400\n",
      "     10            \u001b[36m0.9770\u001b[0m        \u001b[32m0.1545\u001b[0m            0.7132        0.9660  0.0108  7.7440\n",
      "     11            \u001b[36m0.9945\u001b[0m        \u001b[32m0.1127\u001b[0m            0.6213        2.0490  0.0092  7.7800\n",
      "     12            0.9531        0.1291            0.7684        0.8129  0.0075  7.7630\n",
      "     13            0.9917        0.1157            0.6213        2.2978  0.0060  7.6970\n",
      "     14            \u001b[36m0.9954\u001b[0m        \u001b[32m0.1041\u001b[0m            0.6103        2.1495  0.0045  7.7840\n",
      "     15            \u001b[36m0.9982\u001b[0m        \u001b[32m0.0965\u001b[0m            0.5956        2.2745  0.0032  7.9110\n",
      "     16            \u001b[36m0.9991\u001b[0m        \u001b[32m0.0623\u001b[0m            0.6287        2.0254  0.0021  7.9010\n",
      "     17            0.9982        0.0649            0.6691        1.7069  0.0012  7.7000\n",
      "     18            0.9991        0.0664            0.6765        1.5186  0.0005  7.7260\n",
      "     19            0.9991        0.0784            0.6801        1.5044  0.0001  7.7370\n",
      "     20            0.9991        \u001b[32m0.0579\u001b[0m            0.6801        1.5044  0.0000  7.7940\n",
      "Best Cross Validation Kappa Score: 0.76\n",
      "\n",
      " ######################### Training for Subject: 4 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.5579\u001b[0m        \u001b[32m0.4359\u001b[0m            \u001b[35m0.5000\u001b[0m        \u001b[31m4.5564\u001b[0m  0.0200  8.2210\n",
      "      2            \u001b[36m0.8006\u001b[0m        \u001b[32m0.2745\u001b[0m            \u001b[35m0.7463\u001b[0m        \u001b[31m0.6882\u001b[0m  0.0199  8.4730\n",
      "      3            \u001b[36m0.8998\u001b[0m        \u001b[32m0.2420\u001b[0m            0.5441        2.2729  0.0195  7.9830\n",
      "      4            0.6884        \u001b[32m0.2027\u001b[0m            0.7463        0.6928  0.0188  8.1270\n",
      "      5            \u001b[36m0.9697\u001b[0m        \u001b[32m0.1823\u001b[0m            0.6875        1.3015  0.0179  7.8250\n",
      "      6            0.8447        \u001b[32m0.1786\u001b[0m            0.7096        0.6977  0.0168  7.7580\n",
      "      7            0.9522        \u001b[32m0.1509\u001b[0m            0.7022        1.1236  0.0155  7.7460\n",
      "      8            0.9118        0.1794            0.6728        1.1485  0.0140  7.8760\n",
      "      9            \u001b[36m0.9761\u001b[0m        0.1579            0.5809        2.3490  0.0125  7.8550\n",
      "     10            0.9614        0.1678            0.6728        0.9630  0.0108  7.7580\n",
      "     11            \u001b[36m0.9936\u001b[0m        \u001b[32m0.1149\u001b[0m            0.6618        1.4225  0.0092  7.8400\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     12            0.9936        0.1382            0.5919        2.0714  0.0075  7.7050\n",
      "     13            \u001b[36m0.9972\u001b[0m        \u001b[32m0.0787\u001b[0m            0.5993        2.1698  0.0060  7.7630\n",
      "     14            0.9779        0.0924            0.5882        2.8224  0.0045  7.7300\n",
      "     15            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0744\u001b[0m            0.6471        2.0356  0.0032  7.7360\n",
      "     16            1.0000        \u001b[32m0.0602\u001b[0m            0.6434        2.0673  0.0021  7.7760\n",
      "     17            1.0000        0.0629            0.6360        2.3021  0.0012  8.2210\n",
      "     18            1.0000        0.0846            0.6324        2.4866  0.0005  7.8140\n",
      "     19            1.0000        \u001b[32m0.0531\u001b[0m            0.6324        2.4980  0.0001  7.7970\n",
      "     20            1.0000        0.0786            0.6324        2.4779  0.0000  7.7300\n",
      "Best Cross Validation Kappa Score: 0.49\n",
      "\n",
      " ######################### Training for Subject: 5 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr      dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  -------\n",
      "      1            \u001b[36m0.8355\u001b[0m        \u001b[32m0.5801\u001b[0m            \u001b[35m0.5478\u001b[0m        \u001b[31m1.4411\u001b[0m  0.0200  10.1270\n",
      "      2            0.7812        \u001b[32m0.4103\u001b[0m            \u001b[35m0.6691\u001b[0m        \u001b[31m0.6985\u001b[0m  0.0199  9.3500\n",
      "      3            \u001b[36m0.8483\u001b[0m        \u001b[32m0.3583\u001b[0m            0.6176        1.0101  0.0195  8.3350\n",
      "      4            0.5267        \u001b[32m0.2979\u001b[0m            0.5074        1.8545  0.0188  7.7570\n",
      "      5            \u001b[36m0.9577\u001b[0m        0.3418            \u001b[35m0.6728\u001b[0m        0.7626  0.0179  7.7200\n",
      "      6            0.8787        \u001b[32m0.2852\u001b[0m            0.6434        0.8143  0.0168  8.2380\n",
      "      7            0.5322        \u001b[32m0.2778\u001b[0m            0.5257        2.0729  0.0155  8.7860\n",
      "      8            0.9292        \u001b[32m0.2392\u001b[0m            0.6471        1.3155  0.0140  10.5760\n",
      "      9            \u001b[36m0.9761\u001b[0m        0.2665            \u001b[35m0.7132\u001b[0m        \u001b[31m0.6213\u001b[0m  0.0125  10.1110\n",
      "     10            0.8713        \u001b[32m0.1881\u001b[0m            0.6213        0.8953  0.0108  10.0170\n",
      "     11            0.5404        \u001b[32m0.1586\u001b[0m            0.5147        2.5170  0.0092  12.6350\n",
      "     12            \u001b[36m0.9789\u001b[0m        \u001b[32m0.1377\u001b[0m            0.7096        0.7198  0.0075  8.4780\n",
      "     13            \u001b[36m0.9917\u001b[0m        0.1469            \u001b[35m0.7243\u001b[0m        0.6230  0.0060  8.6740\n",
      "     14            \u001b[36m0.9945\u001b[0m        \u001b[32m0.1113\u001b[0m            \u001b[35m0.7279\u001b[0m        0.7271  0.0045  8.6870\n",
      "     15            0.9697        \u001b[32m0.0933\u001b[0m            0.6728        0.8350  0.0032  9.8092\n",
      "     16            \u001b[36m0.9954\u001b[0m        \u001b[32m0.0817\u001b[0m            0.7059        0.8541  0.0021  9.5767\n",
      "     17            \u001b[36m0.9991\u001b[0m        0.0847            \u001b[35m0.7353\u001b[0m        0.7488  0.0012  8.9134\n",
      "     18            0.9972        0.0939            \u001b[35m0.7610\u001b[0m        0.6916  0.0005  7.9988\n",
      "     19            0.9982        0.1033            0.7426        0.7286  0.0001  7.7429\n",
      "     20            0.9991        0.0894            0.7316        0.7449  0.0000  8.1660\n",
      "Best Cross Validation Kappa Score: 0.52\n",
      "\n",
      " ######################### Training for Subject: 6 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.8915\u001b[0m        \u001b[32m0.4526\u001b[0m            \u001b[35m0.7537\u001b[0m        \u001b[31m0.4743\u001b[0m  0.0200  8.9860\n",
      "      2            \u001b[36m0.9651\u001b[0m        \u001b[32m0.2894\u001b[0m            0.7537        \u001b[31m0.4441\u001b[0m  0.0199  7.8030\n",
      "      3            0.9182        \u001b[32m0.2332\u001b[0m            0.6691        0.7344  0.0195  7.6670\n",
      "      4            0.6149        0.2551            0.5846        1.3945  0.0188  8.1160\n",
      "      5            0.9090        \u001b[32m0.2064\u001b[0m            0.7132        0.6618  0.0179  7.7620\n",
      "      6            \u001b[36m0.9743\u001b[0m        \u001b[32m0.1604\u001b[0m            \u001b[35m0.7904\u001b[0m        0.4978  0.0168  8.0160\n",
      "      7            0.5754        \u001b[32m0.1588\u001b[0m            0.5000        3.1275  0.0155  7.7670\n",
      "      8            0.6949        0.1643            0.5074        2.5988  0.0140  7.7230\n",
      "      9            0.6893        \u001b[32m0.1488\u001b[0m            0.5441        2.4300  0.0125  7.8600\n",
      "     10            0.9632        \u001b[32m0.1425\u001b[0m            \u001b[35m0.7941\u001b[0m        0.5447  0.0108  7.7420\n",
      "     11            \u001b[36m0.9835\u001b[0m        \u001b[32m0.1275\u001b[0m            0.6949        0.8652  0.0092  7.7390\n",
      "     12            \u001b[36m0.9890\u001b[0m        \u001b[32m0.1199\u001b[0m            0.6765        0.9320  0.0075  8.0890\n",
      "     13            \u001b[36m0.9899\u001b[0m        \u001b[32m0.0903\u001b[0m            0.7206        0.9065  0.0060  8.3670\n",
      "     14            \u001b[36m0.9963\u001b[0m        0.0971            0.7279        0.7861  0.0045  8.2570\n",
      "     15            0.8686        \u001b[32m0.0773\u001b[0m            0.5588        2.4626  0.0032  7.8730\n",
      "     16            0.9954        0.0787            0.6838        0.9809  0.0021  7.7850\n",
      "     17            0.9926        \u001b[32m0.0573\u001b[0m            0.7904        0.6128  0.0012  7.8950\n",
      "     18            0.9926        0.0770            0.7831        0.5937  0.0005  7.7830\n",
      "     19            0.9917        0.0782            0.7794        0.6173  0.0001  7.7420\n",
      "     20            0.9926        0.0681            0.7721        0.6893  0.0000  7.7590\n",
      "Best Cross Validation Kappa Score: 0.59\n",
      "\n",
      " ######################### Training for Subject: 7 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.7757\u001b[0m        \u001b[32m0.7793\u001b[0m            \u001b[35m0.6875\u001b[0m        \u001b[31m0.6033\u001b[0m  0.0200  8.1190\n",
      "      2            \u001b[36m0.8327\u001b[0m        \u001b[32m0.5018\u001b[0m            0.6103        0.7700  0.0199  7.8140\n",
      "      3            \u001b[36m0.8869\u001b[0m        \u001b[32m0.4722\u001b[0m            0.6176        0.8114  0.0195  7.8130\n",
      "      4            0.8649        \u001b[32m0.3578\u001b[0m            \u001b[35m0.7537\u001b[0m        \u001b[31m0.5222\u001b[0m  0.0188  8.0210\n",
      "      5            0.5165        0.3618            0.5000        2.3294  0.0179  7.7410\n",
      "      6            0.6324        \u001b[32m0.2667\u001b[0m            0.5000        1.3755  0.0168  7.8390\n",
      "      7            0.6250        0.2874            0.6801        0.7654  0.0155  7.8460\n",
      "      8            \u001b[36m0.9053\u001b[0m        \u001b[32m0.2495\u001b[0m            0.6471        0.7592  0.0140  7.9200\n",
      "      9            \u001b[36m0.9458\u001b[0m        \u001b[32m0.2263\u001b[0m            0.6544        0.9176  0.0125  7.8350\n",
      "     10            \u001b[36m0.9614\u001b[0m        \u001b[32m0.1818\u001b[0m            0.6838        0.8948  0.0108  8.3810\n",
      "     11            0.9274        0.1831            0.7243        0.5929  0.0092  7.7730\n",
      "     12            0.8263        \u001b[32m0.1605\u001b[0m            0.6507        0.8667  0.0075  7.7650\n",
      "     13            \u001b[36m0.9862\u001b[0m        \u001b[32m0.1173\u001b[0m            0.7463        0.5707  0.0060  7.7780\n",
      "     14            0.9458        \u001b[32m0.1120\u001b[0m            0.7132        0.6914  0.0045  8.1380\n",
      "     15            0.9485        \u001b[32m0.0979\u001b[0m            0.6360        1.7352  0.0032  7.7630\n",
      "     16            \u001b[36m0.9972\u001b[0m        0.1269            0.7022        0.7707  0.0021  7.9210\n",
      "     17            0.9853        \u001b[32m0.0846\u001b[0m            0.6654        1.4401  0.0012  7.7240\n",
      "     18            \u001b[36m0.9982\u001b[0m        \u001b[32m0.0780\u001b[0m            0.6985        0.8801  0.0005  7.7740\n",
      "     19            0.9982        0.0849            0.6949        0.8542  0.0001  7.9390\n",
      "     20            0.9982        \u001b[32m0.0699\u001b[0m            0.6985        0.9041  0.0000  7.7800\n",
      "Best Cross Validation Kappa Score: 0.51\n",
      "\n",
      " ######################### Training for Subject: 8 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.5285\u001b[0m        \u001b[32m0.8323\u001b[0m            \u001b[35m0.5000\u001b[0m        \u001b[31m1.8205\u001b[0m  0.0200  8.3540\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2            \u001b[36m0.6783\u001b[0m        \u001b[32m0.5039\u001b[0m            \u001b[35m0.5882\u001b[0m        \u001b[31m0.9278\u001b[0m  0.0199  9.1400\n",
      "      3            \u001b[36m0.7987\u001b[0m        \u001b[32m0.4400\u001b[0m            \u001b[35m0.6324\u001b[0m        \u001b[31m0.7870\u001b[0m  0.0195  7.7520\n",
      "      4            \u001b[36m0.8392\u001b[0m        \u001b[32m0.4353\u001b[0m            \u001b[35m0.6507\u001b[0m        0.7998  0.0188  7.7070\n",
      "      5            \u001b[36m0.8465\u001b[0m        \u001b[32m0.3567\u001b[0m            \u001b[35m0.6691\u001b[0m        \u001b[31m0.7436\u001b[0m  0.0179  7.7840\n",
      "      6            \u001b[36m0.9357\u001b[0m        \u001b[32m0.3241\u001b[0m            \u001b[35m0.6985\u001b[0m        \u001b[31m0.7387\u001b[0m  0.0168  7.7670\n",
      "      7            0.6121        0.3307            0.5551        1.6985  0.0155  7.8750\n",
      "      8            \u001b[36m0.9412\u001b[0m        \u001b[32m0.2459\u001b[0m            \u001b[35m0.7500\u001b[0m        \u001b[31m0.6240\u001b[0m  0.0140  7.7620\n",
      "      9            0.7941        0.2543            0.6066        1.6199  0.0125  7.7360\n",
      "     10            0.8732        \u001b[32m0.2328\u001b[0m            0.6507        1.1218  0.0108  7.7840\n",
      "     11            0.5368        \u001b[32m0.1765\u001b[0m            0.5037        3.4215  0.0092  7.7130\n",
      "     12            \u001b[36m0.9550\u001b[0m        \u001b[32m0.1741\u001b[0m            0.6728        1.0528  0.0075  7.7050\n",
      "     13            \u001b[36m0.9871\u001b[0m        \u001b[32m0.1631\u001b[0m            0.6654        0.9220  0.0060  7.7160\n",
      "     14            0.8290        \u001b[32m0.1206\u001b[0m            0.5625        1.8090  0.0045  7.7590\n",
      "     15            \u001b[36m0.9972\u001b[0m        0.1339            0.6875        1.0011  0.0032  8.3920\n",
      "     16            0.9871        0.1251            0.6838        0.8872  0.0021  7.8410\n",
      "     17            0.9972        \u001b[32m0.0838\u001b[0m            0.6544        1.0605  0.0012  7.7660\n",
      "     18            \u001b[36m0.9982\u001b[0m        0.0981            0.6544        1.0915  0.0005  7.8350\n",
      "     19            0.9982        0.1088            0.6618        1.0382  0.0001  7.7690\n",
      "     20            0.9982        0.1088            0.6618        1.0412  0.0000  7.7360\n",
      "Best Cross Validation Kappa Score: 0.50\n"
     ]
    }
   ],
   "source": [
    "for subject in range(len(training_files)):\n",
    "    training_function(subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's Training Time with 0.5, 4.5 sec only"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.datautil import create_from_mne_epochs\n",
    "\n",
    "# convert epochs to braindecode compatible datastructure \n",
    "# 2sec windows with 0.250 sec stride\n",
    "window_size = 1024 #50 # 3072\n",
    "window_stride = 128 #256 # 50\n",
    "\n",
    "windows_datasets_list = []\n",
    "for epoch in epochs_list_train:\n",
    "    windows_datasets_list.append(\n",
    "            create_from_mne_epochs(\n",
    "            [epoch.crop(tmin=0.5, tmax=4.5, include_tmax=False)], # [0.5, 4.5] s, expects list of epochs\n",
    "            window_size_samples = window_size,\n",
    "            window_stride_samples = window_stride,\n",
    "            drop_last_window = False\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windows_datasets_labels(windows_dataset):\n",
    "    labels = []\n",
    "    for i in range(len(windows_dataset.datasets)):\n",
    "        labels.extend(windows_dataset.datasets[i].y)\n",
    "    return np.array(labels) \n",
    "\n",
    "for windows_dataset in windows_datasets_list:\n",
    "    windows_dataset.description = pd.DataFrame(data=get_windows_datasets_labels(windows_dataset), \n",
    "                                           columns=['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Windows in a Single Dataset:  720\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Windows in a Single Dataset: \", len(windows_datasets_list[0].description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.datautil.preprocess import exponential_moving_standardize\n",
    "from braindecode.datautil.preprocess import MNEPreproc, NumpyPreproc, preprocess\n",
    "\n",
    "low_cut_hz = 7.  # low cut frequency for filtering\n",
    "high_cut_hz = 32.  # high cut frequency for filtering\n",
    "# Parameters for exponential moving standardization\n",
    "factor_new = 1e-3\n",
    "init_block_size = 1000\n",
    "\n",
    "def custom_exp_moving_std_fn(epochs, factor_new=factor_new, init_block_size=init_block_size):\n",
    "    data = epochs.get_data()\n",
    "    for i in range(len(data)):\n",
    "        epochs._data[i] = exponential_moving_standardize(data[i], \n",
    "                        factor_new=factor_new, init_block_size=init_block_size)\n",
    "    return epochs\n",
    "\n",
    "preprocessors = [\n",
    "    # keep only EEG sensors\n",
    "    MNEPreproc(fn='pick_types', eeg=True, meg=False, stim=False),\n",
    "    # bandpass filter\n",
    "    MNEPreproc(fn='filter', l_freq=low_cut_hz, h_freq=high_cut_hz),\n",
    "    # exponential moving standardization\n",
    "    MNEPreproc(fn=custom_exp_moving_std_fn, factor_new=factor_new,\n",
    "        init_block_size=init_block_size)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "for windows_dataset in windows_datasets_list: \n",
    "    preprocess(windows_dataset, preprocessors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 #64\n",
    "n_epochs = 20 #25"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model\n",
    "import torch\n",
    "from braindecode.util import set_random_seeds\n",
    "from braindecode.models import ShallowFBCSPNet, EEGNetv4\n",
    "\n",
    "cuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "if cuda:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed = 20200220  # random seed to make results reproducible\n",
    "# Set random seed to be able to reproduce results\n",
    "set_random_seeds(seed=seed, cuda=cuda)\n",
    "\n",
    "n_classes=2\n",
    "# Extract number of chans and time steps from dataset\n",
    "n_chans = windows_datasets_list[0][0][0].shape[0]\n",
    "input_window_samples = windows_datasets_list[0][0][0].shape[1]\n",
    "\n",
    "model = EEGNetv4(\n",
    "    n_chans,\n",
    "    n_classes,\n",
    "    input_window_samples = window_size, #input_window_samples,\n",
    "    final_conv_length='auto',\n",
    ")\n",
    "\n",
    "# Send model to GPU\n",
    "if cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training time\n",
    "from skorch.callbacks import LRScheduler\n",
    "from skorch.helper import predefined_split\n",
    "from braindecode import EEGClassifier\n",
    "\n",
    "lr = 1 * 0.02 \n",
    "weight_decay = 0.5 * 0.001\n",
    "\n",
    "clfs_list = []\n",
    "for i in range(len(epochs_list_train)):\n",
    "    clfs_list.append(\n",
    "        EEGClassifier(\n",
    "                    model,\n",
    "                    criterion=torch.nn.NLLLoss,\n",
    "                    optimizer=torch.optim.AdamW,\n",
    "                    #train_split=predefined_split(train_set),  # using valid_set for validation\n",
    "                    optimizer__lr=lr,\n",
    "                    optimizer__weight_decay=weight_decay,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[\n",
    "                        \"accuracy\", (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n",
    "                    ],\n",
    "                    device=device,\n",
    "                    )\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_function(subject_index=0):\n",
    "    print('\\n', '#'*25, 'Training for Subject:', subject_index+1, '#'*25, '\\n')\n",
    "    dataset = windows_datasets_list[subject_index]\n",
    "    clfs_list[subject_index].fit(dataset, y=dataset.description.labels, epochs=n_epochs);\n",
    "    best_validation_acc = clfs_list[subject_index].callbacks_[4][1].best_score_ # a hack to get best validation accuracy\n",
    "    best_validation_kappa = (2*best_validation_acc)-1\n",
    "    print(\"Best Cross Validation Kappa Score: {:.2f}\".format(best_validation_kappa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ######################### Training for Subject: 1 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.5000\u001b[0m        \u001b[32m0.8045\u001b[0m            \u001b[35m0.5000\u001b[0m        \u001b[31m4.2305\u001b[0m  0.0200  7.0333\n",
      "      2            0.5000        \u001b[32m0.5399\u001b[0m            0.5000        5.3087  0.0199  5.9203\n",
      "      3            \u001b[36m0.6840\u001b[0m        \u001b[32m0.3828\u001b[0m            \u001b[35m0.5069\u001b[0m        \u001b[31m3.6704\u001b[0m  0.0195  5.2713\n",
      "      4            0.6753        \u001b[32m0.2885\u001b[0m            \u001b[35m0.5139\u001b[0m        3.8709  0.0188  4.7727\n",
      "      5            \u001b[36m0.8490\u001b[0m        0.3193            \u001b[35m0.6181\u001b[0m        \u001b[31m1.6126\u001b[0m  0.0179  4.7089\n",
      "      6            0.7344        \u001b[32m0.2852\u001b[0m            0.5278        2.9063  0.0168  4.6951\n",
      "      7            \u001b[36m0.9306\u001b[0m        \u001b[32m0.2309\u001b[0m            \u001b[35m0.6250\u001b[0m        \u001b[31m1.2113\u001b[0m  0.0155  4.7143\n",
      "      8            0.8056        \u001b[32m0.1931\u001b[0m            0.5417        2.3123  0.0140  5.2160\n",
      "      9            0.8941        0.2017            0.5903        1.4570  0.0125  4.5880\n",
      "     10            \u001b[36m0.9531\u001b[0m        \u001b[32m0.1512\u001b[0m            0.5903        1.3504  0.0108  4.9369\n",
      "     11            0.6997        0.1541            0.4931        2.6977  0.0092  4.7991\n",
      "     12            0.5260        \u001b[32m0.1291\u001b[0m            0.5000        4.5102  0.0075  4.6558\n",
      "     13            0.9028        \u001b[32m0.1120\u001b[0m            0.5139        1.5136  0.0060  4.6335\n",
      "     14            0.9271        \u001b[32m0.0906\u001b[0m            0.5417        1.4765  0.0045  4.8043\n",
      "     15            \u001b[36m0.9913\u001b[0m        \u001b[32m0.0834\u001b[0m            0.5833        \u001b[31m1.2065\u001b[0m  0.0032  4.6345\n",
      "     16            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0642\u001b[0m            0.5833        \u001b[31m1.1264\u001b[0m  0.0021  4.7055\n",
      "     17            0.9948        \u001b[32m0.0563\u001b[0m            0.5833        1.2353  0.0012  4.7044\n",
      "     18            1.0000        0.0664            0.5625        1.2194  0.0005  4.7791\n",
      "     19            1.0000        \u001b[32m0.0514\u001b[0m            0.5556        1.2462  0.0001  4.9641\n",
      "     20            1.0000        0.0845            0.5625        1.2634  0.0000  4.7306\n",
      "Best Cross Validation Kappa Score: 0.25\n",
      "\n",
      " ######################### Training for Subject: 2 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.9306\u001b[0m        \u001b[32m0.4402\u001b[0m            \u001b[35m0.8472\u001b[0m        \u001b[31m0.4759\u001b[0m  0.0200  4.8944\n",
      "      2            0.7431        \u001b[32m0.2914\u001b[0m            0.7083        2.0095  0.0199  4.9506\n",
      "      3            \u001b[36m0.9774\u001b[0m        \u001b[32m0.2899\u001b[0m            0.8056        0.5823  0.0195  4.6124\n",
      "      4            0.9201        \u001b[32m0.1837\u001b[0m            0.7222        0.7456  0.0188  4.5803\n",
      "      5            0.9514        \u001b[32m0.1334\u001b[0m            0.6736        0.7452  0.0179  5.3293\n",
      "      6            0.9115        0.2526            0.8194        1.0664  0.0168  4.8036\n",
      "      7            \u001b[36m0.9896\u001b[0m        \u001b[32m0.0990\u001b[0m            0.8264        0.5291  0.0155  4.5878\n",
      "      8            0.9236        \u001b[32m0.0948\u001b[0m            0.7847        1.1468  0.0140  4.5809\n",
      "      9            0.9826        0.1036            0.8403        \u001b[31m0.3329\u001b[0m  0.0125  4.5919\n",
      "     10            0.9653        \u001b[32m0.0862\u001b[0m            0.6736        0.6906  0.0108  4.6587\n",
      "     11            0.9826        \u001b[32m0.0787\u001b[0m            0.7431        0.6128  0.0092  4.7036\n",
      "     12            \u001b[36m0.9965\u001b[0m        \u001b[32m0.0493\u001b[0m            \u001b[35m0.8611\u001b[0m        \u001b[31m0.2948\u001b[0m  0.0075  4.7140\n",
      "     13            0.9792        \u001b[32m0.0454\u001b[0m            0.7292        0.6424  0.0060  4.7760\n",
      "     14            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0380\u001b[0m            0.8264        0.5501  0.0045  5.1792\n",
      "     15            1.0000        0.0402            0.8194        0.4451  0.0032  5.0353\n",
      "     16            1.0000        0.0445            0.8542        0.5691  0.0021  4.6552\n",
      "     17            1.0000        0.0497            0.8542        0.5105  0.0012  4.6586\n",
      "     18            1.0000        0.0501            0.8542        0.5233  0.0005  4.6992\n",
      "     19            1.0000        \u001b[32m0.0212\u001b[0m            0.8403        0.5020  0.0001  4.5535\n",
      "     20            1.0000        0.0322            0.8403        0.5015  0.0000  4.6166\n",
      "Best Cross Validation Kappa Score: 0.72\n",
      "\n",
      " ######################### Training for Subject: 3 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.7240\u001b[0m        \u001b[32m0.9894\u001b[0m            \u001b[35m0.6736\u001b[0m        \u001b[31m1.7626\u001b[0m  0.0200  4.8194\n",
      "      2            \u001b[36m0.7830\u001b[0m        \u001b[32m0.4478\u001b[0m            \u001b[35m0.7778\u001b[0m        \u001b[31m1.0126\u001b[0m  0.0199  5.3738\n",
      "      3            0.7830        \u001b[32m0.3540\u001b[0m            \u001b[35m0.8194\u001b[0m        1.0377  0.0195  5.2309\n",
      "      4            0.7205        \u001b[32m0.2691\u001b[0m            \u001b[35m0.8403\u001b[0m        \u001b[31m0.9821\u001b[0m  0.0188  4.7091\n",
      "      5            \u001b[36m0.8559\u001b[0m        \u001b[32m0.2369\u001b[0m            0.7986        1.4202  0.0179  4.7859\n",
      "      6            \u001b[36m0.9549\u001b[0m        \u001b[32m0.2208\u001b[0m            0.7847        \u001b[31m0.7810\u001b[0m  0.0168  5.4122\n",
      "      7            0.9271        \u001b[32m0.1714\u001b[0m            0.7569        1.5486  0.0155  4.7705\n",
      "      8            0.8733        \u001b[32m0.1634\u001b[0m            0.6181        2.1187  0.0140  4.6571\n",
      "      9            0.9062        0.1719            0.7222        0.8770  0.0125  4.7641\n",
      "     10            \u001b[36m0.9826\u001b[0m        \u001b[32m0.1132\u001b[0m            0.7153        1.4289  0.0108  5.0338\n",
      "     11            \u001b[36m0.9965\u001b[0m        0.1133            0.7222        1.3565  0.0092  5.1674\n",
      "     12            \u001b[36m0.9983\u001b[0m        \u001b[32m0.0856\u001b[0m            0.6181        1.9101  0.0075  4.8450\n",
      "     13            0.9792        \u001b[32m0.0799\u001b[0m            0.5417        2.1364  0.0060  4.7692\n",
      "     14            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0608\u001b[0m            0.6458        1.8562  0.0045  4.5517\n",
      "     15            1.0000        0.1010            0.6250        1.7562  0.0032  4.7229\n",
      "     16            1.0000        \u001b[32m0.0513\u001b[0m            0.6250        1.6226  0.0021  6.3621\n",
      "     17            0.9965        0.0915            0.5903        1.7655  0.0012  6.4376\n",
      "     18            1.0000        \u001b[32m0.0479\u001b[0m            0.6389        1.6500  0.0005  6.6087\n",
      "     19            1.0000        0.0569            0.6458        1.6091  0.0001  5.6504\n",
      "     20            1.0000        0.0625            0.6597        1.5772  0.0000  5.9089\n",
      "Best Cross Validation Kappa Score: 0.68\n",
      "\n",
      " ######################### Training for Subject: 4 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.8819\u001b[0m        \u001b[32m0.4851\u001b[0m            \u001b[35m0.6319\u001b[0m        \u001b[31m3.2219\u001b[0m  0.0200  4.7285\n",
      "      2            0.7274        \u001b[32m0.3728\u001b[0m            0.5764        3.5270  0.0199  4.7718\n",
      "      3            \u001b[36m0.9740\u001b[0m        \u001b[32m0.2421\u001b[0m            \u001b[35m0.6528\u001b[0m        \u001b[31m2.3370\u001b[0m  0.0195  4.6822\n",
      "      4            0.9740        \u001b[32m0.1848\u001b[0m            0.6389        2.6999  0.0188  4.6027\n",
      "      5            \u001b[36m0.9878\u001b[0m        \u001b[32m0.1788\u001b[0m            0.6319        2.4734  0.0179  4.7178\n",
      "      6            0.9670        \u001b[32m0.1078\u001b[0m            0.6042        3.6134  0.0168  4.8241\n",
      "      7            0.9288        0.1359            0.5833        3.9671  0.0155  4.6621\n",
      "      8            \u001b[36m0.9983\u001b[0m        \u001b[32m0.0894\u001b[0m            \u001b[35m0.6667\u001b[0m        \u001b[31m1.7145\u001b[0m  0.0140  4.6544\n",
      "      9            0.9948        \u001b[32m0.0595\u001b[0m            \u001b[35m0.7014\u001b[0m        \u001b[31m1.6349\u001b[0m  0.0125  4.7353\n",
      "     10            0.9826        \u001b[32m0.0550\u001b[0m            0.6875        \u001b[31m1.3555\u001b[0m  0.0108  4.6546\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     11            0.9965        0.0567            0.6458        2.9023  0.0092  5.1639\n",
      "     12            \u001b[36m1.0000\u001b[0m        0.1357            0.6181        1.9206  0.0075  4.8081\n",
      "     13            1.0000        \u001b[32m0.0270\u001b[0m            0.6319        2.1861  0.0060  4.5469\n",
      "     14            0.9983        0.0425            0.6389        2.4431  0.0045  4.7331\n",
      "     15            1.0000        0.0370            0.6181        1.6782  0.0032  4.5736\n",
      "     16            1.0000        0.0417            0.6389        2.3200  0.0021  4.7910\n",
      "     17            1.0000        0.0346            0.6528        2.1852  0.0012  4.5379\n",
      "     18            1.0000        \u001b[32m0.0245\u001b[0m            0.6458        2.2942  0.0005  4.4728\n",
      "     19            1.0000        0.0359            0.6528        2.2856  0.0001  4.8070\n",
      "     20            1.0000        0.0389            0.6528        2.2507  0.0000  4.7285\n",
      "Best Cross Validation Kappa Score: 0.40\n",
      "\n",
      " ######################### Training for Subject: 5 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.6128\u001b[0m        \u001b[32m0.7115\u001b[0m            \u001b[35m0.6736\u001b[0m        \u001b[31m0.8995\u001b[0m  0.0200  4.6946\n",
      "      2            \u001b[36m0.6285\u001b[0m        \u001b[32m0.3321\u001b[0m            0.5903        1.0295  0.0199  4.8541\n",
      "      3            \u001b[36m0.9201\u001b[0m        \u001b[32m0.3035\u001b[0m            \u001b[35m0.7917\u001b[0m        \u001b[31m0.4537\u001b[0m  0.0195  4.6358\n",
      "      4            0.9080        \u001b[32m0.2376\u001b[0m            0.7500        0.8995  0.0188  4.6939\n",
      "      5            \u001b[36m0.9722\u001b[0m        0.2534            0.7847        0.5142  0.0179  4.7552\n",
      "      6            \u001b[36m0.9809\u001b[0m        \u001b[32m0.2190\u001b[0m            \u001b[35m0.8194\u001b[0m        0.5968  0.0168  4.8241\n",
      "      7            0.8368        \u001b[32m0.1731\u001b[0m            0.7639        0.4683  0.0155  4.6653\n",
      "      8            0.9097        \u001b[32m0.1464\u001b[0m            0.7639        0.5448  0.0140  4.7653\n",
      "      9            \u001b[36m0.9896\u001b[0m        \u001b[32m0.1283\u001b[0m            0.7708        0.8114  0.0125  4.8134\n",
      "     10            0.8559        \u001b[32m0.1225\u001b[0m            0.6667        0.8593  0.0108  5.0022\n",
      "     11            0.9358        0.1584            0.7361        1.9196  0.0092  4.5563\n",
      "     12            0.9844        \u001b[32m0.0646\u001b[0m            0.7639        1.2582  0.0075  4.6027\n",
      "     13            0.9722        0.0831            0.7639        1.4786  0.0060  4.7016\n",
      "     14            \u001b[36m0.9931\u001b[0m        \u001b[32m0.0467\u001b[0m            0.7569        1.2757  0.0045  4.8872\n",
      "     15            \u001b[36m1.0000\u001b[0m        0.0524            0.7153        0.8416  0.0032  5.1913\n",
      "     16            1.0000        \u001b[32m0.0430\u001b[0m            0.7222        0.9613  0.0021  5.4618\n",
      "     17            1.0000        \u001b[32m0.0352\u001b[0m            0.7292        0.9583  0.0012  5.5076\n",
      "     18            1.0000        0.0500            0.7083        0.9884  0.0005  5.1409\n",
      "     19            1.0000        0.0493            0.7153        1.0205  0.0001  5.2851\n",
      "     20            1.0000        0.0355            0.7083        1.0446  0.0000  5.6133\n",
      "Best Cross Validation Kappa Score: 0.64\n",
      "\n",
      " ######################### Training for Subject: 6 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.7431\u001b[0m        \u001b[32m0.4290\u001b[0m            \u001b[35m0.7014\u001b[0m        \u001b[31m1.6065\u001b[0m  0.0200  5.2873\n",
      "      2            \u001b[36m0.8854\u001b[0m        \u001b[32m0.2588\u001b[0m            \u001b[35m0.7917\u001b[0m        \u001b[31m0.4660\u001b[0m  0.0199  5.0591\n",
      "      3            0.8854        \u001b[32m0.1972\u001b[0m            0.7500        0.6822  0.0195  4.7130\n",
      "      4            \u001b[36m0.9566\u001b[0m        \u001b[32m0.1373\u001b[0m            0.7778        0.5923  0.0188  5.1300\n",
      "      5            \u001b[36m0.9635\u001b[0m        0.1644            0.7083        0.8336  0.0179  5.1017\n",
      "      6            0.9271        \u001b[32m0.1075\u001b[0m            0.7847        0.5709  0.0168  4.6634\n",
      "      7            0.9497        0.1334            0.6389        1.7214  0.0155  5.6115\n",
      "      8            0.9271        \u001b[32m0.0900\u001b[0m            0.6458        1.9163  0.0140  4.9495\n",
      "      9            \u001b[36m0.9931\u001b[0m        0.1058            0.7778        0.9693  0.0125  4.9938\n",
      "     10            0.9931        0.1398            \u001b[35m0.7986\u001b[0m        0.6041  0.0108  4.9012\n",
      "     11            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0741\u001b[0m            0.7778        0.7196  0.0092  4.7903\n",
      "     12            0.9566        0.0780            0.5764        2.2154  0.0075  4.9168\n",
      "     13            1.0000        0.0823            0.6528        1.1937  0.0060  4.8008\n",
      "     14            0.9948        \u001b[32m0.0536\u001b[0m            0.7153        1.1077  0.0045  4.8283\n",
      "     15            0.9965        0.0723            0.6458        1.5615  0.0032  4.6599\n",
      "     16            0.9931        \u001b[32m0.0373\u001b[0m            0.6319        1.7963  0.0021  4.9095\n",
      "     17            1.0000        0.0486            0.6458        1.4417  0.0012  4.8158\n",
      "     18            1.0000        0.0494            0.6667        1.1475  0.0005  5.1356\n",
      "     19            1.0000        0.0534            0.7292        1.0141  0.0001  4.7609\n",
      "     20            1.0000        \u001b[32m0.0318\u001b[0m            0.7361        0.9362  0.0000  4.9376\n",
      "Best Cross Validation Kappa Score: 0.60\n",
      "\n",
      " ######################### Training for Subject: 7 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.5833\u001b[0m        \u001b[32m0.8367\u001b[0m            \u001b[35m0.5972\u001b[0m        \u001b[31m1.0215\u001b[0m  0.0200  4.5538\n",
      "      2            \u001b[36m0.8958\u001b[0m        \u001b[32m0.5135\u001b[0m            0.5417        \u001b[31m0.9443\u001b[0m  0.0199  4.6277\n",
      "      3            0.5434        \u001b[32m0.4150\u001b[0m            0.5208        2.2877  0.0195  4.6770\n",
      "      4            \u001b[36m0.9462\u001b[0m        \u001b[32m0.3000\u001b[0m            0.4028        1.5189  0.0188  4.7694\n",
      "      5            0.9010        \u001b[32m0.2907\u001b[0m            0.4444        1.4101  0.0179  4.8389\n",
      "      6            0.9444        \u001b[32m0.2701\u001b[0m            0.5000        1.2094  0.0168  5.3821\n",
      "      7            0.8663        \u001b[32m0.1783\u001b[0m            0.5139        2.3357  0.0155  4.7426\n",
      "      8            \u001b[36m0.9826\u001b[0m        0.1872            0.4514        1.2147  0.0140  4.6578\n",
      "      9            0.9410        \u001b[32m0.1455\u001b[0m            0.5347        2.3864  0.0125  4.7549\n",
      "     10            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1085\u001b[0m            0.4444        1.5256  0.0108  4.9114\n",
      "     11            0.8090        \u001b[32m0.0983\u001b[0m            0.5417        2.3898  0.0092  4.7372\n",
      "     12            0.9688        \u001b[32m0.0733\u001b[0m            0.5000        2.3318  0.0075  4.6677\n",
      "     13            0.9983        0.0886            0.4792        1.5730  0.0060  4.6935\n",
      "     14            1.0000        \u001b[32m0.0632\u001b[0m            0.4722        1.8118  0.0045  4.8752\n",
      "     15            0.9844        \u001b[32m0.0528\u001b[0m            0.5764        1.5031  0.0032  4.7661\n",
      "     16            1.0000        0.0661            0.5903        1.3310  0.0021  4.7882\n",
      "     17            1.0000        0.0575            0.5556        1.4122  0.0012  4.6615\n",
      "     18            1.0000        0.0790            0.5000        1.5753  0.0005  4.5987\n",
      "     19            1.0000        0.0871            0.5278        1.5733  0.0001  4.8056\n",
      "     20            1.0000        \u001b[32m0.0416\u001b[0m            0.5139        1.6028  0.0000  4.5912\n",
      "Best Cross Validation Kappa Score: 0.19\n",
      "\n",
      " ######################### Training for Subject: 8 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.6788\u001b[0m        \u001b[32m0.9026\u001b[0m            \u001b[35m0.6458\u001b[0m        \u001b[31m1.1903\u001b[0m  0.0200  4.6563\n",
      "      2            0.5712        \u001b[32m0.5872\u001b[0m            0.5347        1.5001  0.0199  4.6346\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      3            \u001b[36m0.8646\u001b[0m        \u001b[32m0.4170\u001b[0m            \u001b[35m0.7431\u001b[0m        \u001b[31m0.5289\u001b[0m  0.0195  4.8693\n",
      "      4            \u001b[36m0.9028\u001b[0m        \u001b[32m0.3524\u001b[0m            \u001b[35m0.7917\u001b[0m        \u001b[31m0.4134\u001b[0m  0.0188  6.0799\n",
      "      5            0.5486        \u001b[32m0.3232\u001b[0m            0.5208        1.8897  0.0179  4.7583\n",
      "      6            0.7188        \u001b[32m0.2822\u001b[0m            0.6319        0.9857  0.0168  4.7701\n",
      "      7            \u001b[36m0.9323\u001b[0m        \u001b[32m0.1956\u001b[0m            \u001b[35m0.8472\u001b[0m        \u001b[31m0.3701\u001b[0m  0.0155  4.9785\n",
      "      8            \u001b[36m0.9566\u001b[0m        \u001b[32m0.1949\u001b[0m            0.8125        0.4422  0.0140  4.9737\n",
      "      9            0.7066        \u001b[32m0.1432\u001b[0m            0.5347        2.2145  0.0125  5.0568\n",
      "     10            0.5729        \u001b[32m0.1379\u001b[0m            0.5208        2.2381  0.0108  5.2219\n",
      "     11            0.9340        \u001b[32m0.1257\u001b[0m            0.7569        0.7963  0.0092  4.6846\n",
      "     12            0.6285        \u001b[32m0.1113\u001b[0m            0.5139        2.5034  0.0075  4.7141\n",
      "     13            0.6267        \u001b[32m0.0977\u001b[0m            0.5278        2.4294  0.0060  4.5549\n",
      "     14            \u001b[36m0.9670\u001b[0m        \u001b[32m0.0795\u001b[0m            0.7083        1.1190  0.0045  4.6776\n",
      "     15            0.7951        \u001b[32m0.0765\u001b[0m            0.5694        1.9376  0.0032  4.7854\n",
      "     16            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0708\u001b[0m            0.7500        0.7173  0.0021  4.7839\n",
      "     17            0.9965        \u001b[32m0.0699\u001b[0m            0.7222        0.8862  0.0012  4.9422\n",
      "     18            1.0000        \u001b[32m0.0590\u001b[0m            0.7431        0.8026  0.0005  4.8245\n",
      "     19            1.0000        0.0607            0.7500        0.7506  0.0001  4.8011\n",
      "     20            1.0000        \u001b[32m0.0563\u001b[0m            0.7778        0.7189  0.0000  4.7841\n",
      "Best Cross Validation Kappa Score: 0.69\n"
     ]
    }
   ],
   "source": [
    "for subject in range(len(training_files)):\n",
    "    training_function(subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's Training Time with 0.5, 4.5 sec only with a bit more windows but with 10 epochs only\n",
    "\n",
    "as the issue seems to be overfitting as training acc is increasing but valid isn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.datautil import create_from_mne_epochs\n",
    "\n",
    "# convert epochs to braindecode compatible datastructure \n",
    "# 2sec windows with 0.250 sec stride\n",
    "window_size = 1024 #50 # 3072\n",
    "window_stride = 64 #256 # 50\n",
    "\n",
    "windows_datasets_list = []\n",
    "for epoch in epochs_list_train:\n",
    "    windows_datasets_list.append(\n",
    "            create_from_mne_epochs(\n",
    "            [epoch.crop(tmin=0.5, tmax=4.5, include_tmax=False)], # [0.5, 4.5] s, expects list of epochs\n",
    "            window_size_samples = window_size,\n",
    "            window_stride_samples = window_stride,\n",
    "            drop_last_window = False\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windows_datasets_labels(windows_dataset):\n",
    "    labels = []\n",
    "    for i in range(len(windows_dataset.datasets)):\n",
    "        labels.extend(windows_dataset.datasets[i].y)\n",
    "    return np.array(labels) \n",
    "\n",
    "for windows_dataset in windows_datasets_list:\n",
    "    windows_dataset.description = pd.DataFrame(data=get_windows_datasets_labels(windows_dataset), \n",
    "                                           columns=['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Windows in a Single Dataset:  1360\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Windows in a Single Dataset: \", len(windows_datasets_list[0].description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.datautil.preprocess import exponential_moving_standardize\n",
    "from braindecode.datautil.preprocess import MNEPreproc, NumpyPreproc, preprocess\n",
    "\n",
    "low_cut_hz = 7.  # low cut frequency for filtering\n",
    "high_cut_hz = 32.  # high cut frequency for filtering\n",
    "# Parameters for exponential moving standardization\n",
    "factor_new = 1e-3\n",
    "init_block_size = 1000\n",
    "\n",
    "def custom_exp_moving_std_fn(epochs, factor_new=factor_new, init_block_size=init_block_size):\n",
    "    data = epochs.get_data()\n",
    "    for i in range(len(data)):\n",
    "        epochs._data[i] = exponential_moving_standardize(data[i], \n",
    "                        factor_new=factor_new, init_block_size=init_block_size)\n",
    "    return epochs\n",
    "\n",
    "preprocessors = [\n",
    "    # keep only EEG sensors\n",
    "    MNEPreproc(fn='pick_types', eeg=True, meg=False, stim=False),\n",
    "    # bandpass filter\n",
    "    MNEPreproc(fn='filter', l_freq=low_cut_hz, h_freq=high_cut_hz),\n",
    "    # exponential moving standardization\n",
    "    MNEPreproc(fn=custom_exp_moving_std_fn, factor_new=factor_new,\n",
    "        init_block_size=init_block_size)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "for windows_dataset in windows_datasets_list: \n",
    "    preprocess(windows_dataset, preprocessors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 #64\n",
    "n_epochs = 10 #20 #25 few epochs for quick verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model\n",
    "import torch\n",
    "from braindecode.util import set_random_seeds\n",
    "from braindecode.models import ShallowFBCSPNet, EEGNetv4\n",
    "\n",
    "cuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "if cuda:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed = 20200220  # random seed to make results reproducible\n",
    "# Set random seed to be able to reproduce results\n",
    "set_random_seeds(seed=seed, cuda=cuda)\n",
    "\n",
    "n_classes=2\n",
    "# Extract number of chans and time steps from dataset\n",
    "n_chans = windows_datasets_list[0][0][0].shape[0]\n",
    "input_window_samples = windows_datasets_list[0][0][0].shape[1]\n",
    "\n",
    "model = EEGNetv4(\n",
    "    n_chans,\n",
    "    n_classes,\n",
    "    input_window_samples = window_size, #input_window_samples,\n",
    "    final_conv_length='auto',\n",
    ")\n",
    "\n",
    "# Send model to GPU\n",
    "if cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training time\n",
    "from skorch.callbacks import LRScheduler\n",
    "from skorch.helper import predefined_split\n",
    "from braindecode import EEGClassifier\n",
    "\n",
    "lr = 1 * 0.02 \n",
    "weight_decay = 0.5 * 0.001\n",
    "\n",
    "clfs_list = []\n",
    "for i in range(len(epochs_list_train)):\n",
    "    clfs_list.append(\n",
    "        EEGClassifier(\n",
    "                    model,\n",
    "                    criterion=torch.nn.NLLLoss,\n",
    "                    optimizer=torch.optim.AdamW,\n",
    "                    #train_split=predefined_split(train_set),  # using valid_set for validation\n",
    "                    optimizer__lr=lr,\n",
    "                    optimizer__weight_decay=weight_decay,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[\n",
    "                        \"accuracy\", (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n",
    "                    ],\n",
    "                    device=device,\n",
    "                    )\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_function(subject_index=0):\n",
    "    print('\\n', '#'*25, 'Training for Subject:', subject_index+1, '#'*25, '\\n')\n",
    "    dataset = windows_datasets_list[subject_index]\n",
    "    clfs_list[subject_index].fit(dataset, y=dataset.description.labels, epochs=n_epochs);\n",
    "    best_validation_acc = clfs_list[subject_index].callbacks_[4][1].best_score_ # a hack to get best validation accuracy\n",
    "    best_validation_kappa = (2*best_validation_acc)-1\n",
    "    print(\"Best Cross Validation Kappa Score: {:.2f}\".format(best_validation_kappa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ######################### Training for Subject: 1 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.8631\u001b[0m        \u001b[32m0.7389\u001b[0m            \u001b[35m0.6360\u001b[0m        \u001b[31m0.7081\u001b[0m  0.0200  9.8601\n",
      "      2            0.5064        \u001b[32m0.3883\u001b[0m            0.5000        6.7701  0.0194  9.4981\n",
      "      3            \u001b[36m0.9118\u001b[0m        \u001b[32m0.3356\u001b[0m            0.6360        1.2157  0.0177  9.2757\n",
      "      4            \u001b[36m0.9724\u001b[0m        \u001b[32m0.2187\u001b[0m            \u001b[35m0.7022\u001b[0m        0.8782  0.0150  9.6769\n",
      "      5            0.6360        \u001b[32m0.1798\u001b[0m            0.5588        1.9197  0.0117  9.2238\n",
      "      6            \u001b[36m0.9881\u001b[0m        \u001b[32m0.1125\u001b[0m            0.6654        0.8798  0.0083  9.5259\n",
      "      7            0.8989        \u001b[32m0.0927\u001b[0m            0.6471        1.0006  0.0050  9.2492\n",
      "      8            \u001b[36m0.9991\u001b[0m        \u001b[32m0.0800\u001b[0m            0.6103        1.1876  0.0023  9.1772\n",
      "      9            0.9991        \u001b[32m0.0695\u001b[0m            0.6029        1.1716  0.0006  9.3338\n",
      "     10            0.9991        \u001b[32m0.0695\u001b[0m            0.6066        1.2593  0.0000  9.3930\n",
      "Best Cross Validation Kappa Score: 0.40\n",
      "\n",
      " ######################### Training for Subject: 2 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr      dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  -------\n",
      "      1            \u001b[36m0.6397\u001b[0m        \u001b[32m0.4845\u001b[0m            \u001b[35m0.5478\u001b[0m        \u001b[31m1.3409\u001b[0m  0.0200  10.1024\n",
      "      2            \u001b[36m0.9412\u001b[0m        \u001b[32m0.3138\u001b[0m            \u001b[35m0.7978\u001b[0m        \u001b[31m0.4147\u001b[0m  0.0194  9.7561\n",
      "      3            \u001b[36m0.9449\u001b[0m        \u001b[32m0.2609\u001b[0m            0.7610        0.5015  0.0177  9.5813\n",
      "      4            0.9421        \u001b[32m0.1971\u001b[0m            0.7904        0.7771  0.0150  9.6731\n",
      "      5            0.8244        0.2506            0.5882        1.2412  0.0117  9.6378\n",
      "      6            \u001b[36m0.9559\u001b[0m        \u001b[32m0.1420\u001b[0m            0.7647        0.5000  0.0083  9.5106\n",
      "      7            \u001b[36m0.9954\u001b[0m        \u001b[32m0.1043\u001b[0m            0.7941        0.4584  0.0050  9.3451\n",
      "      8            \u001b[36m0.9972\u001b[0m        \u001b[32m0.0911\u001b[0m            \u001b[35m0.8088\u001b[0m        0.4703  0.0023  11.2367\n",
      "      9            \u001b[36m0.9991\u001b[0m        0.1011            0.8051        0.4585  0.0006  9.6712\n",
      "     10            0.9991        \u001b[32m0.0592\u001b[0m            0.8015        0.4824  0.0000  9.5707\n",
      "Best Cross Validation Kappa Score: 0.62\n",
      "\n",
      " ######################### Training for Subject: 3 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr      dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  -------\n",
      "      1            \u001b[36m0.9430\u001b[0m        \u001b[32m0.4519\u001b[0m            \u001b[35m0.5110\u001b[0m        \u001b[31m2.7034\u001b[0m  0.0200  29.3693\n",
      "      2            \u001b[36m0.9485\u001b[0m        \u001b[32m0.2598\u001b[0m            \u001b[35m0.6066\u001b[0m        \u001b[31m2.0526\u001b[0m  0.0194  10.3640\n",
      "      3            0.8879        \u001b[32m0.2220\u001b[0m            \u001b[35m0.6618\u001b[0m        \u001b[31m1.6725\u001b[0m  0.0177  8.8410\n",
      "      4            0.9118        \u001b[32m0.1994\u001b[0m            0.5184        4.9464  0.0150  8.8740\n",
      "      5            0.9246        \u001b[32m0.1398\u001b[0m            0.5074        5.3321  0.0117  9.1022\n",
      "      6            \u001b[36m0.9844\u001b[0m        \u001b[32m0.1215\u001b[0m            0.6176        3.1015  0.0083  9.8465\n",
      "      7            \u001b[36m0.9954\u001b[0m        \u001b[32m0.0984\u001b[0m            0.5809        3.0962  0.0050  9.3021\n",
      "      8            0.9853        \u001b[32m0.0695\u001b[0m            0.5257        3.8022  0.0023  9.1358\n",
      "      9            \u001b[36m0.9991\u001b[0m        0.0695            0.5809        3.2772  0.0006  9.1499\n",
      "     10            0.9991        \u001b[32m0.0591\u001b[0m            0.5882        3.0683  0.0000  8.7505\n",
      "Best Cross Validation Kappa Score: 0.32\n",
      "\n",
      " ######################### Training for Subject: 4 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr      dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  -------\n",
      "      1            \u001b[36m0.7583\u001b[0m        \u001b[32m0.3815\u001b[0m            \u001b[35m0.5257\u001b[0m        \u001b[31m3.4035\u001b[0m  0.0200  19.1817\n",
      "      2            0.5404        \u001b[32m0.1900\u001b[0m            \u001b[35m0.5735\u001b[0m        \u001b[31m1.4691\u001b[0m  0.0194  8.9861\n",
      "      3            \u001b[36m0.8419\u001b[0m        \u001b[32m0.1731\u001b[0m            \u001b[35m0.8088\u001b[0m        \u001b[31m0.7173\u001b[0m  0.0177  8.9897\n",
      "      4            \u001b[36m0.9439\u001b[0m        0.1926            0.6360        1.4321  0.0150  9.0043\n",
      "      5            \u001b[36m0.9945\u001b[0m        \u001b[32m0.1150\u001b[0m            0.7574        0.7450  0.0117  8.8822\n",
      "      6            0.9936        \u001b[32m0.0962\u001b[0m            0.7426        0.9687  0.0083  8.8000\n",
      "      7            \u001b[36m0.9972\u001b[0m        \u001b[32m0.0766\u001b[0m            \u001b[35m0.8125\u001b[0m        0.7807  0.0050  9.0786\n",
      "      8            \u001b[36m0.9982\u001b[0m        0.0865            0.7647        0.9463  0.0023  9.0380\n",
      "      9            \u001b[36m0.9991\u001b[0m        \u001b[32m0.0563\u001b[0m            0.7353        1.1536  0.0006  8.9810\n",
      "     10            0.9982        0.0786            0.7206        1.2528  0.0000  8.9092\n",
      "Best Cross Validation Kappa Score: 0.62\n",
      "\n",
      " ######################### Training for Subject: 5 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr      dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  -------\n",
      "      1            \u001b[36m0.5018\u001b[0m        \u001b[32m0.5118\u001b[0m            \u001b[35m0.5000\u001b[0m        \u001b[31m1.9476\u001b[0m  0.0200  10.3257\n",
      "      2            \u001b[36m0.7831\u001b[0m        \u001b[32m0.2757\u001b[0m            \u001b[35m0.7537\u001b[0m        \u001b[31m0.5549\u001b[0m  0.0194  9.4478\n",
      "      3            \u001b[36m0.8199\u001b[0m        \u001b[32m0.2670\u001b[0m            0.6654        1.9091  0.0177  10.8161\n",
      "      4            \u001b[36m0.9504\u001b[0m        \u001b[32m0.1864\u001b[0m            \u001b[35m0.7868\u001b[0m        0.8822  0.0150  10.0705\n",
      "      5            \u001b[36m0.9881\u001b[0m        \u001b[32m0.1689\u001b[0m            0.7537        0.6054  0.0117  9.4036\n",
      "      6            0.9458        \u001b[32m0.1149\u001b[0m            0.7059        1.5038  0.0083  9.0593\n",
      "      7            0.9678        \u001b[32m0.1097\u001b[0m            0.7169        0.6425  0.0050  9.1382\n",
      "      8            0.9338        \u001b[32m0.0895\u001b[0m            0.7169        1.3693  0.0023  9.2494\n",
      "      9            \u001b[36m0.9890\u001b[0m        \u001b[32m0.0570\u001b[0m            0.7500        0.9526  0.0006  11.9579\n",
      "     10            \u001b[36m0.9917\u001b[0m        0.0793            0.7574        0.8990  0.0000  11.0502\n",
      "Best Cross Validation Kappa Score: 0.57\n",
      "\n",
      " ######################### Training for Subject: 6 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr      dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  -------\n",
      "      1            \u001b[36m0.7886\u001b[0m        \u001b[32m0.3194\u001b[0m            \u001b[35m0.7574\u001b[0m        \u001b[31m1.1366\u001b[0m  0.0200  11.3679\n",
      "      2            \u001b[36m0.9807\u001b[0m        \u001b[32m0.1622\u001b[0m            \u001b[35m0.7721\u001b[0m        \u001b[31m0.6021\u001b[0m  0.0194  10.1517\n",
      "      3            0.9715        \u001b[32m0.1093\u001b[0m            \u001b[35m0.7868\u001b[0m        0.6736  0.0177  9.7392\n",
      "      4            0.5331        0.1096            0.5000        4.9113  0.0150  9.1789\n",
      "      5            \u001b[36m0.9972\u001b[0m        0.1117            0.7500        0.9610  0.0117  9.1916\n",
      "      6            \u001b[36m0.9982\u001b[0m        \u001b[32m0.0540\u001b[0m            0.7390        1.1293  0.0083  9.4272\n",
      "      7            0.9945        \u001b[32m0.0351\u001b[0m            0.7169        1.2557  0.0050  9.2140\n",
      "      8            \u001b[36m0.9991\u001b[0m        0.0444            0.6875        1.3326  0.0023  9.0561\n",
      "      9            \u001b[36m1.0000\u001b[0m        0.0446            \u001b[35m0.7904\u001b[0m        0.8732  0.0006  9.2475\n",
      "     10            0.9991        \u001b[32m0.0196\u001b[0m            0.7868        0.7551  0.0000  9.1403\n",
      "Best Cross Validation Kappa Score: 0.58\n",
      "\n",
      " ######################### Training for Subject: 7 ######################### \n",
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.7178\u001b[0m        \u001b[32m0.7141\u001b[0m            \u001b[35m0.5184\u001b[0m        \u001b[31m1.2939\u001b[0m  0.0200  9.6275\n",
      "      2            \u001b[36m0.7647\u001b[0m        \u001b[32m0.4544\u001b[0m            \u001b[35m0.7978\u001b[0m        \u001b[31m0.3998\u001b[0m  0.0194  9.1792\n",
      "      3            \u001b[36m0.9081\u001b[0m        \u001b[32m0.3771\u001b[0m            0.7353        0.5899  0.0177  8.8885\n",
      "      4            \u001b[36m0.9246\u001b[0m        \u001b[32m0.3164\u001b[0m            0.7206        0.7690  0.0150  8.9194\n",
      "      5            0.5625        \u001b[32m0.2410\u001b[0m            0.5074        3.1520  0.0117  8.9581\n",
      "      6            \u001b[36m0.9256\u001b[0m        \u001b[32m0.1964\u001b[0m            0.6581        1.0022  0.0083  9.1344\n",
      "      7            \u001b[36m0.9301\u001b[0m        \u001b[32m0.1255\u001b[0m            0.6581        1.2392  0.0050  9.2434\n",
      "      8            0.9274        \u001b[32m0.1230\u001b[0m            0.6507        1.2617  0.0023  8.9456\n",
      "      9            \u001b[36m0.9917\u001b[0m        \u001b[32m0.0957\u001b[0m            0.7831        0.5781  0.0006  8.8242\n",
      "     10            0.9917        0.1120            0.7941        0.5259  0.0000  9.1850\n",
      "Best Cross Validation Kappa Score: 0.60\n",
      "\n",
      " ######################### Training for Subject: 8 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.5818\u001b[0m        \u001b[32m0.7595\u001b[0m            \u001b[35m0.5515\u001b[0m        \u001b[31m1.0054\u001b[0m  0.0200  9.1001\n",
      "      2            0.5000        \u001b[32m0.5234\u001b[0m            0.5000        3.2517  0.0194  9.1153\n",
      "      3            0.5000        \u001b[32m0.4392\u001b[0m            0.5000        3.4583  0.0177  9.2225\n",
      "      4            0.5147        \u001b[32m0.3814\u001b[0m            0.5257        1.7140  0.0150  9.3640\n",
      "      5            0.5303        \u001b[32m0.2820\u001b[0m            0.5110        2.1276  0.0117  9.1047\n",
      "      6            \u001b[36m0.6829\u001b[0m        \u001b[32m0.2437\u001b[0m            \u001b[35m0.5882\u001b[0m        1.4455  0.0083  9.0760\n",
      "      7            \u001b[36m0.8483\u001b[0m        \u001b[32m0.1878\u001b[0m            \u001b[35m0.6838\u001b[0m        \u001b[31m0.8243\u001b[0m  0.0050  9.2429\n",
      "      8            \u001b[36m0.9752\u001b[0m        \u001b[32m0.1627\u001b[0m            \u001b[35m0.7904\u001b[0m        \u001b[31m0.5159\u001b[0m  0.0023  9.1421\n",
      "      9            0.9412        \u001b[32m0.1442\u001b[0m            0.6507        1.0282  0.0006  9.1746\n",
      "     10            0.9752        \u001b[32m0.1281\u001b[0m            0.7169        0.7483  0.0000  9.3551\n",
      "Best Cross Validation Kappa Score: 0.58\n"
     ]
    }
   ],
   "source": [
    "for subject in range(len(training_files)):\n",
    "    training_function(subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#next time try whole trial once without much cropping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's Training Time with 0.5, 4.5 sec only with a single windows but with 10 epochs only\n",
    "\n",
    "as the issue seems to be overfitting as training acc is increasing but valid isn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.datautil import create_from_mne_epochs\n",
    "\n",
    "# convert epochs to braindecode compatible datastructure \n",
    "window_size = 2030 #50 # 3072\n",
    "window_stride = 1024 #256 # 50\n",
    "\n",
    "windows_datasets_list = []\n",
    "for epoch in epochs_list_train:\n",
    "    windows_datasets_list.append(\n",
    "            create_from_mne_epochs(\n",
    "            [epoch.crop(tmin=0.5, tmax=4.5, include_tmax=False)], # [0.5, 4.5] s, expects list of epochs\n",
    "            window_size_samples = window_size,\n",
    "            window_stride_samples = window_stride,\n",
    "            drop_last_window = False\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windows_datasets_labels(windows_dataset):\n",
    "    labels = []\n",
    "    for i in range(len(windows_dataset.datasets)):\n",
    "        labels.extend(windows_dataset.datasets[i].y)\n",
    "    return np.array(labels) \n",
    "\n",
    "for windows_dataset in windows_datasets_list:\n",
    "    windows_dataset.description = pd.DataFrame(data=get_windows_datasets_labels(windows_dataset), \n",
    "                                           columns=['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Windows in a Single Dataset:  160\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Windows in a Single Dataset: \", len(windows_datasets_list[0].description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.datautil.preprocess import exponential_moving_standardize\n",
    "from braindecode.datautil.preprocess import MNEPreproc, NumpyPreproc, preprocess\n",
    "\n",
    "low_cut_hz = 7.  # low cut frequency for filtering\n",
    "high_cut_hz = 32.  # high cut frequency for filtering\n",
    "# Parameters for exponential moving standardization\n",
    "factor_new = 1e-3\n",
    "init_block_size = 1000\n",
    "\n",
    "def custom_exp_moving_std_fn(epochs, factor_new=factor_new, init_block_size=init_block_size):\n",
    "    data = epochs.get_data()\n",
    "    for i in range(len(data)):\n",
    "        epochs._data[i] = exponential_moving_standardize(data[i], \n",
    "                        factor_new=factor_new, init_block_size=init_block_size)\n",
    "    return epochs\n",
    "\n",
    "preprocessors = [\n",
    "    # keep only EEG sensors\n",
    "    MNEPreproc(fn='pick_types', eeg=True, meg=False, stim=False),\n",
    "    # bandpass filter\n",
    "    MNEPreproc(fn='filter', l_freq=low_cut_hz, h_freq=high_cut_hz),\n",
    "    # exponential moving standardization\n",
    "    MNEPreproc(fn=custom_exp_moving_std_fn, factor_new=factor_new,\n",
    "        init_block_size=init_block_size)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "for windows_dataset in windows_datasets_list: \n",
    "    preprocess(windows_dataset, preprocessors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 #64\n",
    "n_epochs = 10 #20 #25 few epochs for quick verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model\n",
    "import torch\n",
    "from braindecode.util import set_random_seeds\n",
    "from braindecode.models import ShallowFBCSPNet, EEGNetv4\n",
    "\n",
    "cuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "if cuda:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed = 20200220  # random seed to make results reproducible\n",
    "# Set random seed to be able to reproduce results\n",
    "set_random_seeds(seed=seed, cuda=cuda)\n",
    "\n",
    "n_classes=2\n",
    "# Extract number of chans and time steps from dataset\n",
    "n_chans = windows_datasets_list[0][0][0].shape[0]\n",
    "input_window_samples = windows_datasets_list[0][0][0].shape[1]\n",
    "\n",
    "model = EEGNetv4(\n",
    "    n_chans,\n",
    "    n_classes,\n",
    "    input_window_samples = window_size, #input_window_samples,\n",
    "    final_conv_length='auto',\n",
    ")\n",
    "\n",
    "# Send model to GPU\n",
    "if cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training time\n",
    "from skorch.callbacks import LRScheduler\n",
    "from skorch.helper import predefined_split\n",
    "from braindecode import EEGClassifier\n",
    "\n",
    "lr = 1 * 0.02 \n",
    "weight_decay = 0.5 * 0.001\n",
    "\n",
    "clfs_list = []\n",
    "for i in range(len(epochs_list_train)):\n",
    "    clfs_list.append(\n",
    "        EEGClassifier(\n",
    "                    model,\n",
    "                    criterion=torch.nn.NLLLoss,\n",
    "                    optimizer=torch.optim.AdamW,\n",
    "                    #train_split=predefined_split(train_set),  # using valid_set for validation\n",
    "                    optimizer__lr=lr,\n",
    "                    optimizer__weight_decay=weight_decay,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[\n",
    "                        \"accuracy\", (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n",
    "                    ],\n",
    "                    device=device,\n",
    "                    )\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_function(subject_index=0):\n",
    "    print('\\n', '#'*25, 'Training for Subject:', subject_index+1, '#'*25, '\\n')\n",
    "    dataset = windows_datasets_list[subject_index]\n",
    "    clfs_list[subject_index].fit(dataset, y=dataset.description.labels, epochs=n_epochs);\n",
    "    best_validation_acc = clfs_list[subject_index].callbacks_[4][1].best_score_ # a hack to get best validation accuracy\n",
    "    best_validation_kappa = (2*best_validation_acc)-1\n",
    "    print(\"Best Cross Validation Kappa Score: {:.2f}\".format(best_validation_kappa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ######################### Training for Subject: 1 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.6094\u001b[0m        \u001b[32m0.7857\u001b[0m            \u001b[35m0.5312\u001b[0m        \u001b[31m0.8298\u001b[0m  0.0200  1.9470\n",
      "      2            0.5156        \u001b[32m0.4767\u001b[0m            0.5000        1.9009  0.0194  1.9160\n",
      "      3            0.5000        \u001b[32m0.2453\u001b[0m            0.5000        3.4557  0.0177  1.8300\n",
      "      4            0.5000        \u001b[32m0.1696\u001b[0m            0.5000        4.4461  0.0150  1.7920\n",
      "      5            0.5000        \u001b[32m0.1138\u001b[0m            0.5000        5.5788  0.0117  1.7320\n",
      "      6            0.5000        \u001b[32m0.0987\u001b[0m            0.5000        4.8779  0.0083  1.7310\n",
      "      7            0.5000        \u001b[32m0.0527\u001b[0m            0.5000        4.7223  0.0050  1.7820\n",
      "      8            0.5000        \u001b[32m0.0478\u001b[0m            0.5000        4.5980  0.0023  1.7810\n",
      "      9            0.5000        \u001b[32m0.0388\u001b[0m            0.5000        4.3149  0.0006  1.7730\n",
      "     10            0.5000        0.0391            0.5000        4.0511  0.0000  1.9940\n",
      "Best Cross Validation Kappa Score: 0.06\n",
      "\n",
      " ######################### Training for Subject: 2 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.7891\u001b[0m        \u001b[32m0.9141\u001b[0m            \u001b[35m0.7188\u001b[0m        \u001b[31m0.7262\u001b[0m  0.0200  2.2810\n",
      "      2            \u001b[36m0.9766\u001b[0m        \u001b[32m0.4521\u001b[0m            \u001b[35m0.8750\u001b[0m        \u001b[31m0.6042\u001b[0m  0.0194  2.3660\n",
      "      3            0.8594        \u001b[32m0.1690\u001b[0m            0.7812        0.7723  0.0177  2.3690\n",
      "      4            0.7266        \u001b[32m0.0525\u001b[0m            0.6875        2.2280  0.0150  2.1040\n",
      "      5            0.7891        \u001b[32m0.0400\u001b[0m            0.7188        1.4339  0.0117  1.9730\n",
      "      6            0.9141        \u001b[32m0.0284\u001b[0m            0.7812        \u001b[31m0.4997\u001b[0m  0.0083  1.9100\n",
      "      7            \u001b[36m0.9844\u001b[0m        0.0311            \u001b[35m0.9062\u001b[0m        \u001b[31m0.2433\u001b[0m  0.0050  1.8800\n",
      "      8            \u001b[36m1.0000\u001b[0m        \u001b[32m0.0097\u001b[0m            0.9062        \u001b[31m0.2249\u001b[0m  0.0023  1.7840\n",
      "      9            1.0000        0.0135            \u001b[35m0.9375\u001b[0m        0.2360  0.0006  1.7800\n",
      "     10            1.0000        0.0194            0.9375        0.2537  0.0000  1.8740\n",
      "Best Cross Validation Kappa Score: 0.88\n",
      "\n",
      " ######################### Training for Subject: 3 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.7500\u001b[0m        \u001b[32m0.8784\u001b[0m            \u001b[35m0.7188\u001b[0m        \u001b[31m1.8774\u001b[0m  0.0200  1.8870\n",
      "      2            \u001b[36m0.8203\u001b[0m        \u001b[32m0.5407\u001b[0m            0.7188        \u001b[31m1.4405\u001b[0m  0.0194  1.7880\n",
      "      3            \u001b[36m0.8984\u001b[0m        \u001b[32m0.3013\u001b[0m            0.6562        2.1731  0.0177  1.7590\n",
      "      4            \u001b[36m0.9922\u001b[0m        \u001b[32m0.1979\u001b[0m            0.7188        1.8787  0.0150  1.7740\n",
      "      5            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1316\u001b[0m            0.7188        \u001b[31m1.2639\u001b[0m  0.0117  1.7760\n",
      "      6            1.0000        \u001b[32m0.0380\u001b[0m            0.7188        1.2917  0.0083  1.7280\n",
      "      7            1.0000        \u001b[32m0.0231\u001b[0m            0.6562        \u001b[31m1.2316\u001b[0m  0.0050  1.7810\n",
      "      8            1.0000        0.0325            0.6562        \u001b[31m1.2234\u001b[0m  0.0023  1.7940\n",
      "      9            1.0000        0.0386            0.6562        \u001b[31m1.1919\u001b[0m  0.0006  1.7330\n",
      "     10            1.0000        0.0357            0.6875        \u001b[31m1.1554\u001b[0m  0.0000  1.8630\n",
      "Best Cross Validation Kappa Score: 0.44\n",
      "\n",
      " ######################### Training for Subject: 4 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.9453\u001b[0m        \u001b[32m0.9283\u001b[0m            \u001b[35m0.7188\u001b[0m        \u001b[31m1.2930\u001b[0m  0.0200  1.8870\n",
      "      2            0.9297        \u001b[32m0.2906\u001b[0m            \u001b[35m0.8438\u001b[0m        1.6176  0.0194  1.8280\n",
      "      3            \u001b[36m0.9844\u001b[0m        \u001b[32m0.2413\u001b[0m            0.7812        \u001b[31m1.2570\u001b[0m  0.0177  1.7690\n",
      "      4            \u001b[36m1.0000\u001b[0m        \u001b[32m0.1191\u001b[0m            0.7812        \u001b[31m0.9826\u001b[0m  0.0150  1.8430\n",
      "      5            1.0000        0.1488            0.8438        \u001b[31m0.8993\u001b[0m  0.0117  1.8160\n",
      "      6            1.0000        \u001b[32m0.0461\u001b[0m            0.8438        \u001b[31m0.8171\u001b[0m  0.0083  1.8840\n",
      "      7            1.0000        0.1258            0.8438        0.8800  0.0050  1.9300\n",
      "      8            1.0000        0.0554            0.8438        0.8690  0.0023  1.8880\n",
      "      9            1.0000        \u001b[32m0.0430\u001b[0m            0.8438        0.8560  0.0006  2.1260\n",
      "     10            1.0000        \u001b[32m0.0299\u001b[0m            0.8438        0.8499  0.0000  2.4710\n",
      "Best Cross Validation Kappa Score: 0.69\n",
      "\n",
      " ######################### Training for Subject: 5 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.8984\u001b[0m        \u001b[32m0.6653\u001b[0m            \u001b[35m0.8125\u001b[0m        \u001b[31m1.3623\u001b[0m  0.0200  2.0750\n",
      "      2            0.8594        \u001b[32m0.2500\u001b[0m            0.7188        \u001b[31m1.1152\u001b[0m  0.0194  2.0340\n",
      "      3            0.6875        \u001b[32m0.1121\u001b[0m            0.5000        3.2957  0.0177  2.7170\n",
      "      4            0.6328        \u001b[32m0.0731\u001b[0m            0.5000        3.7442  0.0150  3.0360\n",
      "      5            0.6719        \u001b[32m0.0369\u001b[0m            0.5312        3.5729  0.0117  3.1360\n",
      "      6            0.8047        0.0411            0.6562        2.1824  0.0083  3.6450\n",
      "      7            0.8672        0.0464            0.7188        1.6353  0.0050  2.3760\n",
      "      8            \u001b[36m0.9062\u001b[0m        \u001b[32m0.0302\u001b[0m            0.7500        1.3232  0.0023  2.1870\n",
      "      9            \u001b[36m0.9453\u001b[0m        0.0358            0.7812        \u001b[31m1.0689\u001b[0m  0.0006  1.9350\n",
      "     10            \u001b[36m0.9609\u001b[0m        \u001b[32m0.0281\u001b[0m            0.7812        \u001b[31m0.8969\u001b[0m  0.0000  1.9320\n",
      "Best Cross Validation Kappa Score: 0.62\n",
      "\n",
      " ######################### Training for Subject: 6 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.8203\u001b[0m        \u001b[32m0.4717\u001b[0m            \u001b[35m0.8125\u001b[0m        \u001b[31m1.9446\u001b[0m  0.0200  2.7910\n",
      "      2            0.7031        \u001b[32m0.1708\u001b[0m            0.6562        3.7815  0.0194  2.1220\n",
      "      3            0.6719        0.2546            0.6875        4.1987  0.0177  2.0670\n",
      "      4            0.7734        \u001b[32m0.0445\u001b[0m            0.7500        3.3549  0.0150  2.0950\n",
      "      5            \u001b[36m0.8281\u001b[0m        0.0695            0.7500        2.7950  0.0117  1.9950\n",
      "      6            0.8203        0.0655            0.7812        2.4655  0.0083  1.9770\n",
      "      7            0.8281        \u001b[32m0.0055\u001b[0m            0.7812        2.2595  0.0050  2.0040\n",
      "      8            \u001b[36m0.8438\u001b[0m        0.0168            0.7812        1.9938  0.0023  2.0490\n",
      "      9            \u001b[36m0.8516\u001b[0m        0.0449            0.7812        \u001b[31m1.7342\u001b[0m  0.0006  1.8070\n",
      "     10            \u001b[36m0.8750\u001b[0m        0.0304            0.8125        \u001b[31m1.5586\u001b[0m  0.0000  1.8150\n",
      "Best Cross Validation Kappa Score: 0.62\n",
      "\n",
      " ######################### Training for Subject: 7 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.5469\u001b[0m        \u001b[32m1.5216\u001b[0m            \u001b[35m0.7188\u001b[0m        \u001b[31m0.4782\u001b[0m  0.0200  1.8530\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2            \u001b[36m0.8125\u001b[0m        \u001b[32m0.7220\u001b[0m            \u001b[35m0.7500\u001b[0m        0.6651  0.0194  1.9600\n",
      "      3            \u001b[36m0.9609\u001b[0m        \u001b[32m0.3669\u001b[0m            0.5625        1.5228  0.0177  2.0090\n",
      "      4            0.9375        \u001b[32m0.2437\u001b[0m            0.5312        1.9882  0.0150  1.9020\n",
      "      5            0.9531        \u001b[32m0.2038\u001b[0m            0.5000        2.0008  0.0117  1.9220\n",
      "      6            0.9609        \u001b[32m0.1487\u001b[0m            0.5312        1.9483  0.0083  2.0790\n",
      "      7            \u001b[36m0.9766\u001b[0m        0.1579            0.5625        1.7738  0.0050  2.0820\n",
      "      8            \u001b[36m0.9844\u001b[0m        0.1627            0.5938        1.6637  0.0023  2.4940\n",
      "      9            0.9844        \u001b[32m0.1048\u001b[0m            0.6250        1.5801  0.0006  1.9390\n",
      "     10            0.9844        0.1310            0.6250        1.5213  0.0000  1.9710\n",
      "Best Cross Validation Kappa Score: 0.50\n",
      "\n",
      " ######################### Training for Subject: 8 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.8438\u001b[0m        \u001b[32m1.0396\u001b[0m            \u001b[35m0.6875\u001b[0m        \u001b[31m0.7131\u001b[0m  0.0200  2.1470\n",
      "      2            0.8359        \u001b[32m0.3151\u001b[0m            0.5312        1.7083  0.0194  2.1540\n",
      "      3            \u001b[36m0.8984\u001b[0m        \u001b[32m0.2947\u001b[0m            0.6250        1.5089  0.0177  2.0730\n",
      "      4            \u001b[36m0.9688\u001b[0m        \u001b[32m0.1799\u001b[0m            \u001b[35m0.7188\u001b[0m        0.9136  0.0150  2.0810\n",
      "      5            \u001b[36m0.9844\u001b[0m        \u001b[32m0.1677\u001b[0m            0.6875        0.7326  0.0117  1.9970\n",
      "      6            0.9688        \u001b[32m0.0791\u001b[0m            0.7188        0.9526  0.0083  1.9940\n",
      "      7            0.9688        0.1505            0.6562        1.0200  0.0050  1.9080\n",
      "      8            0.9844        \u001b[32m0.0596\u001b[0m            0.6875        0.9675  0.0023  1.9850\n",
      "      9            0.9844        \u001b[32m0.0571\u001b[0m            0.6875        0.9204  0.0006  1.9980\n",
      "     10            \u001b[36m0.9922\u001b[0m        0.1050            0.6875        0.8856  0.0000  1.9890\n",
      "Best Cross Validation Kappa Score: 0.44\n"
     ]
    }
   ],
   "source": [
    "for subject in range(len(training_files)):\n",
    "    training_function(subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note: results seem better, may be I should try non-overlapping windows so all examples have their own contribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's Training Time with 0.5, 4.5 sec with non-overlapping 2 windows but with 10 epochs only\n",
    "\n",
    "as the issue seems to be overfitting as training acc is increasing but valid isn't"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.datautil import create_from_mne_epochs\n",
    "\n",
    "# convert epochs to braindecode compatible datastructure \n",
    "# 2sec windows with 0.250 sec stride\n",
    "window_size = 1024 #50 # 3072\n",
    "window_stride = 1024 #256 # 50\n",
    "\n",
    "windows_datasets_list = []\n",
    "for epoch in epochs_list_train:\n",
    "    windows_datasets_list.append(\n",
    "            create_from_mne_epochs(\n",
    "            [epoch.crop(tmin=0.5, tmax=4.5, include_tmax=False)], # [0.5, 4.5] s, expects list of epochs\n",
    "            window_size_samples = window_size,\n",
    "            window_stride_samples = window_stride,\n",
    "            drop_last_window = False\n",
    "        )\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_windows_datasets_labels(windows_dataset):\n",
    "    labels = []\n",
    "    for i in range(len(windows_dataset.datasets)):\n",
    "        labels.extend(windows_dataset.datasets[i].y)\n",
    "    return np.array(labels) \n",
    "\n",
    "for windows_dataset in windows_datasets_list:\n",
    "    windows_dataset.description = pd.DataFrame(data=get_windows_datasets_labels(windows_dataset), \n",
    "                                           columns=['labels'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total Windows in a Single Dataset:  160\n"
     ]
    }
   ],
   "source": [
    "print(\"Total Windows in a Single Dataset: \", len(windows_datasets_list[0].description))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from braindecode.datautil.preprocess import exponential_moving_standardize\n",
    "from braindecode.datautil.preprocess import MNEPreproc, NumpyPreproc, preprocess\n",
    "\n",
    "low_cut_hz = 7.  # low cut frequency for filtering\n",
    "high_cut_hz = 32.  # high cut frequency for filtering\n",
    "# Parameters for exponential moving standardization\n",
    "factor_new = 1e-3\n",
    "init_block_size = 1000\n",
    "\n",
    "def custom_exp_moving_std_fn(epochs, factor_new=factor_new, init_block_size=init_block_size):\n",
    "    data = epochs.get_data()\n",
    "    for i in range(len(data)):\n",
    "        epochs._data[i] = exponential_moving_standardize(data[i], \n",
    "                        factor_new=factor_new, init_block_size=init_block_size)\n",
    "    return epochs\n",
    "\n",
    "preprocessors = [\n",
    "    # keep only EEG sensors\n",
    "    MNEPreproc(fn='pick_types', eeg=True, meg=False, stim=False),\n",
    "    # bandpass filter\n",
    "    MNEPreproc(fn='filter', l_freq=low_cut_hz, h_freq=high_cut_hz),\n",
    "    # exponential moving standardization\n",
    "    MNEPreproc(fn=custom_exp_moving_std_fn, factor_new=factor_new,\n",
    "        init_block_size=init_block_size)\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "for windows_dataset in windows_datasets_list: \n",
    "    preprocess(windows_dataset, preprocessors)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 16 #64\n",
    "n_epochs = 10 #20 #25 few epochs for quick verification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Creating a model\n",
    "import torch\n",
    "from braindecode.util import set_random_seeds\n",
    "from braindecode.models import ShallowFBCSPNet, EEGNetv4\n",
    "\n",
    "cuda = torch.cuda.is_available()  # check if GPU is available, if True chooses to use it\n",
    "device = 'cuda' if cuda else 'cpu'\n",
    "if cuda:\n",
    "    torch.backends.cudnn.benchmark = True\n",
    "seed = 20200220  # random seed to make results reproducible\n",
    "# Set random seed to be able to reproduce results\n",
    "set_random_seeds(seed=seed, cuda=cuda)\n",
    "\n",
    "n_classes=2\n",
    "# Extract number of chans and time steps from dataset\n",
    "n_chans = windows_datasets_list[0][0][0].shape[0]\n",
    "input_window_samples = windows_datasets_list[0][0][0].shape[1]\n",
    "\n",
    "model = EEGNetv4(\n",
    "    n_chans,\n",
    "    n_classes,\n",
    "    input_window_samples = window_size, #input_window_samples,\n",
    "    final_conv_length='auto',\n",
    ")\n",
    "\n",
    "# Send model to GPU\n",
    "if cuda:\n",
    "    model.cuda()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training time\n",
    "from skorch.callbacks import LRScheduler\n",
    "from skorch.helper import predefined_split\n",
    "from braindecode import EEGClassifier\n",
    "\n",
    "lr = 1 * 0.02 \n",
    "weight_decay = 0.5 * 0.001\n",
    "\n",
    "clfs_list = []\n",
    "for i in range(len(epochs_list_train)):\n",
    "    clfs_list.append(\n",
    "        EEGClassifier(\n",
    "                    model,\n",
    "                    criterion=torch.nn.NLLLoss,\n",
    "                    optimizer=torch.optim.AdamW,\n",
    "                    #train_split=predefined_split(train_set),  # using valid_set for validation\n",
    "                    optimizer__lr=lr,\n",
    "                    optimizer__weight_decay=weight_decay,\n",
    "                    batch_size=batch_size,\n",
    "                    callbacks=[\n",
    "                        \"accuracy\", (\"lr_scheduler\", LRScheduler('CosineAnnealingLR', T_max=n_epochs - 1)),\n",
    "                    ],\n",
    "                    device=device,\n",
    "                    )\n",
    "                )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_function(subject_index=0):\n",
    "    print('\\n', '#'*25, 'Training for Subject:', subject_index+1, '#'*25, '\\n')\n",
    "    dataset = windows_datasets_list[subject_index]\n",
    "    clfs_list[subject_index].fit(dataset, y=dataset.description.labels, epochs=n_epochs);\n",
    "    best_validation_acc = clfs_list[subject_index].callbacks_[4][1].best_score_ # a hack to get best validation accuracy\n",
    "    best_validation_kappa = (2*best_validation_acc)-1\n",
    "    print(\"Best Cross Validation Kappa Score: {:.2f}\".format(best_validation_kappa))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      " ######################### Training for Subject: 1 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.6641\u001b[0m        \u001b[32m0.8516\u001b[0m            \u001b[35m0.5625\u001b[0m        \u001b[31m0.7750\u001b[0m  0.0200  1.1591\n",
      "      2            0.5156        \u001b[32m0.6555\u001b[0m            \u001b[35m0.5938\u001b[0m        1.5629  0.0194  1.0865\n",
      "      3            0.5000        \u001b[32m0.4358\u001b[0m            0.5312        3.2451  0.0177  1.0006\n",
      "      4            0.5000        \u001b[32m0.4021\u001b[0m            0.5000        5.8631  0.0150  1.0866\n",
      "      5            0.5000        \u001b[32m0.3263\u001b[0m            0.5000        6.2139  0.0117  0.9545\n",
      "      6            0.5000        \u001b[32m0.2675\u001b[0m            0.5000        5.5091  0.0083  1.3795\n",
      "      7            0.5000        \u001b[32m0.1801\u001b[0m            0.5000        5.0791  0.0050  1.2650\n",
      "      8            0.5000        0.1913            0.5000        4.5800  0.0023  1.6510\n",
      "      9            0.5078        \u001b[32m0.1650\u001b[0m            0.5000        3.9977  0.0006  1.3970\n",
      "     10            0.5156        0.1875            0.5000        3.5109  0.0000  1.3150\n",
      "Best Cross Validation Kappa Score: 0.19\n",
      "\n",
      " ######################### Training for Subject: 2 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.8672\u001b[0m        \u001b[32m0.8364\u001b[0m            \u001b[35m0.7500\u001b[0m        \u001b[31m0.6569\u001b[0m  0.0200  1.3150\n",
      "      2            0.8047        \u001b[32m0.5549\u001b[0m            0.6562        0.9251  0.0194  1.4250\n",
      "      3            0.7109        \u001b[32m0.3026\u001b[0m            0.5938        1.6436  0.0177  1.0710\n",
      "      4            \u001b[36m0.8906\u001b[0m        \u001b[32m0.2238\u001b[0m            0.6875        1.2248  0.0150  1.1080\n",
      "      5            \u001b[36m0.9141\u001b[0m        \u001b[32m0.2203\u001b[0m            0.6250        1.3217  0.0117  1.0400\n",
      "      6            \u001b[36m0.9219\u001b[0m        \u001b[32m0.1850\u001b[0m            0.6250        1.2127  0.0083  1.2070\n",
      "      7            \u001b[36m0.9609\u001b[0m        \u001b[32m0.1230\u001b[0m            0.6562        1.0630  0.0050  1.0030\n",
      "      8            \u001b[36m0.9844\u001b[0m        0.1330            0.6562        0.9570  0.0023  1.0970\n",
      "      9            0.9844        \u001b[32m0.1220\u001b[0m            0.6875        0.9274  0.0006  0.9990\n",
      "     10            \u001b[36m0.9922\u001b[0m        0.1333            0.6875        0.9098  0.0000  0.9710\n",
      "Best Cross Validation Kappa Score: 0.50\n",
      "\n",
      " ######################### Training for Subject: 3 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.5781\u001b[0m        \u001b[32m1.0901\u001b[0m            \u001b[35m0.6250\u001b[0m        \u001b[31m1.8178\u001b[0m  0.0200  1.1330\n",
      "      2            0.5000        \u001b[32m0.6837\u001b[0m            0.5000        3.4570  0.0194  1.4570\n",
      "      3            0.5391        \u001b[32m0.4919\u001b[0m            0.5312        2.6699  0.0177  1.2400\n",
      "      4            \u001b[36m0.7344\u001b[0m        \u001b[32m0.3490\u001b[0m            \u001b[35m0.6875\u001b[0m        \u001b[31m1.4513\u001b[0m  0.0150  1.1480\n",
      "      5            \u001b[36m0.8438\u001b[0m        \u001b[32m0.3224\u001b[0m            0.6875        \u001b[31m1.0657\u001b[0m  0.0117  1.4350\n",
      "      6            \u001b[36m0.8828\u001b[0m        \u001b[32m0.2068\u001b[0m            0.6875        \u001b[31m0.9245\u001b[0m  0.0083  1.5570\n",
      "      7            \u001b[36m0.8984\u001b[0m        0.2107            \u001b[35m0.7188\u001b[0m        \u001b[31m0.8750\u001b[0m  0.0050  1.4530\n",
      "      8            \u001b[36m0.9141\u001b[0m        0.2118            0.7188        \u001b[31m0.8433\u001b[0m  0.0023  1.2880\n",
      "      9            \u001b[36m0.9219\u001b[0m        0.2330            0.7188        \u001b[31m0.8092\u001b[0m  0.0006  1.1440\n",
      "     10            0.9219        0.2338            0.7188        \u001b[31m0.7810\u001b[0m  0.0000  1.1590\n",
      "Best Cross Validation Kappa Score: 0.44\n",
      "\n",
      " ######################### Training for Subject: 4 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.7734\u001b[0m        \u001b[32m0.7515\u001b[0m            \u001b[35m0.5938\u001b[0m        \u001b[31m0.8594\u001b[0m  0.0200  1.1780\n",
      "      2            \u001b[36m0.9531\u001b[0m        \u001b[32m0.3761\u001b[0m            \u001b[35m0.7188\u001b[0m        1.0998  0.0194  1.1880\n",
      "      3            0.9219        \u001b[32m0.3059\u001b[0m            0.7188        1.0835  0.0177  1.1300\n",
      "      4            0.9141        \u001b[32m0.2779\u001b[0m            0.6875        0.8691  0.0150  0.9960\n",
      "      5            0.9453        \u001b[32m0.1870\u001b[0m            \u001b[35m0.7500\u001b[0m        \u001b[31m0.7246\u001b[0m  0.0117  0.9670\n",
      "      6            0.8828        0.1969            0.6875        0.7460  0.0083  1.2240\n",
      "      7            0.8984        0.1875            0.6875        0.7747  0.0050  1.1680\n",
      "      8            0.8906        \u001b[32m0.1230\u001b[0m            0.6875        0.8369  0.0023  1.0330\n",
      "      9            0.8984        \u001b[32m0.0992\u001b[0m            0.6875        0.8442  0.0006  1.2090\n",
      "     10            0.9219        0.1103            0.6875        0.8384  0.0000  1.2310\n",
      "Best Cross Validation Kappa Score: 0.50\n",
      "\n",
      " ######################### Training for Subject: 5 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.5000\u001b[0m        \u001b[32m0.9027\u001b[0m            \u001b[35m0.5000\u001b[0m        \u001b[31m4.4622\u001b[0m  0.0200  1.2410\n",
      "      2            \u001b[36m0.8359\u001b[0m        \u001b[32m0.4829\u001b[0m            \u001b[35m0.8125\u001b[0m        \u001b[31m0.5014\u001b[0m  0.0194  1.0770\n",
      "      3            \u001b[36m0.8438\u001b[0m        \u001b[32m0.3446\u001b[0m            0.8125        0.6394  0.0177  1.3320\n",
      "      4            0.6406        0.3469            0.6250        1.2497  0.0150  1.1520\n",
      "      5            0.5312        \u001b[32m0.2654\u001b[0m            0.5312        1.7323  0.0117  1.1290\n",
      "      6            0.5000        \u001b[32m0.2313\u001b[0m            0.5000        2.4146  0.0083  1.5050\n",
      "      7            0.5000        \u001b[32m0.1921\u001b[0m            0.5000        2.3391  0.0050  1.2810\n",
      "      8            0.5000        \u001b[32m0.1768\u001b[0m            0.5312        2.0796  0.0023  1.2820\n",
      "      9            0.5078        0.1936            0.5312        1.8484  0.0006  1.2280\n",
      "     10            0.5156        0.2088            0.5000        1.6792  0.0000  1.0300\n",
      "Best Cross Validation Kappa Score: 0.62\n",
      "\n",
      " ######################### Training for Subject: 6 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.5000\u001b[0m        \u001b[32m0.6034\u001b[0m            \u001b[35m0.5312\u001b[0m        \u001b[31m3.8803\u001b[0m  0.0200  1.1680\n",
      "      2            \u001b[36m0.7344\u001b[0m        \u001b[32m0.5131\u001b[0m            \u001b[35m0.6250\u001b[0m        \u001b[31m1.5790\u001b[0m  0.0194  1.0720\n",
      "      3            0.6797        \u001b[32m0.3710\u001b[0m            0.5625        2.0135  0.0177  1.1040\n",
      "      4            \u001b[36m0.8984\u001b[0m        \u001b[32m0.2738\u001b[0m            \u001b[35m0.6875\u001b[0m        \u001b[31m1.2473\u001b[0m  0.0150  1.1500\n",
      "      5            \u001b[36m0.9766\u001b[0m        \u001b[32m0.2013\u001b[0m            \u001b[35m0.8125\u001b[0m        \u001b[31m0.6654\u001b[0m  0.0117  1.2250\n",
      "      6            0.9766        0.2033            0.7812        \u001b[31m0.4437\u001b[0m  0.0083  1.5290\n",
      "      7            0.9766        \u001b[32m0.1900\u001b[0m            0.7812        \u001b[31m0.4166\u001b[0m  0.0050  1.4150\n",
      "      8            \u001b[36m0.9844\u001b[0m        \u001b[32m0.1320\u001b[0m            \u001b[35m0.8438\u001b[0m        0.4297  0.0023  1.1790\n",
      "      9            0.9844        0.1573            0.8125        0.4421  0.0006  1.4640\n",
      "     10            0.9844        \u001b[32m0.1160\u001b[0m            0.8125        0.4429  0.0000  1.3230\n",
      "Best Cross Validation Kappa Score: 0.69\n",
      "\n",
      " ######################### Training for Subject: 7 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.5391\u001b[0m        \u001b[32m1.1134\u001b[0m            \u001b[35m0.5312\u001b[0m        \u001b[31m1.1119\u001b[0m  0.0200  1.3470\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      2            \u001b[36m0.5703\u001b[0m        \u001b[32m0.6242\u001b[0m            \u001b[35m0.5625\u001b[0m        \u001b[31m1.1025\u001b[0m  0.0194  1.2680\n",
      "      3            \u001b[36m0.6328\u001b[0m        \u001b[32m0.5345\u001b[0m            0.5625        \u001b[31m0.8642\u001b[0m  0.0177  1.1420\n",
      "      4            \u001b[36m0.7031\u001b[0m        \u001b[32m0.4364\u001b[0m            \u001b[35m0.6250\u001b[0m        \u001b[31m0.7926\u001b[0m  0.0150  1.3540\n",
      "      5            \u001b[36m0.8672\u001b[0m        \u001b[32m0.3806\u001b[0m            \u001b[35m0.6562\u001b[0m        \u001b[31m0.6081\u001b[0m  0.0117  1.1600\n",
      "      6            \u001b[36m0.9141\u001b[0m        \u001b[32m0.2874\u001b[0m            \u001b[35m0.6875\u001b[0m        \u001b[31m0.5892\u001b[0m  0.0083  1.1540\n",
      "      7            0.8906        \u001b[32m0.2688\u001b[0m            0.6250        \u001b[31m0.5826\u001b[0m  0.0050  1.1120\n",
      "      8            0.8672        \u001b[32m0.2622\u001b[0m            0.6562        0.5884  0.0023  0.9950\n",
      "      9            0.8906        \u001b[32m0.2316\u001b[0m            0.6250        0.5893  0.0006  0.9900\n",
      "     10            0.8984        0.2479            0.6250        0.5870  0.0000  1.0570\n",
      "Best Cross Validation Kappa Score: 0.38\n",
      "\n",
      " ######################### Training for Subject: 8 ######################### \n",
      "\n",
      "  epoch    train_accuracy    train_loss    valid_accuracy    valid_loss      lr     dur\n",
      "-------  ----------------  ------------  ----------------  ------------  ------  ------\n",
      "      1            \u001b[36m0.5000\u001b[0m        \u001b[32m0.9658\u001b[0m            \u001b[35m0.5000\u001b[0m        \u001b[31m2.3061\u001b[0m  0.0200  1.0590\n",
      "      2            \u001b[36m0.5312\u001b[0m        \u001b[32m0.6381\u001b[0m            \u001b[35m0.5312\u001b[0m        \u001b[31m1.8385\u001b[0m  0.0194  1.0360\n",
      "      3            0.5156        \u001b[32m0.5218\u001b[0m            0.5000        1.9158  0.0177  1.0210\n",
      "      4            \u001b[36m0.5469\u001b[0m        \u001b[32m0.3576\u001b[0m            0.5000        \u001b[31m1.6884\u001b[0m  0.0150  1.0380\n",
      "      5            \u001b[36m0.5859\u001b[0m        0.4058            0.5000        \u001b[31m1.3894\u001b[0m  0.0117  1.0550\n",
      "      6            \u001b[36m0.6484\u001b[0m        \u001b[32m0.3007\u001b[0m            0.5312        \u001b[31m1.2826\u001b[0m  0.0083  1.0490\n",
      "      7            \u001b[36m0.6641\u001b[0m        0.3074            0.5312        \u001b[31m1.2493\u001b[0m  0.0050  1.0810\n",
      "      8            \u001b[36m0.6797\u001b[0m        \u001b[32m0.2409\u001b[0m            0.5312        \u001b[31m1.1602\u001b[0m  0.0023  0.9790\n",
      "      9            \u001b[36m0.7031\u001b[0m        \u001b[32m0.2160\u001b[0m            0.5000        \u001b[31m1.1147\u001b[0m  0.0006  1.1150\n",
      "     10            \u001b[36m0.7266\u001b[0m        0.2489            0.5312        \u001b[31m1.0681\u001b[0m  0.0000  1.1400\n",
      "Best Cross Validation Kappa Score: 0.06\n"
     ]
    }
   ],
   "source": [
    "for subject in range(len(training_files)):\n",
    "    training_function(subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Note\n",
    "Somehow the problem seems to be with data augmentation as training acc is increasing but validation one isn't. Moreover, we need a large samples not just a hundred to get some results. Maybe cropped decoding would be better but who knows!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
