{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clinical BCI Challenge-WCCI2020\n",
    "- [website link](https://sites.google.com/view/bci-comp-wcci/?fbclid=IwAR37WLQ_xNd5qsZvktZCT8XJerHhmVb_bU5HDu69CnO85DE3iF0fs57vQ6M)\n",
    "\n",
    "\n",
    " - [Dataset Link](https://github.com/5anirban9/Clinical-Brain-Computer-Interfaces-Challenge-WCCI-2020-Glasgow)\n",
    " \n",
    " \n",
    " - [FBCSP Github Repo Link](https://github.com/jesus-333/FBCSP-Python)\n",
    " \n",
    "For FBCSP and EEGNET I will use hold-out cross-validation with 75:25\n",
    "\n",
    "I have changed the source code to give cohen's kappa score instead of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FBCSP.FBCSP_V4 import FBCSP_V4 as FBCSP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import mne\n",
    "from scipy.io import loadmat\n",
    "import scipy\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from mne.decoding import CSP\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as lda"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # to ignore warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False                    # global variable to suppress output display of MNE functions\n",
    "mne.set_log_level(verbose=verbose) # to suppress large info outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using kappa as evaluation metric\n",
    "kappa = sklearn.metrics.make_scorer(sklearn.metrics.cohen_kappa_score) # kappa scorer\n",
    "acc = sklearn.metrics.make_scorer(sklearn.metrics.accuracy_score)      # accuracy scorer\n",
    "scorer = kappa          # just assign another scorer to replace kappa scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = None  # for multicore parallel processing, set it to 1 if cause memory issues, for full utilization set to -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Conversion to MNE Datatypes\n",
    "[Mike Cohen Tutorials link for EEG Preprocessing](https://www.youtube.com/watch?v=uWB5tjhataY&list=PLn0OLiymPak2gDD-VDA90w9_iGDgOOb2o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_folder = globals()['_dh'][0]  # a hack to get path of current folder in which juptyter file is located\n",
    "data_path = os.path.join(current_folder, 'Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 8, 10)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files        = glob.glob(data_path + '/*.mat')\n",
    "training_files   = glob.glob(data_path + '/*T.mat')\n",
    "evaluation_files = glob.glob(data_path + '/*E.mat')\n",
    "len(all_files), len(training_files), len(evaluation_files)     # if these return zero,then no file is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mne_epochs(filepath, verbose=verbose, t_start=2, fs=512, mode='train'):\n",
    "    '''\n",
    "    This function reads the EEG data from .mat file and convert it to MNE-Python Compatible epochs\n",
    "    data structure. It takes data from [0, 8] sec range and return it by setting t = 0 at cue onset\n",
    "    i.e. 3 seconds and dropping first two seconds so the output data is in [-1.0, 5.0] sec range. The\n",
    "    Details can be found in the preprocessing section of the attached document\n",
    "    '''\n",
    "    mat_data = loadmat(filepath) # read .mat file\n",
    "    eeg_data= mat_data['RawEEGData']\n",
    "    idx_start = fs*t_start      \n",
    "    eeg_data = eeg_data[:, :, idx_start:]\n",
    "    event_id = {'left-hand': 1, 'right-hand': 2}\n",
    "    channel_names = ['F3', 'FC3', 'C3', 'CP3', 'P3', 'FCz', 'CPz', 'F4', 'FC4', 'C4', 'CP4', 'P4']\n",
    "    info = mne.create_info(ch_names=channel_names, sfreq=fs, ch_types='eeg')\n",
    "    epochs = mne.EpochsArray(eeg_data, info, verbose=verbose, tmin=t_start-3.0)\n",
    "    epochs.set_montage('standard_1020')\n",
    "    epochs.filter(1., None) \n",
    "    epochs.apply_baseline(baseline=(-.250, 0)) # linear baseline correction\n",
    "    \n",
    "    if mode == 'train': # this in only applicable for training data\n",
    "        epochs.event_id = event_id\n",
    "        epochs.events[:,2] = mat_data['Labels'].ravel()    \n",
    "    return epochs \n",
    "\n",
    "def get_labels(filepath):\n",
    "    mat_data = loadmat(filepath) # read .mat file\n",
    "    return mat_data['Labels'].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of EEG Data:  (80, 12, 3072) \t Shape of Labels:  (80,)\n"
     ]
    }
   ],
   "source": [
    "epochs, labels = get_mne_epochs(training_files[0], verbose=verbose), get_labels(training_files[0])\n",
    "data = epochs.get_data()\n",
    "print('Shape of EEG Data: ', data.shape, '\\t Shape of Labels: ', labels.shape) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'left-hand': 1, 'right-hand': 2}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs.event_id"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading original data\n",
    "epochs_list_train = []\n",
    "for i in training_files:\n",
    "    epochs_list_train.append(get_mne_epochs(i, verbose=verbose))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation Data\n",
    "first 8 for single subject and last 2 are for cross subject"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_list_eval = []\n",
    "for i in evaluation_files:\n",
    "    epochs_list_eval.append(get_mne_epochs(i, mode='test', verbose=verbose))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bandpass filtering of data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "for epoch in epochs_list_train:\n",
    "    epoch.filter(7.0, 32.0)\n",
    "    \n",
    "for epoch in epochs_list_eval:\n",
    "    epoch.filter(7.0, 32.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## FBCSP \n",
    "The class must receive in input with the initialization a training set inside a dictionary. The keys of the dictionary must be the label of the two class and each element must be a numpy matrix of dimension \"n. trials x n. channels x n.samples\". The class must also receive the frequency sampling of the signal.\n",
    "\n",
    "Once initialized the class automaticaly found the various spatial filter and the best features to train the classifier. The classifier used is an LDA classifier implemented through sklearn. You could also provided another sklean classifier during the initialization of the class if you prefer. The class split the data in a train and test set and then trains and evaluates the classifiers.\n",
    "\n",
    "An evalaution method to classify new trials is implemented with the name evaluateTrial(). The input of method must have the dimensions \"n. trials x n. channels x n.samples\". The method return a vector where each element is the label of the respective trial. The label are \"1\" for class 1 and \"2\" for class 2.\n",
    "\n",
    "**Note: I changed the source code to incorporate custom value of freq bands from (4, 40) to (7,32). The author has also changed it accordingly**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FBCSP.FBCSP_V4 import FBCSP_V4 as FBCSP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = epochs_list_train[0]\n",
    "data, labels = epochs.get_data(), epochs.events[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_dict = {'left-hand':  epochs['left-hand'].get_data()[:,:,256+512:-256], # [0.5, 4.5] sec data\n",
    "             'right-hand': epochs['right-hand'].get_data()[:,:,256+512:-256]}\n",
    "fs = epochs.info['sfreq']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------- FBCSP with LDA ---------------\n",
      "Features used for classification:  8\n",
      "Score on Training set:  0.7991071428571428\n",
      "Score on Validation set:  0.5833333333333333 \n",
      "\n",
      "--------------- FBCSP with Linear SVM ---------------\n",
      "Features used for classification:  8\n",
      "Score on Training set:  0.9332591768631813\n",
      "Score on Validation set:  0.801980198019802 \n",
      "\n"
     ]
    }
   ],
   "source": [
    "freqs_band = np.linspace(8, 40, 7)\n",
    "print('-'*15, 'FBCSP with LDA', '-'*15)\n",
    "fbcsp_clf = FBCSP(data_dict, fs, freqs_band=np.linspace(8, 32, 7), classifier=lda())\n",
    "print('-'*15, 'FBCSP with Linear SVM', '-'*15)\n",
    "fbcsp_clf = FBCSP(data_dict, fs, freqs_band=np.linspace(8, 32, 7), classifier=LinearSVC())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Trying MOABB Implementation of FBCSP"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Custom Transformer \n",
    "Applies CSP algo in each window of eeg separately and merge the features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "from mne.decoding import CSP\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "\n",
    "class Custom_Segmented_CSP(TransformerMixin, BaseEstimator):\n",
    "    \"\"\"\n",
    "    Apply CSP individually to each window and then merge their features\n",
    "    Expects data in the format (trials, channels, eeg_data)\n",
    "    individually apply CSP on each band and then concatenate to give output of the form (trials, csp_filtered_data)\n",
    "    By Default Applies CSP on a single window [0.0,4.0] sec\n",
    "    Note: This funciton expects arrays/lists as input for t_start and t_end\n",
    "    \"\"\"\n",
    "    def __init__(self, n_components=4, t_start=[0.0], t_end=[4.0], fs=512):\n",
    "        self.n_components = n_components           # csp components to retain\n",
    "        self.Csp = []                              # would carry list of CSP's applied individually to each window\n",
    "        self.t_start = t_start\n",
    "        self.t_end = t_end\n",
    "        self.fs = 512\n",
    "        self.start_idxs = (np.array(self.t_start)*self.fs).astype(np.int)\n",
    "        self.end_idxs =   (np.array(self.t_end)*self.fs).astype(np.int)\n",
    "        self.num_windows = 0\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        self.num_windows = len(self.start_idxs)\n",
    "        self.Csp = [CSP(n_components=self.n_components) for _ in range(self.num_windows)]\n",
    "        for i in range(self.num_windows):\n",
    "            x_seg = x[:,:,self.start_idxs[i]:self.end_idxs[i]]\n",
    "            self.Csp[i].fit(x_seg, y)\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y=None):\n",
    "        dummy_array = []\n",
    "        for i in range(self.num_windows):\n",
    "            x_seg = x[:,:,self.start_idxs[i]:self.end_idxs[i]]\n",
    "            dummy_array.append(self.Csp[i].transform(x_seg))\n",
    "        return np.concatenate(dummy_array, axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets try doing some classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs = epochs_list_train[4]\n",
    "data = epochs.get_data()\n",
    "data = data[:,:,512+256:-256] # # from 0.5s to 4.5s\n",
    "labels = epochs.events[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train set:  features:  (60, 12, 2048) labels:  (60,)\n",
      "Test  set:  features:  (20, 12, 2048) labels:  (20,)\n"
     ]
    }
   ],
   "source": [
    "x_trainVal, x_test, y_trainVal, y_test = train_test_split(data, labels.ravel(), shuffle=True, stratify=labels, random_state=0) # to avoid confusing names and reusing x_trainVal\n",
    "print('train set:  features: ', x_trainVal.shape, 'labels: ', y_trainVal.shape)\n",
    "print('Test  set:  features: ', x_test.shape, 'labels: ', y_test.shape)\n",
    "y_train = y_trainVal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "scorer?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits=5, random_state=0) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Classification Scores Comparison with default Parameters **********\n",
      "KNN           :  0.4666666666666668\n",
      "Log-Regression:  0.7333333333333334\n",
      "Linear SVM    :  0.6666666666666667\n",
      "kernal SVM    :  0.4666666666666668\n",
      "LDA           :  0.2666666666666667\n"
     ]
    }
   ],
   "source": [
    "# using all channels, custom csp on 11 overlapping windows of segmented data\n",
    "t_start = np.arange(0,1.6,0.15)\n",
    "length_window = 2.5\n",
    "t_end = t_start + length_window\n",
    "i = 4\n",
    "x_train = x_trainVal\n",
    "print('*'*10, 'Classification Scores Comparison with default Parameters' ,'*'*10)\n",
    "print('KNN           : ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), KNeighborsClassifier()), x_train, y_train, cv=cv, scoring=scorer)))\n",
    "print('Log-Regression: ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), LogisticRegression(max_iter=1000)), x_train, y_train, cv=cv, scoring=scorer)))\n",
    "print('Linear SVM    : ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), LinearSVC(random_state=0)), x_train, y_train, cv=cv, scoring=scorer)))\n",
    "print('kernal SVM    : ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), SVC(gamma='scale')), x_train, y_train, cv=cv, scoring=scorer)))\n",
    "print('LDA           : ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), lda()), x_train, y_train, cv=cv, scoring=scorer)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Classification Scores Comparison with default Parameters **********\n",
      "KNN           :  0.4666666666666668\n",
      "Log-Regression:  0.7333333333333334\n",
      "Linear SVM    :  0.6666666666666667\n",
      "kernal SVM    :  0.4666666666666668\n",
      "LDA           :  0.4666666666666667\n"
     ]
    }
   ],
   "source": [
    "# using all channels, custom csp on 5 overlapping windows of segmented data\n",
    "t_start = np.arange(0,1.6,0.375)\n",
    "length_window = 2.5\n",
    "t_end = t_start + length_window\n",
    "i = 4\n",
    "x_train = x_trainVal\n",
    "print('*'*10, 'Classification Scores Comparison with default Parameters' ,'*'*10)\n",
    "print('KNN           : ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), KNeighborsClassifier()), x_train, y_train, cv=cv, scoring=scorer)))\n",
    "print('Log-Regression: ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), LogisticRegression(max_iter=1000)), x_train, y_train, cv=cv, scoring=scorer)))\n",
    "print('Linear SVM    : ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), LinearSVC(random_state=0)), x_train, y_train, cv=cv, scoring=scorer)))\n",
    "print('kernal SVM    : ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), SVC(gamma='scale')), x_train, y_train, cv=cv, scoring=scorer)))\n",
    "print('LDA           : ', np.mean(cross_val_score(make_pipeline(Custom_Segmented_CSP(i,t_start,t_end), lda()), x_train, y_train, cv=cv, scoring=scorer)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search\n",
    "- csp_comps = 4 only \n",
    "- CV=10\n",
    "- 5 windows with 2.5s window size and 375 ms displacement"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------- Information About Window Selection ----------\n",
      "Total Windows:  5\n",
      "Starting Time of Windows:  [0.    0.375 0.75  1.125 1.5  ]\n",
      "Ending Time of Windows  : [2.5   2.875 3.25  3.625 4.   ]\n"
     ]
    }
   ],
   "source": [
    "print('-'*10, 'Information About Window Selection', '-'*10)\n",
    "t_start = np.arange(0,1.501,0.375)\n",
    "length_window = 2.5\n",
    "t_end = t_start + length_window\n",
    "print('Total Windows: ', len(t_start))\n",
    "print('Starting Time of Windows: ', t_start)\n",
    "print('Ending Time of Windows  :' ,  t_end)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedShuffleSplit(10, random_state=0)\n",
    "csp_comps = [4]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for linear svm\n",
    "param_grid_linear_svm =   { 'linearsvc__C' : np.logspace(-4, 3, 15), \n",
    "                           'custom_segmented_csp__n_components': csp_comps}\n",
    "\n",
    "# lda, auto shrinkage performs pretty well mostly \n",
    "shrinkage = list(np.arange(0,1.01,0.1))\n",
    "shrinkage.append('auto')\n",
    "param_grid_lda = {'custom_segmented_csp__n_components': csp_comps, \n",
    "                  'lineardiscriminantanalysis__shrinkage': shrinkage} "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "grids_linear_svm_list = [GridSearchCV(make_pipeline(Custom_Segmented_CSP(t_start=t_start, t_end=t_end), \n",
    "                                                    LinearSVC(random_state=0)), \n",
    "                               param_grid=param_grid_linear_svm, cv=cv, n_jobs=n_jobs, scoring=scorer)\n",
    "                        for _ in range(len(training_files))]\n",
    "\n",
    "grids_lda_list = [GridSearchCV(make_pipeline(Custom_Segmented_CSP(t_start=t_start, t_end=t_end), lda(solver='eigen')), \n",
    "                        param_grid=param_grid_lda, cv=cv, n_jobs=n_jobs, scoring=scorer)\n",
    "                 for _ in range(len(training_files))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_function(subject_index=0):\n",
    "    # this time training function trains on whole training set\n",
    "    print('-'*25, 'Training for Subject:', subject_index+1, '-'*25)\n",
    "    epochs = epochs_list_train[subject_index]\n",
    "    data = epochs.get_data()\n",
    "    data = data[:,:,256+512:-256] # from 0.5s to 4.5s\n",
    "    labels = epochs.events[:,-1]\n",
    "\n",
    "    grids_linear_svm_list[subject_index].fit(data, labels)\n",
    "    print('LinearSVM: Maximum Cross Validation Score = ', round(grids_linear_svm_list[subject_index].best_score_,3))\n",
    "    grids_lda_list[subject_index].fit(data, labels)\n",
    "    print('LDA      : Maximum Cross Validation Score = ', round(grids_lda_list[subject_index].best_score_,3))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluation_function(subject_index=0):  \n",
    "    # prints the prediction counts for each class\n",
    "    epochs = epochs_list_eval[subject_index]\n",
    "    data = epochs.get_data()\n",
    "    data = data[:,:,256+512:-256] # from 0.5s to 4.5s\n",
    "    preds_linear_svm = grids_linear_svm_list[subject_index].predict(data)\n",
    "    preds_lda =  grids_lda_list[subject_index].predict(data)\n",
    "    print('-'*25, 'Predictions Counts Subject:', subject_index+1, '-'*25)\n",
    "    print('Linear SVM: Class 1 =', sum(preds_linear_svm==1), 'Class 2 =', sum(preds_linear_svm==2))\n",
    "    print('LDA       : Class 1 =', sum(preds_lda==1), 'Class 2 =', sum(preds_lda==2))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It's Training Time\n",
    "with 5 overlapping windows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- Training for Subject: 1 -------------------------\n",
      "LinearSVM: Maximum Cross Validation Score =  0.75\n",
      "LDA      : Maximum Cross Validation Score =  0.75\n",
      "\n",
      "------------------------- Training for Subject: 2 -------------------------\n",
      "LinearSVM: Maximum Cross Validation Score =  0.85\n",
      "LDA      : Maximum Cross Validation Score =  0.775\n",
      "\n",
      "------------------------- Training for Subject: 3 -------------------------\n",
      "LinearSVM: Maximum Cross Validation Score =  0.6\n",
      "LDA      : Maximum Cross Validation Score =  0.6\n",
      "\n",
      "------------------------- Training for Subject: 4 -------------------------\n",
      "LinearSVM: Maximum Cross Validation Score =  0.575\n",
      "LDA      : Maximum Cross Validation Score =  0.625\n",
      "\n",
      "------------------------- Training for Subject: 5 -------------------------\n",
      "LinearSVM: Maximum Cross Validation Score =  0.725\n",
      "LDA      : Maximum Cross Validation Score =  0.725\n",
      "\n",
      "------------------------- Training for Subject: 6 -------------------------\n",
      "LinearSVM: Maximum Cross Validation Score =  0.525\n",
      "LDA      : Maximum Cross Validation Score =  0.575\n",
      "\n",
      "------------------------- Training for Subject: 7 -------------------------\n",
      "LinearSVM: Maximum Cross Validation Score =  0.35\n",
      "LDA      : Maximum Cross Validation Score =  0.35\n",
      "\n",
      "------------------------- Training for Subject: 8 -------------------------\n",
      "LinearSVM: Maximum Cross Validation Score =  0.35\n",
      "LDA      : Maximum Cross Validation Score =  0.275\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for subject in range(len(training_files)):\n",
    "    training_function(subject)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- Predictions Counts Subject: 1 -------------------------\n",
      "Linear SVM: Class 1 = 17 Class 2 = 23\n",
      "LDA       : Class 1 = 20 Class 2 = 20\n",
      "\n",
      "------------------------- Predictions Counts Subject: 2 -------------------------\n",
      "Linear SVM: Class 1 = 28 Class 2 = 12\n",
      "LDA       : Class 1 = 19 Class 2 = 21\n",
      "\n",
      "------------------------- Predictions Counts Subject: 3 -------------------------\n",
      "Linear SVM: Class 1 = 26 Class 2 = 14\n",
      "LDA       : Class 1 = 28 Class 2 = 12\n",
      "\n",
      "------------------------- Predictions Counts Subject: 4 -------------------------\n",
      "Linear SVM: Class 1 = 3 Class 2 = 37\n",
      "LDA       : Class 1 = 2 Class 2 = 38\n",
      "\n",
      "------------------------- Predictions Counts Subject: 5 -------------------------\n",
      "Linear SVM: Class 1 = 30 Class 2 = 10\n",
      "LDA       : Class 1 = 26 Class 2 = 14\n",
      "\n",
      "------------------------- Predictions Counts Subject: 6 -------------------------\n",
      "Linear SVM: Class 1 = 19 Class 2 = 21\n",
      "LDA       : Class 1 = 24 Class 2 = 16\n",
      "\n",
      "------------------------- Predictions Counts Subject: 7 -------------------------\n",
      "Linear SVM: Class 1 = 4 Class 2 = 36\n",
      "LDA       : Class 1 = 9 Class 2 = 31\n",
      "\n",
      "------------------------- Predictions Counts Subject: 8 -------------------------\n",
      "Linear SVM: Class 1 = 19 Class 2 = 21\n",
      "LDA       : Class 1 = 19 Class 2 = 21\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for subject in range(len(training_files)):\n",
    "    evaluation_function(subject)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Winners for individual subjects\n",
    "- Linear SVM:  2, 3, 8\n",
    "- LDA       :  1, 4, 5,6, 7"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
