{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clinical BCI Challenge-WCCI2020\n",
    "- [website link](https://sites.google.com/view/bci-comp-wcci/?fbclid=IwAR37WLQ_xNd5qsZvktZCT8XJerHhmVb_bU5HDu69CnO85DE3iF0fs57vQ6M)\n",
    "\n",
    "\n",
    "- [Dataset Link](https://github.com/5anirban9/Clinical-Brain-Computer-Interfaces-Challenge-WCCI-2020-Glasgow)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import mne\n",
    "from scipy.io import loadmat\n",
    "import scipy\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from mne.decoding import CSP\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedShuffleSplit, RandomizedSearchCV\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as lda\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # to ignore warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False                    # global variable to suppress output display of MNE functions\n",
    "mne.set_log_level(verbose=verbose) # to suppress large info outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using kappa as evaluation metric\n",
    "kappa = sklearn.metrics.make_scorer(sklearn.metrics.cohen_kappa_score) # kappa scorer\n",
    "acc = sklearn.metrics.make_scorer(sklearn.metrics.accuracy_score)      # accuracy scorer\n",
    "scorer = kappa          # just assign another scorer to replace kappa scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = -1  # for multicore parallel processing, set it to 1 if cause memory issues, for full utilization set to -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Conversion to MNE Datatypes\n",
    "[Mike Cohen Tutorials link for EEG Preprocessing](https://www.youtube.com/watch?v=uWB5tjhataY&list=PLn0OLiymPak2gDD-VDA90w9_iGDgOOb2o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_folder = globals()['_dh'][0]  # a hack to get path of current folder in which juptyter file is located\n",
    "data_path = os.path.join(current_folder, 'Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18, 8, 10)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_files        = glob.glob(data_path + '/*.mat')\n",
    "training_files   = glob.glob(data_path + '/*T.mat')\n",
    "evaluation_files = glob.glob(data_path + '/*E.mat')\n",
    "len(all_files), len(training_files), len(evaluation_files)     # if these return zero,then no file is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mne_epochs(filepath, verbose=verbose, t_start=2, fs=512, mode='train'):\n",
    "    '''\n",
    "    This function reads the EEG data from .mat file and convert it to MNE-Python Compatible epochs\n",
    "    data structure. It takes data from [0, 8] sec range and return it by setting t = 0 at cue onset\n",
    "    i.e. 3 seconds and dropping first two seconds so the output data is in [-1.0, 5.0] sec range. The\n",
    "    Details can be found in the preprocessing section of the attached document\n",
    "    '''\n",
    "    mat_data = loadmat(filepath) # read .mat file\n",
    "    eeg_data= mat_data['RawEEGData']\n",
    "    idx_start = fs*t_start      \n",
    "    eeg_data = eeg_data[:, :, idx_start:]\n",
    "    event_id = {'left-hand': 1, 'right-hand': 2}\n",
    "    channel_names = ['F3', 'FC3', 'C3', 'CP3', 'P3', 'FCz', 'CPz', 'F4', 'FC4', 'C4', 'CP4', 'P4']\n",
    "    info = mne.create_info(ch_names=channel_names, sfreq=fs, ch_types='eeg')\n",
    "    epochs = mne.EpochsArray(eeg_data, info, verbose=verbose, tmin=t_start-3.0)\n",
    "    epochs.set_montage('standard_1020')\n",
    "    epochs.filter(1., None) \n",
    "    epochs.apply_baseline(baseline=(-.250, 0)) # linear baseline correction\n",
    "    \n",
    "    if mode == 'train': # this in only applicable for training data\n",
    "        epochs.event_id = event_id\n",
    "        epochs.events[:,2] = mat_data['Labels'].ravel()    \n",
    "    return epochs \n",
    "\n",
    "def get_labels(filepath):\n",
    "    mat_data = loadmat(filepath) # read .mat file\n",
    "    return mat_data['Labels'].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of EEG Data:  (80, 12, 3072) \t Shape of Labels:  (80,)\n"
     ]
    }
   ],
   "source": [
    "epochs, labels = get_mne_epochs(training_files[0], verbose=verbose), get_labels(training_files[0])\n",
    "data = epochs.get_data()\n",
    "print('Shape of EEG Data: ', data.shape, '\\t Shape of Labels: ', labels.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets Append Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mne_epochs_complete(files_paths, verbose=verbose, t_start=2, fs=512, mode='train'):\n",
    "    '''\n",
    "    similar to get_mne_epochs, just appends data from all relevant files together to give a single\n",
    "    epoch object\n",
    "    '''\n",
    "    eeg_data = []\n",
    "    for filepath in files_paths:\n",
    "        mat_data = loadmat(filepath)\n",
    "        eeg_data.extend(mat_data['RawEEGData'])\n",
    "\n",
    "    idx_start = fs*t_start      # fs*ts\n",
    "    eeg_data = np.array(eeg_data)\n",
    "    eeg_data = eeg_data[:, :, idx_start:]\n",
    "    event_id = {'left-hand': 1, 'right-hand': 2}\n",
    "    channel_names = ['F3', 'FC3', 'C3', 'CP3', 'P3', 'FCz', 'CPz', 'F4', 'FC4', 'C4', 'CP4', 'P4']\n",
    "    info = mne.create_info(ch_names=channel_names, sfreq=fs, ch_types='eeg')\n",
    "    epochs = mne.EpochsArray(eeg_data, info, verbose=verbose, tmin=t_start-3.0)\n",
    "    epochs.set_montage('standard_1020')\n",
    "    epochs.filter(1., None) # required be ICA, (7-30 Hz) later\n",
    "    epochs.apply_baseline(baseline=(-.250, 0)) # linear baseline correction\n",
    "    \n",
    "    if mode == 'train': # this in only applicable for training data\n",
    "        labels = []\n",
    "        for filepath in files_paths:\n",
    "            mat_data = loadmat(filepath)\n",
    "            labels.extend(mat_data['Labels'].ravel())\n",
    "        epochs.event_id = event_id\n",
    "        epochs.events[:,2] = labels    \n",
    "    return epochs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading with Band Pass Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading relevant files\n",
    "training_epochs_all = get_mne_epochs_complete(training_files).filter(7,32)            # for all training subjects\n",
    "evaluation_epochs_9 = get_mne_epochs(evaluation_files[-2], mode='eval').filter(7,32)  # for subject 9\n",
    "evaluation_epochs_10 = get_mne_epochs(evaluation_files[-1], mode='eval').filter(7,32) # for subject 10"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Leave One Group Out CV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group parameter for leave one group out cross validation in sklearn, each subject is given unique identifier\n",
    "group_list = []\n",
    "for subject in np.linspace(1,8,8):\n",
    "    group_list.extend([subject for _ in range(80)]) # since we have 80 samples in each training file\n",
    "groups = np.array(group_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = LeaveOneGroupOut()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets try classification with psds as features\n",
    "faster than applying csp and results aren't bad either"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Classification Scores Comparison with default Parameters **********\n",
      "KNN           :  0.2\n",
      "Log-Regression:  0.3375\n",
      "Linear SVM    :  0.3375\n",
      "kernal SVM    :  0.33125\n",
      "LDA           :  0.21250000000000002\n"
     ]
    }
   ],
   "source": [
    "# from 0.5-4.5 sec\n",
    "epochs = training_epochs_all.copy()\n",
    "psds, freqs = mne.time_frequency.psd_multitaper(epochs, tmin=0.5, tmax=4.5, fmin=8, fmax=30 ,n_jobs=1)\n",
    "psds = 10 * np.log10(psds) # to convert powers to DB\n",
    "labels = epochs.events[:,-1]\n",
    "psds = psds.reshape(psds.shape[0], -1)\n",
    "x_train = psds\n",
    "y_train = labels\n",
    "\n",
    "print('*'*10, 'Classification Scores Comparison with default Parameters' ,'*'*10)\n",
    "print('KNN           : ', np.mean(cross_val_score(make_pipeline(StandardScaler(),KNeighborsClassifier()), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))\n",
    "print('Log-Regression: ', np.mean(cross_val_score(make_pipeline(StandardScaler(),LogisticRegression(max_iter=1000)), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))\n",
    "print('Linear SVM    : ', np.mean(cross_val_score(make_pipeline(StandardScaler(),LinearSVC(random_state=0)), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))\n",
    "print('kernal SVM    : ', np.mean(cross_val_score(make_pipeline(StandardScaler(), SVC(gamma='scale')), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))\n",
    "print('LDA           : ', np.mean(cross_val_score(make_pipeline(StandardScaler(), lda()), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Classification Scores Comparison with default Parameters **********\n",
      "KNN           :  0.23750000000000004\n",
      "Log-Regression:  0.359375\n",
      "Linear SVM    :  0.33125\n",
      "kernal SVM    :  0.35624999999999996\n",
      "LDA           :  0.24375\n"
     ]
    }
   ],
   "source": [
    "# from 1.5-4.5 sec\n",
    "epochs = training_epochs_all.copy()\n",
    "psds, freqs = mne.time_frequency.psd_multitaper(epochs, tmin=1.5, tmax=4.5, fmin=8, fmax=30 ,n_jobs=1)\n",
    "psds = 10 * np.log10(psds) # to convert powers to DB\n",
    "labels = epochs.events[:,-1]\n",
    "psds = psds.reshape(psds.shape[0], -1)\n",
    "x_train = psds\n",
    "y_train = labels\n",
    "\n",
    "# using all channels\n",
    "print('*'*10, 'Classification Scores Comparison with default Parameters' ,'*'*10)\n",
    "print('KNN           : ', np.mean(cross_val_score(make_pipeline(StandardScaler(),KNeighborsClassifier()), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))\n",
    "print('Log-Regression: ', np.mean(cross_val_score(make_pipeline(StandardScaler(),LogisticRegression(max_iter=1000)), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))\n",
    "print('Linear SVM    : ', np.mean(cross_val_score(make_pipeline(StandardScaler(),LinearSVC(random_state=0)), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))\n",
    "print('kernal SVM    : ', np.mean(cross_val_score(make_pipeline(StandardScaler(), SVC(gamma='scale')), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))\n",
    "print('LDA           : ', np.mean(cross_val_score(make_pipeline(StandardScaler(), lda()), x_train, y_train, cv=cv, scoring=scorer, groups=groups)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Grid Search \n",
    "from 1.5-4.5 sec window"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for training set\n",
    "psds_train, _ = mne.time_frequency.psd_multitaper(training_epochs_all.copy(), tmin=1.5, tmax=4.5, fmin=8, fmax=30 ,n_jobs=1)\n",
    "psds_train = 10 * np.log10(psds_train) # to convert powers to dB"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train  =  psds_train.reshape(psds_train.shape[0], -1)\n",
    "y_train = training_epochs_all.events[:,-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Data Shape  :  (640, 792)\n",
      "Training Labels Shape:  (640,)\n"
     ]
    }
   ],
   "source": [
    "print('Training Data Shape  : ', x_train.shape)\n",
    "print('Training Labels Shape: ', y_train.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Cross Validation Score: 0.244\n",
      "Optimal Parameters:  {'kneighborsclassifier__n_neighbors': 8}\n"
     ]
    }
   ],
   "source": [
    "param_grid_knn = {'kneighborsclassifier__n_neighbors': np.arange(2,15,2)}\n",
    "grid_knn = GridSearchCV(make_pipeline(StandardScaler(), KNeighborsClassifier()), \n",
    "                        param_grid=param_grid_knn, cv=cv, scoring=scorer, n_jobs=n_jobs)\n",
    "grid_knn.fit(x_train, y_train, groups=groups) \n",
    "print('Maximum Cross Validation Score: {:.3f}'.format(grid_knn.best_score_))\n",
    "print('Optimal Parameters: ', grid_knn.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Cross Validation Score:  0.422\n",
      "Optimal Parameters:  {'logisticregression__C': 0.0025118864315095794}\n"
     ]
    }
   ],
   "source": [
    "# for logistic regression\n",
    "param_grid_log_reg = {'logisticregression__C' : np.logspace(-4, 3, 16)}\n",
    "grid_log_reg = GridSearchCV(make_pipeline(StandardScaler(), LogisticRegression()), \n",
    "                            param_grid=param_grid_log_reg, cv=cv, scoring=scorer) \n",
    "grid_log_reg.fit(x_train, y_train, groups=groups) \n",
    "print('Maximum Cross Validation Score: ',  round(grid_log_reg.best_score_,3))\n",
    "print('Optimal Parameters: ', grid_log_reg.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Cross Validation Score:  0.434\n",
      "Optimal Parameters:  {'linearsvc__C': 0.0031622776601683794}\n"
     ]
    }
   ],
   "source": [
    "# for linear svm\n",
    "param_grid_linear_svm =     {'linearsvc__C' : np.logspace(-4, 3, 15)}\n",
    "grid_linear_svm = GridSearchCV(make_pipeline(StandardScaler(), LinearSVC()), \n",
    "                               param_grid=param_grid_linear_svm, cv=cv, scoring=scorer, n_jobs=n_jobs)\n",
    "grid_linear_svm.fit(x_train, y_train, groups=groups) \n",
    "print('Maximum Cross Validation Score: ',  round(grid_linear_svm.best_score_,3))\n",
    "print('Optimal Parameters: ', grid_linear_svm.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Cross Validation Score:  0.4\n",
      "Optimal Parameters:  {'svc__C': 100.0, 'svc__gamma': 1.5625e-05}\n"
     ]
    }
   ],
   "source": [
    "# kernel svm\n",
    "param_grid_kernel_svm = {'svc__C': np.logspace(-4, 3, 8),\n",
    "                         'svc__gamma': np.logspace(-4, 2, 4) / x_train.shape[0]} \n",
    "                         \n",
    "grid_kernel_svm = GridSearchCV(make_pipeline(StandardScaler(), SVC()), \n",
    "                            param_grid=param_grid_kernel_svm, cv=cv, scoring=scorer, n_jobs=n_jobs)\n",
    "grid_kernel_svm.fit(x_train, y_train, groups=groups) \n",
    "print('Maximum Cross Validation Score: ',  round(grid_kernel_svm.best_score_,3))\n",
    "print('Optimal Parameters: ', grid_kernel_svm.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum Cross Validation Score:  0.434\n",
      "Optimal Parameters:  {'lineardiscriminantanalysis__shrinkage': 0.5}\n"
     ]
    }
   ],
   "source": [
    "# lda, auto shrinkage works pretty well\n",
    "shrinkage = list(np.arange(0.0,1.01,0.05))\n",
    "shrinkage.append('auto')\n",
    "\n",
    "param_grid_lda = {'lineardiscriminantanalysis__shrinkage': shrinkage}   \n",
    "grid_lda = GridSearchCV(make_pipeline(StandardScaler(), lda(solver='eigen')), \n",
    "                        param_grid=param_grid_lda, cv=cv, scoring=scorer, n_jobs=n_jobs)\n",
    "grid_lda.fit(x_train, y_train, groups=groups) \n",
    "print('Maximum Cross Validation Score: ',  round(grid_lda.best_score_,3))\n",
    "print('Optimal Parameters: ', grid_lda.best_params_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Predictions Counts on Evaluation set **********\n",
      "KNN          :  Class 1 = 22 Class 2 = 18\n",
      "LogReg       :  Class 1 = 32 Class 2 = 8\n",
      "LinearSVM    :  Class 1 = 33 Class 2 = 7\n",
      "KernelSVM    :  Class 1 = 30 Class 2 = 10\n",
      "LDA          :  Class 1 = 34 Class 2 = 6\n",
      "RandomForest :  Class 1 = 29 Class 2 = 11\n"
     ]
    }
   ],
   "source": [
    "# predictions counts on evaluation subject 9\n",
    "psds_eval, _ = mne.time_frequency.psd_multitaper(evaluation_epochs_9.copy(), tmin=1.5, tmax=4.5, fmin=8, fmax=30 ,n_jobs=1)\n",
    "psds_eval = 10 * np.log10(psds_eval) # to convert powers to DB\n",
    "x_eval = psds_eval.reshape(psds_eval.shape[0], -1)\n",
    "\n",
    "preds_knn = grid_knn.predict(x_eval)\n",
    "preds_log_reg = grid_log_reg.predict(x_eval)\n",
    "preds_linear_svm = grid_linear_svm.predict(x_eval)\n",
    "preds_kernel_svm = grid_kernel_svm.predict(x_eval)\n",
    "preds_lda = grid_lda.predict(x_eval)\n",
    "print('*'*10, 'Predictions Counts on Subject 9' ,'*'*10)\n",
    "print('KNN          : ', 'Class 1 =', sum(preds_knn==1), 'Class 2 =', sum(preds_knn==2)) \n",
    "print('LogReg       : ', 'Class 1 =', sum(preds_log_reg==1), 'Class 2 =', sum(preds_log_reg==2))\n",
    "print('LinearSVM    : ', 'Class 1 =', sum(preds_linear_svm==1), 'Class 2 =', sum(preds_linear_svm==2))\n",
    "print('KernelSVM    : ', 'Class 1 =', sum(preds_kernel_svm==1), 'Class 2 =', sum(preds_kernel_svm==2))\n",
    "print('LDA          : ', 'Class 1 =', sum(preds_lda==1), 'Class 2 =', sum(preds_lda==2)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "********** Predictions Counts on Evaluation set **********\n",
      "KNN          :  Class 1 = 22 Class 2 = 18\n",
      "LogReg       :  Class 1 = 18 Class 2 = 22\n",
      "LinearSVM    :  Class 1 = 15 Class 2 = 25\n",
      "KernelSVM    :  Class 1 = 15 Class 2 = 25\n",
      "LDA          :  Class 1 = 11 Class 2 = 29\n",
      "RandomForest :  Class 1 = 21 Class 2 = 19\n"
     ]
    }
   ],
   "source": [
    "# predictions counts on evaluation subject 10\n",
    "psds_eval, _ = mne.time_frequency.psd_multitaper(evaluation_epochs_10.copy(), tmin=1.5, tmax=4.5, fmin=8, fmax=30 ,n_jobs=1)\n",
    "psds_eval = 10 * np.log10(psds_eval) # to convert powers to DB\n",
    "x_eval = psds_eval.reshape(psds_eval.shape[0], -1)\n",
    "\n",
    "preds_knn = grid_knn.predict(x_eval)\n",
    "preds_log_reg = grid_log_reg.predict(x_eval)\n",
    "preds_linear_svm = grid_linear_svm.predict(x_eval)\n",
    "preds_kernel_svm = grid_kernel_svm.predict(x_eval)\n",
    "preds_lda = grid_lda.predict(x_eval)\n",
    "print('*'*10, 'Predictions Counts on Subject 10' ,'*'*10)\n",
    "print('KNN          : ', 'Class 1 =', sum(preds_knn==1), 'Class 2 =', sum(preds_knn==2)) \n",
    "print('LogReg       : ', 'Class 1 =', sum(preds_log_reg==1), 'Class 2 =', sum(preds_log_reg==2))\n",
    "print('LinearSVM    : ', 'Class 1 =', sum(preds_linear_svm==1), 'Class 2 =', sum(preds_linear_svm==2))\n",
    "print('KernelSVM    : ', 'Class 1 =', sum(preds_kernel_svm==1), 'Class 2 =', sum(preds_kernel_svm==2))\n",
    "print('LDA          : ', 'Class 1 =', sum(preds_lda==1), 'Class 2 =', sum(preds_lda==2)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Results\n",
    "results are good for subject 10 and bad for 9. linear svm and lda both good. I'd pick linSVM as the winner."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
