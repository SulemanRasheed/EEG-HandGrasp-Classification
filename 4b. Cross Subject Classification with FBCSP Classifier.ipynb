{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clinical BCI Challenge-WCCI2020\n",
    "- [website link](https://sites.google.com/view/bci-comp-wcci/?fbclid=IwAR37WLQ_xNd5qsZvktZCT8XJerHhmVb_bU5HDu69CnO85DE3iF0fs57vQ6M)\n",
    "\n",
    "\n",
    " - [Dataset Link](https://github.com/5anirban9/Clinical-Brain-Computer-Interfaces-Challenge-WCCI-2020-Glasgow)\n",
    " \n",
    " \n",
    " - [FBCSP Github Repo Link](https://github.com/jesus-333/FBCSP-Python)\n",
    " \n",
    "I have changed the source code to give cohen's kappa score instead of accuracy as an evaluation measure and I have also made a few other small changes as well. Moreover, for cross subject analysis I have also changed the implementation stuff a bit. \n",
    "\n",
    "LinearSVM showed some errors while making predictions from evaluateTrail() method. So, I replaced it with LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FBCSP.FBCSP_V4_CS import FBCSP_V4 as FBCSP "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import mne\n",
    "from scipy.io import loadmat\n",
    "import scipy\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import glob\n",
    "from mne.decoding import CSP\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as lda\n",
    "from sklearn.model_selection import LeaveOneGroupOut\n",
    "from sklearn.metrics import cohen_kappa_score as kappa_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # to ignore warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False                    # global variable to suppress output display of MNE functions\n",
    "mne.set_log_level(verbose=verbose) # to suppress large info outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose_clf = True # control output of FBCSP function\n",
    "freqs_band = np.linspace(8, 32, 7) # filter bank choice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using kappa as evaluation metric\n",
    "kappa = sklearn.metrics.make_scorer(sklearn.metrics.cohen_kappa_score) # kappa scorer\n",
    "acc = sklearn.metrics.make_scorer(sklearn.metrics.accuracy_score)      # accuracy scorer\n",
    "scorer = kappa          # just assign another scorer to replace kappa scorer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Conversion to MNE Datatypes\n",
    "[Mike Cohen Tutorials link for EEG Preprocessing](https://www.youtube.com/watch?v=uWB5tjhataY&list=PLn0OLiymPak2gDD-VDA90w9_iGDgOOb2o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_folder = globals()['_dh'][0]  # a hack to get path of current folder in which juptyter file is located\n",
    "data_path = os.path.join(current_folder, 'Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files   = glob.glob(data_path + '/*T.mat')\n",
    "len(training_files)     # if  return zero,then no file is loaded"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Lets Append Epochs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mne_epochs_complete(files_paths, verbose=verbose, t_start=2, fs=512, mode='train'):\n",
    "    '''\n",
    "    similar to get_mne_epochs, just appends data from all relevant files together to give a single\n",
    "    epoch object\n",
    "    '''\n",
    "    eeg_data = []\n",
    "    for filepath in files_paths:\n",
    "        mat_data = loadmat(filepath)\n",
    "        eeg_data.extend(mat_data['RawEEGData'])\n",
    "\n",
    "    idx_start = fs*t_start      # fs*ts\n",
    "    eeg_data = np.array(eeg_data)\n",
    "    eeg_data = eeg_data[:, :, idx_start:]\n",
    "    event_id = {'left-hand': 1, 'right-hand': 2}\n",
    "    channel_names = ['F3', 'FC3', 'C3', 'CP3', 'P3', 'FCz', 'CPz', 'F4', 'FC4', 'C4', 'CP4', 'P4']\n",
    "    info = mne.create_info(ch_names=channel_names, sfreq=fs, ch_types='eeg')\n",
    "    epochs = mne.EpochsArray(eeg_data, info, verbose=verbose, tmin=t_start-3.0)\n",
    "    epochs.set_montage('standard_1020')\n",
    "    epochs.filter(1., None) # required be ICA, (7-30 Hz) later\n",
    "    epochs.apply_baseline(baseline=(-.250, 0)) # linear baseline correction\n",
    "    \n",
    "    if mode == 'train': # this in only applicable for training data\n",
    "        labels = []\n",
    "        for filepath in files_paths:\n",
    "            mat_data = loadmat(filepath)\n",
    "            labels.extend(mat_data['Labels'].ravel())\n",
    "        epochs.event_id = event_id\n",
    "        epochs.events[:,2] = labels    \n",
    "    return epochs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data Loading with Band Pass Filtering"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# loading relevant files\n",
    "training_epochs_all = get_mne_epochs_complete(training_files).filter(7,32)            # for all training subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of EEG Data:  (640, 12, 3072) \t Shape of Labels:  (640,)\n"
     ]
    }
   ],
   "source": [
    "epochs = training_epochs_all.copy()\n",
    "data, labels = epochs.get_data(), epochs.events[:,-1]\n",
    "print('Shape of EEG Data: ', data.shape, '\\t Shape of Labels: ', labels.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training with Leave One Group Out CV\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = LeaveOneGroupOut()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# group parameter for leave one group out cross validation in sklearn, each subject is given unique identifier\n",
    "group_list = []\n",
    "for subject in np.linspace(1,8,8):\n",
    "    group_list.extend([subject for _ in range(80)]) # since we have 80 samples in each training file\n",
    "groups = np.array(group_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Iteration: 1 --------------------\n",
      "Features used for classification:  8\n",
      "Score on Training set:  0.5178571428571428\n",
      "\n",
      "-------------------- Iteration: 2 --------------------\n",
      "Features used for classification:  8\n",
      "Score on Training set:  0.5\n",
      "\n",
      "-------------------- Iteration: 3 --------------------\n",
      "Features used for classification:  8\n",
      "Score on Training set:  0.5678571428571428\n",
      "\n",
      "-------------------- Iteration: 4 --------------------\n",
      "Features used for classification:  8\n",
      "Score on Training set:  0.5285714285714286\n",
      "\n",
      "-------------------- Iteration: 5 --------------------\n",
      "Features used for classification:  8\n",
      "Score on Training set:  0.43214285714285716\n",
      "\n",
      "-------------------- Iteration: 6 --------------------\n",
      "Features used for classification:  8\n",
      "Score on Training set:  0.4392857142857143\n",
      "\n",
      "-------------------- Iteration: 7 --------------------\n",
      "Features used for classification:  8\n",
      "Score on Training set:  0.5642857142857143\n",
      "\n",
      "-------------------- Iteration: 8 --------------------\n",
      "Features used for classification:  8\n",
      "Score on Training set:  0.5928571428571429\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "fs = epochs.info['sfreq']\n",
    "valid_scores_lda = []\n",
    "    \n",
    "for train_idx, valid_idx in cv.split(epochs, y=labels, groups=groups):\n",
    "    print('-'*20, \"Iteration:\", i, '-'*20) \n",
    "    train_epochs = epochs[train_idx]\n",
    "    valid_epochs = epochs[valid_idx]\n",
    "\n",
    "    valid_data, valid_labels = valid_epochs.get_data()[:,:,256+512:-256], valid_epochs.events[:,-1]\n",
    "    \n",
    "    data_dict_train = {'left-hand':  train_epochs['left-hand'].get_data()[:,:,256+512:-256], # [0.5, 4.5] sec data\n",
    "             'right-hand': train_epochs['right-hand'].get_data()[:,:,256+512:-256]}\n",
    "        \n",
    "    # using LDA as classifier\n",
    "    fbcsp_clf_lda = FBCSP(data_dict_train, fs, freqs_band=freqs_band, \n",
    "                          classifier=lda(), print_var=verbose_clf)\n",
    "    preds_fbcsp_clf_lda = fbcsp_clf_lda.evaluateTrial(valid_data)[0]\n",
    "    valid_scores_lda.append(kappa_score(preds_fbcsp_clf_lda, valid_labels))\n",
    "    \n",
    "    i = i+1\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-------------------- Iteration: 1 --------------------\n",
      "Features used for classification:  8\n",
      "Score on Training set:  0.5142857142857142\n",
      "\n",
      "-------------------- Iteration: 2 --------------------\n",
      "Features used for classification:  8\n",
      "Score on Training set:  0.5285714285714286\n",
      "\n",
      "-------------------- Iteration: 3 --------------------\n",
      "Features used for classification:  8\n",
      "Score on Training set:  0.575\n",
      "\n",
      "-------------------- Iteration: 4 --------------------\n",
      "Features used for classification:  8\n",
      "Score on Training set:  0.525\n",
      "\n",
      "-------------------- Iteration: 5 --------------------\n",
      "Features used for classification:  8\n",
      "Score on Training set:  0.4464285714285714\n",
      "\n",
      "-------------------- Iteration: 6 --------------------\n",
      "Features used for classification:  8\n",
      "Score on Training set:  0.4464285714285714\n",
      "\n",
      "-------------------- Iteration: 7 --------------------\n",
      "Features used for classification:  8\n",
      "Score on Training set:  0.5642857142857143\n",
      "\n",
      "-------------------- Iteration: 8 --------------------\n",
      "Features used for classification:  8\n",
      "Score on Training set:  0.6\n",
      "\n"
     ]
    }
   ],
   "source": [
    "i = 1\n",
    "valid_scores_logreg = []\n",
    "    \n",
    "for train_idx, valid_idx in cv.split(epochs, y=labels, groups=groups):\n",
    "    print('-'*20, \"Iteration:\", i, '-'*20) \n",
    "    train_epochs = epochs[train_idx]\n",
    "    valid_epochs = epochs[valid_idx]\n",
    "\n",
    "    valid_data, valid_labels = valid_epochs.get_data()[:,:,256+512:-256], valid_epochs.events[:,-1]\n",
    "    \n",
    "    data_dict_train = {'left-hand':  train_epochs['left-hand'].get_data()[:,:,256+512:-256], # [0.5, 4.5] sec data\n",
    "             'right-hand': train_epochs['right-hand'].get_data()[:,:,256+512:-256]}\n",
    "    fs = epochs.info['sfreq']\n",
    "    \n",
    "    # using Logistic Regression as classifier\n",
    "    fbcsp_clf_logreg = FBCSP(data_dict_train, fs, freqs_band=freqs_band, \n",
    "                             classifier=LogisticRegression(), print_var=verbose_clf)\n",
    "    preds_fbcsp_clf_logreg = fbcsp_clf_logreg.evaluateTrial(valid_data)[0]\n",
    "    valid_scores_logreg.append(kappa_score(preds_fbcsp_clf_logreg, valid_labels))\n",
    "    \n",
    "    i = i+1\n",
    "    print()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "FBCSP-LDA    Cross Validation Score: 0.39999999999999997\n",
      "FBCSP-Logreg Cross Validation Score: 0.39374999999999993\n"
     ]
    }
   ],
   "source": [
    "print(\"FBCSP-LDA    Cross Validation Score:\", np.mean(valid_scores_lda))\n",
    "print(\"FBCSP-Logreg Cross Validation Score:\", np.mean(valid_scores_logreg)) \n",
    "# we aren't doing grid search here so wouldn't take max score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results\n",
    "Winner for cross subject task is FBCSP-LDA"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
