{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clinical BCI Challenge-WCCI2020\n",
    "- [website link](https://sites.google.com/view/bci-comp-wcci/?fbclid=IwAR37WLQ_xNd5qsZvktZCT8XJerHhmVb_bU5HDu69CnO85DE3iF0fs57vQ6M)\n",
    "\n",
    "\n",
    " - [Dataset Link](https://github.com/5anirban9/Clinical-Brain-Computer-Interfaces-Challenge-WCCI-2020-Glasgow)\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import mne\n",
    "import pywt\n",
    "import scipy\n",
    "import sklearn\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import glob\n",
    "from mne.decoding import CSP\n",
    "from scipy.io import loadmat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.model_selection import train_test_split, cross_val_score, GridSearchCV, StratifiedShuffleSplit\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.pipeline import make_pipeline\n",
    "from sklearn.discriminant_analysis import LinearDiscriminantAnalysis as LDA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore') # to ignore warnings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "verbose = False                    # global variable to suppress output display of MNE functions\n",
    "mne.set_log_level(verbose=verbose) # to suppress large info outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using kappa as evaluation metric\n",
    "kappa = sklearn.metrics.make_scorer(sklearn.metrics.cohen_kappa_score) # kappa scorer\n",
    "acc = sklearn.metrics.make_scorer(sklearn.metrics.accuracy_score)      # accuracy scorer\n",
    "scorer = kappa          # just assign another scorer to replace kappa scorer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_jobs = -1  # for multicore parallel processing, set it to 1 if cause memory issues"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loading and Conversion to MNE Datatypes\n",
    "[Mike Cohen Tutorials link for EEG Preprocessing](https://www.youtube.com/watch?v=uWB5tjhataY&list=PLn0OLiymPak2gDD-VDA90w9_iGDgOOb2o)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "current_folder = globals()['_dh'][0]  # a hack to get path of current folder in which juptyter file is located\n",
    "data_path = os.path.join(current_folder, 'Data')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "training_files   = glob.glob(data_path + '/*T.mat')\n",
    "len(training_files)     # if  return zero,then no file is loaded"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mne_epochs(filepath, verbose=verbose, t_start=2, fs=512, mode='train'):\n",
    "    '''\n",
    "    This function reads the EEG data from .mat file and convert it to MNE-Python Compatible epochs\n",
    "    data structure. It takes data from [0, 8] sec range and return it by setting t = 0 at cue onset\n",
    "    i.e. 3 seconds and dropping first two seconds so the output data is in [-1.0, 5.0] sec range. The\n",
    "    Details can be found in the preprocessing section of the attached document\n",
    "    '''\n",
    "    mat_data = loadmat(filepath) # read .mat file\n",
    "    eeg_data= mat_data['RawEEGData']\n",
    "    idx_start = fs*t_start      \n",
    "    eeg_data = eeg_data[:, :, idx_start:]\n",
    "    event_id = {'left-hand': 1, 'right-hand': 2}\n",
    "    channel_names = ['F3', 'FC3', 'C3', 'CP3', 'P3', 'FCz', 'CPz', 'F4', 'FC4', 'C4', 'CP4', 'P4']\n",
    "    info = mne.create_info(ch_names=channel_names, sfreq=fs, ch_types='eeg')\n",
    "    epochs = mne.EpochsArray(eeg_data, info, verbose=verbose, tmin=t_start-3.0)\n",
    "    epochs.set_montage('standard_1020')\n",
    "    epochs.filter(1., None) \n",
    "    epochs.apply_baseline(baseline=(-.250, 0)) # linear baseline correction\n",
    "    \n",
    "    if mode == 'train': # this in only applicable for training data\n",
    "        epochs.event_id = event_id\n",
    "        epochs.events[:,2] = mat_data['Labels'].ravel()    \n",
    "    return epochs \n",
    "\n",
    "def get_labels(filepath):\n",
    "    mat_data = loadmat(filepath) # read .mat file\n",
    "    return mat_data['Labels'].ravel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape of EEG Data:  (80, 12, 3072) \t Shape of Labels:  (80,)\n"
     ]
    }
   ],
   "source": [
    "epochs, labels = get_mne_epochs(training_files[0], verbose=verbose), get_labels(training_files[0])\n",
    "data = epochs.get_data()\n",
    "print('Shape of EEG Data: ', data.shape, '\\t Shape of Labels: ', labels.shape) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "epochs_list_train = []\n",
    "for i in training_files:\n",
    "    epochs_list_train.append(get_mne_epochs(i, verbose=verbose).filter(7,32))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Wavelet Packet Decomposition WPD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def wpd(X, wavelet ='coif1', maxlevel=3): \n",
    "    coeffs = pywt.WaveletPacket(X, wavelet ,mode='symmetric',maxlevel=maxlevel) \n",
    "    return coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wpd_coeffs(x, wavelet='coif1', wpd_levels=3, order='natural'):\n",
    "    '''\n",
    "    expects data in (trials, channels, raw_data) and return in (trials, bands, channels, wpd_coeffs) for sklearn compatibility\n",
    "    '''\n",
    "    \n",
    "    num_bands = 2**wpd_levels\n",
    "    trials, channels = x.shape[0], x.shape[1]\n",
    "    \n",
    "    # first find the last dim size, bcz it is different for different levels and different mother wavelets\n",
    "    coeff_size = wpd(x[0,0,:], wavelet, wpd_levels).get_level(wpd_levels)[0].data.shape[0]\n",
    "    \n",
    "    # we shall fill this array (trials, bands, channels, eegdata)\n",
    "    bands = np.empty((trials, num_bands, channels, coeff_size))\n",
    "       \n",
    "    for trial in range(trials):\n",
    "        for ch in range(channels):\n",
    "            pos = []\n",
    "            coeff = wpd(x[trial,ch,:], wavelet, wpd_levels) \n",
    "            nodes_paths = [node.path for node in coeff.get_level(level=wpd_levels, order=order)]\n",
    "            \n",
    "            for band, path in enumerate(nodes_paths): \n",
    "                bands[trial,band,ch,:] = coeff[path].data \n",
    "        \n",
    "    return bands\n",
    "\n",
    "#wpd_data = get_wpd_coeffs(data, wavelet='coif1', wpd_levels=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_power_wpd(data):\n",
    "    trials, bands, channels, N = data.shape\n",
    "    power = np.empty(shape=(trials, bands, channels))\n",
    "    \n",
    "    for trial in range(trials):\n",
    "        for band in range(bands):\n",
    "            for ch in range(channels):\n",
    "                power[trial,band,ch] = (sum(data[trial,band,ch,:]**2))/N\n",
    "    return power      \n",
    "\n",
    "#e = calculate_avg_power_wpd(wpd_data) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may try using extra statitical measures\n",
    "def calculate_statistics_wpd(data, ax=-1):\n",
    "    #minimum = np.nanmin(data, axis=ax)\n",
    "    #maximum = np.nanmax(data, axis=ax) \n",
    "    mean = np.nanmean(data, axis=ax)\n",
    "    std = np.nanstd(data, axis=ax)\n",
    "    #var = np.nanvar(data, axis=ax)\n",
    "    #skew = scipy.stats.skew(data, axis=ax)\n",
    "    #kurt = scipy.stats.kurtosis(data, axis=ax)\n",
    "    #return np.stack([minimum, maximum, mean, std, var, skew, kurt], axis=ax)\n",
    "    return np.stack([mean, std], axis=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_wpd_features(data, wavelet='coif2', level=5, retained_coeffs = 'All', stat_feat=True, power_feat=True):\n",
    "    '''\n",
    "    takes (trials, channels, data) as input and returns (trials, bands, channels, features) as output\n",
    "    has flexibility to control which set of features to use\n",
    "    '''\n",
    "    wpd_coeffs = get_wpd_coeffs(data,  wavelet=wavelet, wpd_levels=level)\n",
    "    \n",
    "    if retained_coeffs != 'All':\n",
    "        wpd_coeffs = wpd_coeffs[:,retained_coeffs,:,:]\n",
    "        \n",
    "    if power_feat == True and stat_feat == True:        # both power and statistical features\n",
    "        power = np.expand_dims(calculate_avg_power_wpd(wpd_coeffs), axis=-1)\n",
    "        statistics = calculate_statistics_wpd(wpd_coeffs)\n",
    "        band_features = np.concatenate((power, statistics), axis=-1)\n",
    "    elif power_feat == True and stat_feat == False:     # only power features\n",
    "        power = np.expand_dims(calculate_avg_power_wpd(wpd_coeffs), axis=-1)\n",
    "        band_features = power\n",
    "    elif power_feat == False and stat_feat == True:     # only statistical features\n",
    "        statistics = calculate_statistics_wpd(wpd_coeffs)\n",
    "        band_features = statistics\n",
    "    elif power_feat == False and stat_feat == False:    # invalid case\n",
    "        print('Cannot Set Both Power and Statistical Features to False')\n",
    "        \n",
    "    return band_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 32, 12, 3)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "wpd_features = get_wpd_features(data)\n",
    "wpd_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Discrete Wavelet Transform DWT\n",
    "wavedec returns [cA_n, cD_n, cD_n-1, ..., cD2, cD1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dwt(data, wavelet='coif1', level=3):\n",
    "    coeffs = pywt.wavedec(data, wavelet=wavelet, level=level)\n",
    "    return coeffs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_avg_power_dwt(data):\n",
    "    # expects individual coefficient of DWT as input\n",
    "    trials, channels, N = data.shape\n",
    "    power = np.empty(shape=(trials, channels))\n",
    "    \n",
    "    for trial in range(trials):\n",
    "        for ch in range(channels):\n",
    "            power[trial,ch] = (sum(data[trial,ch,:]**2))/N # division by N converts energy to power\n",
    "    return power"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# if you only want to use power features then use this function \n",
    "def get_dwt_power_features(data, wavelet='coif1', level=3, retained_coeffs = 'All'):\n",
    "    '''\n",
    "    takes (trials, channels, data) as input and returns (trials, bands, channels) as output\n",
    "    '''\n",
    "    dwt_coeffs = pywt.wavedec(data, wavelet=wavelet, level=level)\n",
    "    if retained_coeffs == 'All':\n",
    "        energy_dwt = np.stack(arrays = ([calculate_avg_power_dwt(dwt_coeffs[i]) for i in range(level+1)]), axis=1)\n",
    "    else: \n",
    "        energy_dwt = np.stack(arrays = ([calculate_avg_power_dwt(dwt_coeffs[i]) for i in retained_coeffs]), axis=1)\n",
    "    return energy_dwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# may try using extra statitical measures\n",
    "def calculate_statistics_dwt(data, ax=-1):\n",
    "    #minimum = np.nanmin(data, axis=ax)\n",
    "    #maximum = np.nanmax(data, axis=ax) \n",
    "    mean = np.nanmean(data, axis=ax)\n",
    "    std = np.nanstd(data, axis=ax)\n",
    "    #var = np.nanvar(data, axis=ax)\n",
    "    #skew = scipy.stats.skew(data, axis=ax)\n",
    "    #kurt = scipy.stats.kurtosis(data, axis=ax)\n",
    "    #return np.stack([minimum, maximum, mean, std, var, skew, kurt], axis=ax)\n",
    "    return np.stack([mean, std], axis=ax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_dwt_features(data, wavelet='coif2', level=5, retained_coeffs = 'All', stat_feat=True, power_feat=True):\n",
    "    '''\n",
    "    takes (trials, channels, data) as input and returns (trials, bands, channels, features) as output\n",
    "    has flexibility to control which set of features to use\n",
    "    '''\n",
    "    dwt_coeffs = pywt.wavedec(data, wavelet=wavelet, level=level)\n",
    "    dummy_list = []\n",
    "    \n",
    "    if retained_coeffs == 'All':\n",
    "        iter_list = range(len(dwt_coeffs))\n",
    "    else:\n",
    "        iter_list = retained_coeffs\n",
    "    \n",
    "    for i in iter_list:\n",
    "        \n",
    "        if power_feat == True and stat_feat == True:         # both power and statistical features\n",
    "            power = np.expand_dims(calculate_avg_power_dwt(dwt_coeffs[i]), axis=-1)\n",
    "            statistics = calculate_statistics_dwt(dwt_coeffs[i])\n",
    "            band_features = np.concatenate((power, statistics), axis = -1)\n",
    "        elif power_feat == True and stat_feat == False:     # only power features\n",
    "            power = np.expand_dims(calculate_avg_power_dwt(dwt_coeffs[i]), axis=-1)\n",
    "            band_features = power\n",
    "        elif power_feat == False and stat_feat == True:     # only statistical features\n",
    "            statistics = calculate_statistics_dwt(dwt_coeffs[i])\n",
    "            band_features = statistics\n",
    "        elif power_feat == False and stat_feat == False:    # invalid case\n",
    "            print('Cannot Set Both Power and Statistical Features to False')\n",
    "            break\n",
    "        \n",
    "        dummy_list.append(band_features)\n",
    "        \n",
    "    return np.stack(dummy_list, axis=1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(80, 6, 12, 3)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dwt_features = get_dwt_features(data)\n",
    "dwt_features.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Designing a custom transformer \n",
    "that can combine csp and reshape the wpd_data/dwt_data from (traila, bands, channels, data) to  (trials, channels, data) by applying CSP individually on each band and then concatenating the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import StandardScaler\n",
    "from mne.decoding import CSP\n",
    "from sklearn.base import BaseEstimator, TransformerMixin\n",
    "import numpy as np\n",
    "import scipy\n",
    "\n",
    "class Custom_WT_CSP(TransformerMixin, BaseEstimator):\n",
    "    \"\"\"\n",
    "    Expects data in the format (trials, bands, channels, wavelet_coeffs_data)\n",
    "    individually apply CSP on each band and then concatenate to give output of the form (trials, channels, csp_filtered_data)\n",
    "    \"\"\"\n",
    "    def __init__(self, csp_components=4):\n",
    "        self.csp_components = csp_components       # csp components to retain\n",
    "        self.num_bands = 0                         # captures the total frequency bands in filtered data\n",
    "        self.Csp = []                              # would carry list of CSP's applied individually to each freq band\n",
    "        \n",
    "    def fit(self, x, y):\n",
    "        self.num_bands = x.shape[1]\n",
    "        self.Csp = [CSP(n_components=self.csp_components, reg=None, log=True, norm_trace=False) for _ in range(self.num_bands)]\n",
    "        [self.Csp[i].fit(x[:,i,:,:], y) for i in range(self.num_bands)]\n",
    "        return self\n",
    "    \n",
    "    def transform(self, x, y=None):\n",
    "        return np.concatenate(tuple(self.Csp[i].transform(x[:,i,:,:]) for i in range(self.num_bands)),axis=-1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Helper Functions to Check which Wavelet to Use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "wavelets = ['haar', 'db2', 'db4', 'coif1', 'coif2', 'coif4', 'sym2', 'sym4']\n",
    "level_wpd = 3\n",
    "level_dwt = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# a function to calculate mean scores across classifiers\n",
    "def find_mean_score(data, labels, scorer=kappa, csp_comps=4):\n",
    "    classifiers = [\n",
    "        LinearSVC(random_state=0),\n",
    "        LDA(solver='eigen', shrinkage='auto')\n",
    "        ]\n",
    "    \n",
    "    scores = []\n",
    "    for clf in classifiers:\n",
    "        scores.append(np.mean(cross_val_score(make_pipeline(Custom_WT_CSP(csp_comps), StandardScaler(), clf), data, labels, scoring=scorer, cv=cv\n",
    "                                             )))\n",
    "    return np.round(np.mean(scores), 3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_best_wavelet(data, labels, wavelets, level_dwt=5, level_wpd=3, csp_comps=4):\n",
    "    scores = []\n",
    "    for wavelet in wavelets:\n",
    "        dwt_features = get_dwt_features(data, wavelet=wavelet, level=level_dwt)\n",
    "        wpd_features = get_wpd_features(data, wavelet=wavelet, level=level_wpd)\n",
    "        combo_features = np.concatenate((wpd_features, dwt_features), axis=1)\n",
    "        scores.append(find_mean_score(combo_features, labels, csp_comps=csp_comps))\n",
    "    best_wavelet = wavelets[np.argmax(scores)]\n",
    "    print('Best Wavelet:', best_wavelet, '\\tBest Score:', max(scores))    \n",
    "    return best_wavelet     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "---------------- Subject: 1 ----------------\n",
      "Best Wavelet: coif1 \tBest Score: 0.55\n",
      "\n",
      "---------------- Subject: 2 ----------------\n",
      "Best Wavelet: db2 \tBest Score: 0.675\n",
      "\n",
      "---------------- Subject: 3 ----------------\n",
      "Best Wavelet: db4 \tBest Score: 0.7\n",
      "\n",
      "---------------- Subject: 4 ----------------\n",
      "Best Wavelet: coif1 \tBest Score: 0.525\n",
      "\n",
      "---------------- Subject: 5 ----------------\n",
      "Best Wavelet: db2 \tBest Score: 0.7\n",
      "\n",
      "---------------- Subject: 6 ----------------\n",
      "Best Wavelet: db4 \tBest Score: 0.6\n",
      "\n",
      "---------------- Subject: 7 ----------------\n",
      "Best Wavelet: db2 \tBest Score: 0.65\n",
      "\n",
      "---------------- Subject: 8 ----------------\n",
      "Best Wavelet: sym4 \tBest Score: 0.5\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_wavelets = []\n",
    "subject = 1\n",
    "for epochs in epochs_list_train:     \n",
    "    data, labels = epochs.get_data(), epochs.events[:,-1]\n",
    "    data = data[:,:,256+512:-256]        # to capture data from 0.5s to 4.5s range\n",
    "    print('-'*16, 'Subject:', subject, '-'*16)\n",
    "    best_wavelets.append(get_best_wavelet(data, labels, wavelets, csp_comps=4))\n",
    "    print()\n",
    "    subject += 1 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['coif1', 'db2', 'db4', 'coif1', 'db2', 'db4', 'db2', 'sym4']"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_wavelets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training and Evaluation Scripts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 218,
   "metadata": {},
   "outputs": [],
   "source": [
    "def training_function(subject_index=0):\n",
    "    # this time training function trains on whole training set\n",
    "    print('-'*25, 'Training for Subject:', subject_index+1, '-'*25)\n",
    "    epochs = epochs_list_train[subject_index]\n",
    "    data = epochs.get_data()\n",
    "    data = data[:,:,256+512:-256] # from 0.5s to 4.5s\n",
    "    labels = epochs.events[:,-1]\n",
    "    best_wavelet = best_wavelets[subject_index]\n",
    "    dwt_features = get_dwt_features(data, wavelet= best_wavelet, level=level_dwt)\n",
    "    wpd_features = get_wpd_features(data, wavelet= best_wavelet, level=level_wpd)\n",
    "    combo_wavelet_features = np.concatenate((wpd_features, dwt_features), axis=1)\n",
    "    \n",
    "    grids_linear_svm_list[subject_index].fit(combo_wavelet_features, labels)\n",
    "    print('LinearSVM: Maximum Cross Validation Score = ', round(grids_linear_svm_list[subject_index].best_score_,3))\n",
    "    grids_lda_list[subject_index].fit(combo_wavelet_features, labels)\n",
    "    print('LDA      : Maximum Cross Validation Score = ', round(grids_lda_list[subject_index].best_score_,3))\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Training Time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {},
   "outputs": [],
   "source": [
    "cv = StratifiedShuffleSplit(n_splits=10, random_state=0)  # cross validation strategy to use "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {},
   "outputs": [],
   "source": [
    "csp_comps = [4]\n",
    "# linear svm\n",
    "param_grid_linear_svm =  {'linearsvc__C' : np.logspace(-4, 2, 15),\n",
    "                          'custom_wt_csp__csp_components': csp_comps}\n",
    "\n",
    "# for lda\n",
    "shrinkage = list(np.arange(0.0,1.01,0.1))\n",
    "shrinkage.append('auto')\n",
    "param_grid_lda = {'lineardiscriminantanalysis__shrinkage': shrinkage,\n",
    "                  'custom_wt_csp__csp_components': csp_comps}  \n",
    "\n",
    "num_subjects = len(training_files)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# linear svm \n",
    "grids_linear_svm_list = [GridSearchCV(make_pipeline(Custom_WT_CSP(), StandardScaler(), LinearSVC()), \n",
    "                            param_grid=param_grid_linear_svm, cv=cv, scoring=scorer, n_jobs=n_jobs)\n",
    "                         for _ in range(num_subjects)]\n",
    "\n",
    "#lda \n",
    "grids_lda_list = [GridSearchCV(make_pipeline(Custom_WT_CSP(), StandardScaler(), LDA(solver='eigen')), \n",
    "                        param_grid=param_grid_lda, cv=cv, scoring=scorer, n_jobs=n_jobs)\n",
    "                  for _ in range(num_subjects)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "------------------------- Training for Subject: 1 -------------------------\n",
      "LinearSVM: Maximum Cross Validation Score =  0.6\n",
      "LDA      : Maximum Cross Validation Score =  0.65\n",
      "\n",
      "------------------------- Training for Subject: 2 -------------------------\n",
      "LinearSVM: Maximum Cross Validation Score =  0.75\n",
      "LDA      : Maximum Cross Validation Score =  0.775\n",
      "\n",
      "------------------------- Training for Subject: 3 -------------------------\n",
      "LinearSVM: Maximum Cross Validation Score =  0.65\n",
      "LDA      : Maximum Cross Validation Score =  0.7\n",
      "\n",
      "------------------------- Training for Subject: 4 -------------------------\n",
      "LinearSVM: Maximum Cross Validation Score =  0.575\n",
      "LDA      : Maximum Cross Validation Score =  0.6\n",
      "\n",
      "------------------------- Training for Subject: 5 -------------------------\n",
      "LinearSVM: Maximum Cross Validation Score =  0.675\n",
      "LDA      : Maximum Cross Validation Score =  0.675\n",
      "\n",
      "------------------------- Training for Subject: 6 -------------------------\n",
      "LinearSVM: Maximum Cross Validation Score =  0.575\n",
      "LDA      : Maximum Cross Validation Score =  0.575\n",
      "\n",
      "------------------------- Training for Subject: 7 -------------------------\n",
      "LinearSVM: Maximum Cross Validation Score =  0.575\n",
      "LDA      : Maximum Cross Validation Score =  0.575\n",
      "\n",
      "------------------------- Training for Subject: 8 -------------------------\n",
      "LinearSVM: Maximum Cross Validation Score =  0.675\n",
      "LDA      : Maximum Cross Validation Score =  0.7\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for subject in range(len(training_files)):\n",
    "    training_function(subject)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
